name: Deploy Infrastructure (deploy-infra)

# This workflow manages infrastructure deployments (VPC, OIDC, DNS/ACM, ECS Fargate)
# It is manual-only for safety and control over infrastructure changes.

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target Environment'
        required: true
        default: 'dev'
        type: choice
        options: [dev, staging, production]
      module_path:
        description: 'Terraform module to deploy (e.g., 01-vpc, 04-ecs-fargate)'
        required: true
        type: string
      action:
        description: 'Terraform action to perform'
        required: true
        default: 'plan'
        type: choice
        options: [plan, apply, destroy]

jobs:
  deploy:
    name: ${{ inputs.action }} - ${{ inputs.module_path }}
    runs-on: ubuntu-latest
    
    permissions:
      id-token: write
      contents: read

    environment:
      name: ${{ inputs.environment }}

    steps:
      - name: Checkout CI repository
        uses: actions/checkout@v4

      - name: Debug GitHub context
        run: |
          echo "repository: $GITHUB_REPOSITORY"
          echo "ref:        $GITHUB_REF"
          echo "event_name: $GITHUB_EVENT_NAME"

      - name: Debug AWS role and region
        run: |
          echo "Has AWS_ROLE_ARN: $([ -n \"$AWS_ROLE_ARN\" ] && echo yes || echo no)"
          echo "AWS_ROLE_ARN length: ${#AWS_ROLE_ARN}"
          echo "AWS_REGION: $AWS_REGION"
        env:
          AWS_ROLE_ARN: ${{ secrets.AWS_ROLE_ARN }}
          AWS_REGION: ${{ secrets.AWS_REGION }}

      - name: Terraform Setup
        uses: ./.github/actions/terraform-setup
        with:
          devops_repo: ${{ secrets.DEVOPS_REPO_NAME || 'orshalit/DEVOPS' }}
          devops_repo_key: ${{ secrets.DEVOPS_REPO_KEY }}
          aws_role_arn: ${{ secrets.AWS_ROLE_ARN }}
          aws_region: ${{ secrets.AWS_REGION }}
          terraform_version: '1.6.0'

      - name: Terraform Format Check
        id: fmt
        run: terraform fmt -check -recursive DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}
        continue-on-error: true

      - name: Terraform Init
        id: init
        run: terraform -chdir=DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }} init

      - name: Terraform Validate
        id: validate
        run: terraform -chdir=DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }} validate -no-color || echo "Terraform validate failed (non-blocking); relying on plan for validation."

      - name: Check for terraform.tfvars
        id: check_tfvars
        run: |
          if [ -f "DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}/terraform.tfvars" ]; then
            echo "tfvars_exists=true" >> $GITHUB_OUTPUT
          else
            echo "tfvars_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Check for services.generated.tfvars
        id: check_services_tfvars
        run: |
          if [ -f "DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}/services.generated.tfvars" ]; then
            echo "services_tfvars_exists=true" >> $GITHUB_OUTPUT
          else
            echo "services_tfvars_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Build var files list
        id: build_var_files
        if: inputs.action == 'plan' || inputs.action == 'apply'
        run: |
          VAR_FILES=""
          if [ "${{ steps.check_tfvars.outputs.tfvars_exists }}" == "true" ]; then
            VAR_FILES="terraform.tfvars"
          fi
          if [ "${{ steps.check_services_tfvars.outputs.services_tfvars_exists }}" == "true" ]; then
            if [ -n "$VAR_FILES" ]; then
              VAR_FILES="$VAR_FILES services.generated.tfvars"
            else
              VAR_FILES="services.generated.tfvars"
            fi
          fi
          echo "var_files=$VAR_FILES" >> $GITHUB_OUTPUT

      - name: Terraform Refresh State
        id: refresh
        if: inputs.action == 'plan' || inputs.action == 'apply'
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          REFRESH_ARGS="-no-color"
          
          # Add var files if they exist (same logic as plan)
          if [ "${{ steps.check_tfvars.outputs.tfvars_exists }}" == "true" ]; then
            REFRESH_ARGS="$REFRESH_ARGS -var-file=terraform.tfvars"
          fi
          if [ "${{ steps.check_services_tfvars.outputs.services_tfvars_exists }}" == "true" ]; then
            REFRESH_ARGS="$REFRESH_ARGS -var-file=services.generated.tfvars"
          fi
          
          echo "::notice::Refreshing Terraform state to sync with AWS..."
          echo "::notice::This will update the remote state to match actual AWS resources..."
          terraform -chdir="$PLAN_DIR" refresh $REFRESH_ARGS
          REFRESH_EXITCODE=$?
          
          if [ $REFRESH_EXITCODE -ne 0 ]; then
            echo "::warning::State refresh failed with exit code $REFRESH_EXITCODE, but continuing with plan..."
          else
            echo "::notice::State refresh completed successfully"
          fi

      - name: Debug - Check Configuration Values
        id: debug_config
        if: inputs.action == 'plan' || inputs.action == 'apply' && inputs.module_path == '04-ecs-fargate'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          echo "::notice::Debugging: Checking health_check_path values in configuration..."
          
          if [ -f "$PLAN_DIR/services.generated.tfvars" ]; then
            echo "::notice::Checking services.generated.tfvars for health_check_path:"
            grep -A 2 -B 2 "health_check_path" "$PLAN_DIR/services.generated.tfvars" || echo "No health_check_path found in tfvars"
          fi
          
          echo "::notice::Checking what Terraform sees in state vs configuration:"
          cd "$PLAN_DIR"
          
          # Check state value
          echo "State value for api target group health check path:"
          terraform state show 'module.ecs_fargate.aws_lb_target_group.services["app_shared::api"]' 2>/dev/null | grep -A 1 "path" | head -2 || echo "Could not read state"
          
          echo ""
          echo "State value for api_single target group health check path:"
          terraform state show 'module.ecs_fargate.aws_lb_target_group.services["app_single::api_single"]' 2>/dev/null | grep -A 1 "path" | head -2 || echo "Could not read state"

      - name: Detect State Drift (Target Groups)
        id: detect_drift
        if: inputs.action == 'plan' || inputs.action == 'apply'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          # Only check for ECS Fargate module
          if [[ "${{ inputs.module_path }}" == "04-ecs-fargate" ]]; then
            echo "::notice::Checking for state drift in target group health check paths..."
            
            if [ -f "scripts/verify-target-group-health-checks.sh" ]; then
              chmod +x scripts/verify-target-group-health-checks.sh
              export TERRAFORM_DIR="$PLAN_DIR"
              export ENVIRONMENT="${{ inputs.environment }}"
              
              if scripts/verify-target-group-health-checks.sh; then
                echo "drift_detected=false" >> $GITHUB_OUTPUT
                echo "::notice::No state drift detected"
              else
                echo "drift_detected=true" >> $GITHUB_OUTPUT
                echo "::warning::State drift detected! Consider running terraform refresh manually."
              fi
            else
              echo "::notice::State drift detection script not found, skipping..."
              echo "drift_detected=unknown" >> $GITHUB_OUTPUT
            fi
          else
            echo "::notice::State drift detection only runs for ECS Fargate module"
            echo "drift_detected=skipped" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Plan
        id: plan
        if: inputs.action == 'plan' || inputs.action == 'apply'
        uses: ./.github/actions/terraform-plan
        with:
          terraform_path: DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}
          var_files: ${{ steps.build_var_files.outputs.var_files }}
          plan_file: "tfplan"
        continue-on-error: true

      - name: Debug Plan - Show Replacement Reasons
        if: inputs.action == 'plan' || inputs.action == 'apply'
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          if [ -f "$PLAN_DIR/tfplan" ]; then
            echo "::group::Debug: Resources Being Replaced"
            echo "Checking for resources that need replacement..."
            terraform -chdir="$PLAN_DIR" show -no-color tfplan | grep -A 10 "must be replaced" || echo "No replacements detected"
            echo "::endgroup::"
            
            echo "::group::Debug: Detailed Plan Output"
            terraform -chdir="$PLAN_DIR" show -no-color -detailed-exitcode tfplan || true
            echo "::endgroup::"
          else
            echo "::notice::Plan file not available for debugging"
          fi

      - name: Add Plan to Summary
        if: inputs.action == 'plan' || (inputs.action == 'apply' && steps.plan.outcome == 'success')
        run: |
          echo "## Terraform Plan Output" >> $GITHUB_STEP_SUMMARY
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          if [ -f "$PLAN_DIR/tfplan" ]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            terraform -chdir="$PLAN_DIR" show -no-color tfplan >> $GITHUB_STEP_SUMMARY 2>&1
            echo '```' >> $GITHUB_STEP_SUMMARY
            
            # Add replacement analysis to summary
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ðŸ” Replacement Analysis" >> $GITHUB_STEP_SUMMARY
            REPLACEMENTS=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 | grep -c "must be replaced" || echo "0")
            if [ "$REPLACEMENTS" -gt "0" ]; then
              echo "âš ï¸ **Warning:** $REPLACEMENTS resource(s) will be replaced" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "Resources being replaced:" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 | grep -B 2 -A 5 "must be replaced" >> $GITHUB_STEP_SUMMARY || true
              echo '```' >> $GITHUB_STEP_SUMMARY
            else
              echo "âœ… No replacements detected" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "Plan file not available (tfplan not found)" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check if plan has changes
        id: plan_changes
        if: inputs.action == 'apply' && steps.plan.outcome == 'success'
        run: |
          PLAN_OUTPUT=$(terraform -chdir=DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }} show -no-color tfplan 2>&1)
          if echo "$PLAN_OUTPUT" | grep -q "No changes"; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "::notice::No changes detected. Skipping apply."
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "::notice::Changes detected. Proceeding with apply."
          fi

      - name: Save Terraform state before apply
        id: save_state
        if: inputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        run: |
          echo "::notice::Saving Terraform state before apply for potential rollback..."
          # Create a backup of the state file if it exists
          if [ -f "DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}/.terraform/terraform.tfstate" ]; then
            cp "DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}/.terraform/terraform.tfstate" \
               "DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}/.terraform/terraform.tfstate.backup" || true
            echo "state_backed_up=true" >> $GITHUB_OUTPUT
          else
            echo "state_backed_up=false" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Apply
        id: apply
        if: inputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        uses: ./.github/actions/terraform-apply
        with:
          terraform_path: DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}
          plan_file: "tfplan"
          auto_approve: "true"

      - name: Verify Target Group Health Check Paths (Post-Apply)
        id: verify_health_checks
        if: inputs.action == 'apply' && steps.apply.outcome == 'success' && inputs.module_path == '04-ecs-fargate'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          echo "::notice::Verifying target group health check paths after apply..."
          
          if [ -f "scripts/verify-target-group-health-checks.sh" ]; then
            chmod +x scripts/verify-target-group-health-checks.sh
            export TERRAFORM_DIR="$PLAN_DIR"
            export ENVIRONMENT="${{ inputs.environment }}"
            
            if scripts/verify-target-group-health-checks.sh; then
              echo "::notice::âœ“ All target group health check paths verified successfully"
              echo "verification_passed=true" >> $GITHUB_OUTPUT
            else
              echo "::warning::âš  State drift detected after apply. Target groups may not match configuration."
              echo "verification_passed=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "::notice::Verification script not found, skipping..."
            echo "verification_passed=skipped" >> $GITHUB_OUTPUT
          fi

      - name: Cleanup partially created resources on apply failure
        if: inputs.action == 'apply' && steps.apply.outcome == 'failure' && steps.save_state.outputs.state_backed_up == 'true' && inputs.environment != 'production'
        continue-on-error: true
        run: |
          echo "::error::Terraform apply failed. Attempting to destroy partially created resources..."
          
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          # Try to refresh state to see what was actually created
          echo "::notice::Refreshing Terraform state to identify created resources..."
          terraform -chdir="$PLAN_DIR" refresh -no-color || echo "::warning::State refresh failed, proceeding with destroy attempt"
          
          # Attempt to destroy what was created
          # Use the same variable files as the apply
          DESTROY_ARGS="-auto-approve"
          if [ -n "${{ steps.build_var_files.outputs.var_files }}" ]; then
            for var_file in ${{ steps.build_var_files.outputs.var_files }}; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                DESTROY_ARGS="$DESTROY_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          echo "::notice::Running terraform destroy to clean up partially created resources..."
          terraform -chdir="$PLAN_DIR" destroy $DESTROY_ARGS || {
            echo "::error::Destroy failed. Manual cleanup may be required."
            echo "::error::Please review the Terraform state and manually remove any partially created resources."
            exit 1
          }
          
          echo "::notice::Cleanup completed. Partially created resources have been destroyed."

      - name: Production apply failure notification
        if: inputs.action == 'apply' && steps.apply.outcome == 'failure' && inputs.environment == 'production'
        run: |
          echo "::error::Terraform apply failed in PRODUCTION environment."
          echo "::error::Automatic cleanup is disabled for production. Manual intervention required."
          echo "::error::Please review the Terraform state and decide on appropriate action:"
          echo "::error::  1. Review what resources were created before the failure"
          echo "::error::  2. Manually destroy partially created resources if needed"
          echo "::error::  3. Fix the issue and retry the deployment"
          exit 1

      - name: No Changes Detected
        if: inputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'false'
        run: |
          echo "::notice::No changes to apply. Infrastructure is already up to date."
          echo "## âœ… No Changes Required" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The Terraform plan showed no changes. Your infrastructure matches the configuration." >> $GITHUB_STEP_SUMMARY

      - name: Plan Failed
        if: inputs.action == 'apply' && steps.plan.outcome == 'failure'
        run: |
          echo "::error::Terraform plan failed. Apply cannot proceed."
          exit 1

      - name: Terraform Destroy (Production Protection)
        if: inputs.action == 'destroy' && inputs.environment == 'production'
        run: |
          echo "::error::Destroy action is blocked for production environment for safety."
          exit 1

      - name: Build var files list for destroy
        id: build_var_files_destroy
        if: inputs.action == 'destroy' && inputs.environment != 'production'
        run: |
          VAR_FILES=""
          if [ "${{ steps.check_tfvars.outputs.tfvars_exists }}" == "true" ]; then
            VAR_FILES="terraform.tfvars"
          fi
          if [ "${{ steps.check_services_tfvars.outputs.services_tfvars_exists }}" == "true" ]; then
            if [ -n "$VAR_FILES" ]; then
              VAR_FILES="$VAR_FILES services.generated.tfvars"
            else
              VAR_FILES="services.generated.tfvars"
            fi
          fi
          echo "var_files=$VAR_FILES" >> $GITHUB_OUTPUT

      - name: Terraform Destroy
        if: inputs.action == 'destroy' && inputs.environment != 'production'
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          DESTROY_ARGS="-auto-approve"
          if [ -n "${{ steps.build_var_files_destroy.outputs.var_files }}" ]; then
            for var_file in ${{ steps.build_var_files_destroy.outputs.var_files }}; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                DESTROY_ARGS="$DESTROY_ARGS -var-file=$var_file"
              fi
            done
          fi
          terraform -chdir="$PLAN_DIR" destroy $DESTROY_ARGS
