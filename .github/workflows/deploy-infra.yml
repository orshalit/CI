name: Deploy Infrastructure (deploy-infra)

# This workflow manages infrastructure deployments (VPC, OIDC, DNS/ACM, ECS Fargate)
# It is manual-only for safety and control over infrastructure changes.

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target Environment'
        required: true
        default: 'dev'
        type: choice
        options: [dev, staging, production]
      module_path:
        description: 'Terraform module to deploy (e.g., 01-vpc, 04-ecs-fargate)'
        required: true
        type: string
      action:
        description: 'Terraform action to perform'
        required: true
        default: 'plan'
        type: choice
        options: [plan, apply, destroy]

# Prevent concurrent runs on the same environment/module to avoid state corruption
concurrency:
  group: deploy-${{ github.event.inputs.environment }}-${{ github.event.inputs.module_path }}
  cancel-in-progress: false  # Wait for current run to finish (safer than canceling)

jobs:
  deploy:
    name: ${{ inputs.action }} - ${{ inputs.module_path }}
    runs-on: ubuntu-latest
    
    permissions:
      id-token: write
      contents: read

    environment:
      name: ${{ inputs.environment }}

    steps:
      - name: Checkout CI repository
        uses: actions/checkout@v4

      - name: Debug GitHub context
        run: |
          echo "repository: $GITHUB_REPOSITORY"
          echo "ref:        $GITHUB_REF"
          echo "event_name: $GITHUB_EVENT_NAME"

      - name: Debug AWS role and region
        run: |
          echo "Has AWS_ROLE_ARN: $([ -n \"$AWS_ROLE_ARN\" ] && echo yes || echo no)"
          echo "AWS_ROLE_ARN length: ${#AWS_ROLE_ARN}"
          echo "AWS_REGION: $AWS_REGION"
        env:
          AWS_ROLE_ARN: ${{ secrets.AWS_ROLE_ARN }}
          AWS_REGION: ${{ secrets.AWS_REGION }}

      - name: Terraform Setup
        uses: ./.github/actions/terraform-setup
        with:
          devops_repo: ${{ secrets.DEVOPS_REPO_NAME || 'orshalit/DEVOPS' }}
          devops_repo_key: ${{ secrets.DEVOPS_REPO_KEY }}
          aws_role_arn: ${{ secrets.AWS_ROLE_ARN }}
          aws_region: ${{ secrets.AWS_REGION }}
          terraform_version: '1.6.0'

      - name: Terraform Format
        id: fmt
        run: |
          TERRAFORM_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          echo "::notice::Formatting Terraform files in $TERRAFORM_DIR..."
          
          # Format all Terraform files
          terraform fmt -recursive "$TERRAFORM_DIR"
          
          # Check if any files were modified by comparing before/after
          cd DEVOPS
          # Get list of files that would be formatted (all .tf and .tfvars files)
          FORMATTED_FILES=$(find "live/${{ inputs.environment }}/${{ inputs.module_path }}" -type f \( -name "*.tf" -o -name "*.tfvars" \) 2>/dev/null || true)
          
          if [ -z "$FORMATTED_FILES" ]; then
            echo "::notice::No Terraform files found to format"
            exit 0
          fi
          
          # Check git status for changes in the terraform directory
          if git diff --quiet "live/${{ inputs.environment }}/${{ inputs.module_path }}"; then
            echo "::notice::‚úì All Terraform files are properly formatted"
          else
            echo "::error::‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
            echo "::error::‚ïë  ‚ö†Ô∏è  TERRAFORM FORMATTING ISSUES DETECTED  ‚ö†Ô∏è  ‚ïë"
            echo "::error::‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
            echo ""
            echo "::error::The following files need formatting:"
            git status --short "live/${{ inputs.environment }}/${{ inputs.module_path }}" | while read -r line; do
              echo "::error::  ‚Ä¢ $line"
            done
            echo ""
            echo "::error::Files have been automatically formatted in this workflow run."
            echo "::error::"
            echo "::error::To fix this:"
            echo "::error::  1. Run 'terraform fmt -recursive' locally in: $TERRAFORM_DIR"
            echo "::error::  2. Commit the formatted files"
            echo "::error::  3. Push and re-run this workflow"
            echo ""
            echo "::group::Show formatted changes"
            git diff "live/${{ inputs.environment }}/${{ inputs.module_path }}"
            echo "::endgroup::"
            exit 1
          fi

      - name: Terraform Init
        id: init
        run: terraform -chdir=DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }} init

      - name: Check for terraform.tfvars
        id: check_tfvars
        run: |
          if [ -f "DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}/terraform.tfvars" ]; then
            echo "tfvars_exists=true" >> $GITHUB_OUTPUT
          else
            echo "tfvars_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Check for services.generated.tfvars
        id: check_services_tfvars
        run: |
          if [ -f "DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}/services.generated.tfvars" ]; then
            echo "services_tfvars_exists=true" >> $GITHUB_OUTPUT
          else
            echo "services_tfvars_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Check if services is empty
        id: check_services_empty
        if: inputs.action == 'plan' || inputs.action == 'apply'
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          # Check if services.generated.tfvars exists and has non-empty services
          if [ -f "$PLAN_DIR/services.generated.tfvars" ]; then
            # Check if services map is empty (services = {})
            # An empty services map will have "services = {}" on a single line or just "{}" after "services ="
            # Service keys may be quoted (e.g., "legacy::api") or unquoted (e.g., legacy::api)
            if grep -qE "^\s*services\s*=\s*\{\}\s*$" "$PLAN_DIR/services.generated.tfvars" || \
               (grep -qE "^\s*services\s*=\s*\{$" "$PLAN_DIR/services.generated.tfvars" && \
                ! grep -A 10 "services = {" "$PLAN_DIR/services.generated.tfvars" | grep -qE "^\s+(\"[^\"]+::[^\"]+\"|[a-zA-Z0-9_-]+::[a-zA-Z0-9_-]+)\s*=\s*\{"); then
              echo "services_empty=true" >> $GITHUB_OUTPUT
              echo "::notice::Services map is empty (services = {}), will exclude from var files to avoid Terraform crash"
            else
              echo "services_empty=false" >> $GITHUB_OUTPUT
              echo "::notice::Services map has content"
            fi
          else
            echo "services_empty=true" >> $GITHUB_OUTPUT
            echo "::notice::services.generated.tfvars not found"
          fi

      - name: Terraform Validate
        id: validate
        # Skip validate for ECS Fargate module entirely to avoid Terraform crash
        # This is a known Terraform bug with marked values in conditional expressions
        # terraform plan will catch all validation errors anyway
        if: inputs.module_path != '04-ecs-fargate'
        run: terraform -chdir=DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }} validate -no-color || echo "Terraform validate failed (non-blocking); relying on plan for validation."

      - name: Build var files list
        id: build_var_files
        if: inputs.action == 'plan' || inputs.action == 'apply'
        run: |
          VAR_FILES=""
          if [ "${{ steps.check_tfvars.outputs.tfvars_exists }}" == "true" ]; then
            VAR_FILES="terraform.tfvars"
          fi
          # Only include services.generated.tfvars if it exists AND services is not empty
          # This prevents Terraform crash when services = {} (empty services map)
          if [ "${{ steps.check_services_tfvars.outputs.services_tfvars_exists }}" == "true" ] && \
             [ "${{ steps.check_services_empty.outputs.services_empty }}" != "true" ]; then
            if [ -n "$VAR_FILES" ]; then
              VAR_FILES="$VAR_FILES services.generated.tfvars"
            else
              VAR_FILES="services.generated.tfvars"
            fi
          fi
          echo "var_files=$VAR_FILES" >> $GITHUB_OUTPUT

      - name: Check for existing services in state
        id: check_state_services
        if: |
          (inputs.action == 'plan' || inputs.action == 'apply') &&
          steps.check_services_empty.outputs.services_empty == 'true' &&
          inputs.module_path == '04-ecs-fargate'
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          echo "::notice::Checking Terraform state for existing services..."
          
          # Initialize Terraform to access state (without var files to avoid crash)
          cd "$PLAN_DIR"
          terraform init -no-color -input=false > /dev/null 2>&1 || true
          
          # Check if state has any ECS services
          # Use terraform state list to find services (safe even with empty services config)
          if terraform state list -no-color 2>/dev/null | grep -q "aws_ecs_service.services\["; then
            echo "services_in_state=true" >> $GITHUB_OUTPUT
            EXISTING_SERVICES=$(terraform state list -no-color 2>/dev/null | grep "aws_ecs_service.services\[" || echo "")
            echo "existing_services<<EOF" >> $GITHUB_OUTPUT
            echo "$EXISTING_SERVICES" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
            
            echo ""
            echo "::error::‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
            echo "::error::‚ïë  ‚ö†Ô∏è  DEPLOYMENT BLOCKED: DANGEROUS CONFIGURATION DETECTED  ‚ö†Ô∏è  ‚ïë"
            echo "::error::‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
            echo ""
            echo "::error::Problem: services = {} (empty) but Terraform state has existing services!"
            echo ""
            echo "::error::If you proceed, Terraform will DESTROY all existing services:"
            echo "$EXISTING_SERVICES" | while read -r service; do
              echo "::error::  ‚ùå $service"
            done
            echo ""
            echo "::error::Why this is blocked:"
            echo "::error::  ‚Ä¢ Config says: 'deploy zero services' (services = {})"
            echo "::error::  ‚Ä¢ State says: 'these services exist'"
            echo "::error::  ‚Ä¢ Terraform will destroy existing services to match empty config"
            echo "::error::  ‚Ä¢ This is almost certainly a mistake, not intentional"
            echo ""
            echo "::error::Solution - Generate services configuration first:"
            echo "::error::  1. Go to: Actions ‚Üí 'Create / Update ECS Service (generate DEVOPS PR)'"
            echo "::error::  2. Run the workflow to generate services.generated.tfvars from YAML"
            echo "::error::  3. Review and merge the PR in DEVOPS repository"
            echo "::error::  4. Then run this deployment workflow again"
            echo ""
            echo "::error::This ensures services in config match your YAML definitions."
            exit 1
          else
            echo "services_in_state=false" >> $GITHUB_OUTPUT
            echo "::notice::‚úì No existing services in state"
            echo "::notice::‚úì Safe to proceed with empty services config (fresh deployment)"
          fi

      - name: Comprehensive State Validation and Auto-Import
        id: validate_state
        if: |
          (inputs.action == 'plan' || inputs.action == 'apply') &&
          inputs.module_path == '04-ecs-fargate' &&
          steps.check_services_empty.outputs.services_empty != 'true'
        continue-on-error: true
        timeout-minutes: 5
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          if [ -f "scripts/validate-and-import-state.sh" ]; then
            chmod +x scripts/validate-and-import-state.sh
            scripts/validate-and-import-state.sh "$PLAN_DIR" "${{ inputs.environment }}" "${{ inputs.module_path }}" || {
              EXIT_CODE=$?
              if [ $EXIT_CODE -eq 1 ]; then
                echo "::error::State validation found resources that couldn't be imported"
                echo "::error::These will cause 'already exists' errors. Manual intervention may be needed."
              fi
              exit 0  # Don't fail workflow, but warn
            }
          else
            echo "::warning::State validation script not found, skipping comprehensive validation"
          fi

      - name: Terraform Refresh State (Pre-Plan Sync)
        id: refresh
        if: inputs.action == 'plan' || inputs.action == 'apply'
        continue-on-error: true
        timeout-minutes: 5
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          REFRESH_ARGS="-no-color"
          
          # Check if state exists - if not, skip refresh (fresh deployment)
          if ! terraform -chdir="$PLAN_DIR" state list -no-color >/dev/null 2>&1; then
            echo "::notice::No Terraform state found - skipping refresh (fresh deployment)"
            echo "::notice::This is expected when starting with a new/empty state"
            exit 0
          fi
          
          # Use the same var files logic as plan (which excludes empty services.generated.tfvars)
          # This prevents Terraform crash when services = {} (empty services map)
          VAR_FILES="${{ steps.build_var_files.outputs.var_files }}"
          
          if [ -n "$VAR_FILES" ]; then
            for var_file in $VAR_FILES; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                REFRESH_ARGS="$REFRESH_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          echo "::notice::üîÑ Refreshing Terraform state to sync with AWS (preventing state drift)..."
          if [ "${{ steps.check_services_empty.outputs.services_empty }}" == "true" ]; then
            echo "::notice::Services map is empty - refreshing infrastructure only (services excluded to avoid crash)"
          else
            echo "::notice::Refreshing all resources including services"
          fi
          
          # Use refresh-only mode to update state without making changes
          # This syncs state with actual AWS resources
          timeout 300 terraform -chdir="$PLAN_DIR" apply -refresh-only -auto-approve $REFRESH_ARGS 2>&1 | tee /tmp/refresh_output.txt || {
            REFRESH_EXITCODE=$?
            if [ $REFRESH_EXITCODE -eq 124 ]; then
              echo "::warning::State refresh timed out after 5 minutes, but continuing with plan..."
            else
              # Check if refresh-only failed because there are no changes (exit code 0 from apply -refresh-only)
              if grep -q "No changes" /tmp/refresh_output.txt 2>/dev/null; then
                echo "::notice::‚úì State is already in sync with AWS (no changes needed)"
              else
                echo "::warning::State refresh failed with exit code $REFRESH_EXITCODE, but continuing with plan..."
                echo "::warning::Refresh output:"
                cat /tmp/refresh_output.txt 2>/dev/null || true
              fi
            fi
            exit 0  # Don't fail the workflow, just warn
          }
          
          # Check if refresh-only made any changes
          STATE_UPDATED=false
          if grep -q "No changes" /tmp/refresh_output.txt 2>/dev/null; then
            echo "::notice::‚úì State is already in sync with AWS"
          else
            echo "::notice::‚úì State refreshed - updated to match AWS resources"
            STATE_UPDATED=true
            # Show what was updated
            if grep -q "updated in-place" /tmp/refresh_output.txt 2>/dev/null; then
              echo "::notice::Resources updated in state:"
              grep "updated in-place" /tmp/refresh_output.txt | head -5 || true
            fi
            # Check if any resources were added to state (imported via refresh)
            if grep -q "has been imported" /tmp/refresh_output.txt 2>/dev/null; then
              echo "::notice::Resources imported during refresh:"
              grep "has been imported" /tmp/refresh_output.txt | head -5 || true
            fi
          fi
          
          # If state was updated, invalidate any existing plan file
          # This ensures plan is recreated with fresh state
          if [ "$STATE_UPDATED" = "true" ] && [ -f "$PLAN_DIR/tfplan" ]; then
            echo "::notice::‚ö† State was updated - existing plan file will be invalidated"
            echo "::notice::Plan will be recreated with fresh state"
            rm -f "$PLAN_DIR/tfplan" || true
            echo "state_updated=true" >> $GITHUB_OUTPUT
          else
            echo "state_updated=false" >> $GITHUB_OUTPUT
          fi
          
          rm -f /tmp/refresh_output.txt

      - name: Detect State Drift (Target Groups)
        id: detect_drift
        if: inputs.action == 'plan' || inputs.action == 'apply'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          # Only check for ECS Fargate module
          if [[ "${{ inputs.module_path }}" == "04-ecs-fargate" ]]; then
            echo "::notice::Checking for state drift in target group health check paths..."
            
            if [ -f "scripts/verify-target-group-health-checks.sh" ]; then
              chmod +x scripts/verify-target-group-health-checks.sh
              export TERRAFORM_DIR="$PLAN_DIR"
              export ENVIRONMENT="${{ inputs.environment }}"
              
              if scripts/verify-target-group-health-checks.sh; then
                echo "drift_detected=false" >> $GITHUB_OUTPUT
                echo "::notice::No state drift detected"
              else
                echo "drift_detected=true" >> $GITHUB_OUTPUT
                echo "::warning::State drift detected! Consider running terraform refresh manually."
              fi
            else
              echo "::notice::State drift detection script not found, skipping..."
              echo "drift_detected=unknown" >> $GITHUB_OUTPUT
            fi
          else
            echo "::notice::State drift detection only runs for ECS Fargate module"
            echo "drift_detected=skipped" >> $GITHUB_OUTPUT
          fi

      - name: Check for service key mismatches (pre-plan)
        id: check_service_keys
        if: |
          (inputs.action == 'plan' || inputs.action == 'apply') &&
          steps.check_services_empty.outputs.services_empty != 'true' &&
          inputs.module_path == '04-ecs-fargate'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          echo "::notice::Checking for service key mismatches between state and config..."
          
          # Initialize Terraform to access state
          cd "$PLAN_DIR"
          terraform init -no-color -input=false > /dev/null 2>&1 || true
          
          # Get services from state
          STATE_SERVICES=$(terraform state list -no-color 2>/dev/null | grep "aws_ecs_service.services\[" | sed 's/aws_ecs_service.services\["\(.*\)"\]/\1/' || echo "")
          
          # Get services from config (parse services.generated.tfvars)
          if [ -f "$PLAN_DIR/services.generated.tfvars" ]; then
            CONFIG_SERVICES=$(grep -E '^\s+"[^"]+::[^"]+"\s*=\s*\{' "$PLAN_DIR/services.generated.tfvars" | sed 's/^\s*"\([^"]*\)".*/\1/' || echo "")
          else
            CONFIG_SERVICES=""
          fi
          
          if [ -n "$STATE_SERVICES" ] && [ -n "$CONFIG_SERVICES" ]; then
            # Find services in state but not in config (will be destroyed)
            SERVICES_TO_DESTROY=""
            SERVICES_TO_CREATE=""
            SERVICES_TO_UPDATE=""
            
            for state_svc in $STATE_SERVICES; do
              if ! echo "$CONFIG_SERVICES" | grep -q "^$state_svc$"; then
                SERVICES_TO_DESTROY="$SERVICES_TO_DESTROY $state_svc"
              else
                SERVICES_TO_UPDATE="$SERVICES_TO_UPDATE $state_svc"
              fi
            done
            
            for config_svc in $CONFIG_SERVICES; do
              if ! echo "$STATE_SERVICES" | grep -q "^$config_svc$"; then
                SERVICES_TO_CREATE="$SERVICES_TO_CREATE $config_svc"
              fi
            done
            
            if [ -n "$SERVICES_TO_DESTROY" ]; then
              echo "::warning::‚ö†Ô∏è Service key mismatch detected!"
              echo "::warning::The following services in state will be DESTROYED (not in new config):"
              for svc in $SERVICES_TO_DESTROY; do
                echo "::warning::  ‚ùå $svc"
              done
              echo ""
              echo "::warning::Possible reasons:"
              echo "::warning::  ‚Ä¢ Service key changed (e.g., 'api' ‚Üí 'legacy::api')"
              echo "::warning::  ‚Ä¢ Service removed from YAML definitions"
              echo "::warning::  ‚Ä¢ Service renamed"
              echo ""
              echo "::warning::To preserve services, consider:"
              echo "::warning::  1. State migration: terraform state mv 'aws_ecs_service.services[\"old-key\"]' 'aws_ecs_service.services[\"new-key\"]'"
              echo "::warning::  2. Or add the service back to YAML if it was accidentally removed"
              echo "key_mismatch=true" >> $GITHUB_OUTPUT
            else
              echo "::notice::‚úì All services in state match config keys"
              echo "key_mismatch=false" >> $GITHUB_OUTPUT
            fi
            
            if [ -n "$SERVICES_TO_CREATE" ]; then
              echo "::notice::The following NEW services will be CREATED:"
              for svc in $SERVICES_TO_CREATE; do
                echo "::notice::  ‚ûï $svc"
              done
            fi
            
            if [ -n "$SERVICES_TO_UPDATE" ]; then
              echo "::notice::The following services will be UPDATED (if config changed):"
              for svc in $SERVICES_TO_UPDATE; do
                echo "::notice::  üîÑ $svc"
              done
            fi
          else
            echo "::notice::Cannot compare - state or config services list is empty"
            echo "key_mismatch=unknown" >> $GITHUB_OUTPUT
          fi

      - name: Pre-Plan State Validation Summary
        id: pre_plan_summary
        if: |
          (inputs.action == 'plan' || inputs.action == 'apply') &&
          inputs.module_path == '04-ecs-fargate' &&
          steps.check_services_empty.outputs.services_empty != 'true'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          echo "::notice::üìä State Validation Summary:"
          
          # Get expected service keys
          if [ ! -f "$PLAN_DIR/services.generated.tfvars" ]; then
            echo "::notice::No services.generated.tfvars found"
            exit 0
          fi
          
          EXPECTED_KEYS=$(grep -E '^\s+"[^"]+::[^"]+"\s*=\s*\{' "$PLAN_DIR/services.generated.tfvars" | \
            sed 's/^\s*"\([^"]*\)".*/\1/' || echo "")
          
          if [ -z "$EXPECTED_KEYS" ]; then
            echo "::notice::No service keys found in config"
            exit 0
          fi
          
          # Check which services are in state
          STATE_SERVICES=$(terraform -chdir="$PLAN_DIR" state list 2>/dev/null | \
            grep 'module.ecs_fargate.aws_service_discovery_service.services\["' | \
            sed 's/.*\["\(.*\)"\]/\1/' || echo "")
          
          STATE_ECS=$(terraform -chdir="$PLAN_DIR" state list 2>/dev/null | \
            grep 'module.ecs_fargate.aws_ecs_service.services\["' | \
            sed 's/.*\["\(.*\)"\]/\1/' || echo "")
          
          MISSING_SD=""
          MISSING_ECS=""
          
          for tf_key in $EXPECTED_KEYS; do
            if ! echo "$STATE_SERVICES" | grep -q "^$tf_key$"; then
              MISSING_SD="$MISSING_SD $tf_key"
            fi
            if ! echo "$STATE_ECS" | grep -q "^$tf_key$"; then
              MISSING_ECS="$MISSING_ECS $tf_key"
            fi
          done
          
          if [ -n "$MISSING_SD" ] || [ -n "$MISSING_ECS" ]; then
            if [ -n "$MISSING_SD" ]; then
              echo "::warning::‚ö†Ô∏è Service Discovery services NOT in state:"
              for svc in $MISSING_SD; do
                echo "::warning::  - $svc"
              done
            fi
            if [ -n "$MISSING_ECS" ]; then
              echo "::notice::‚ÑπÔ∏è ECS services NOT in state (will be created):"
              for svc in $MISSING_ECS; do
                echo "::notice::  - $svc"
              done
            fi
            echo "resources_missing_in_state=true" >> $GITHUB_OUTPUT
          else
            echo "::notice::‚úì All expected resources are in state"
            echo "resources_missing_in_state=false" >> $GITHUB_OUTPUT
          fi

      - name: Terraform Plan
        id: plan
        if: inputs.action == 'plan' || inputs.action == 'apply'
        uses: ./.github/actions/terraform-plan
        with:
          terraform_path: DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}
          var_files: ${{ steps.build_var_files.outputs.var_files }}
          plan_file: "tfplan"
        continue-on-error: true

      - name: Debug Plan - Show Replacement Reasons
        if: inputs.action == 'plan' || inputs.action == 'apply'
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          if [ -f "$PLAN_DIR/tfplan" ]; then
            echo "::group::Debug: Resources Being Replaced"
            echo "Checking for resources that need replacement..."
            terraform -chdir="$PLAN_DIR" show -no-color tfplan | grep -A 10 "must be replaced" || echo "No replacements detected"
            echo "::endgroup::"
            
            echo "::group::Debug: Detailed Plan Output"
            terraform -chdir="$PLAN_DIR" show -no-color tfplan || true
            echo "::endgroup::"
          else
            echo "::notice::Plan file not available for debugging"
          fi

      - name: Add Plan to Summary
        if: inputs.action == 'plan' || (inputs.action == 'apply' && steps.plan.outcome == 'success')
        run: |
          echo "## Terraform Plan Output" >> $GITHUB_STEP_SUMMARY
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          if [ -f "$PLAN_DIR/tfplan" ]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            terraform -chdir="$PLAN_DIR" show -no-color tfplan >> $GITHUB_STEP_SUMMARY 2>&1
            echo '```' >> $GITHUB_STEP_SUMMARY
            
            # Add replacement analysis to summary
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## üîç Replacement Analysis" >> $GITHUB_STEP_SUMMARY
            REPLACEMENTS=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 | grep -c "must be replaced" || echo "0")
            if [ "$REPLACEMENTS" -gt "0" ]; then
              echo "‚ö†Ô∏è **Warning:** $REPLACEMENTS resource(s) will be replaced" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "Resources being replaced:" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 | grep -B 2 -A 5 "must be replaced" >> $GITHUB_STEP_SUMMARY || true
              echo '```' >> $GITHUB_STEP_SUMMARY
            else
              echo "‚úÖ No replacements detected" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "Plan file not available (tfplan not found)" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check if plan has changes
        id: plan_changes
        if: inputs.action == 'apply' && steps.plan.outcome == 'success'
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          PLAN_OUTPUT=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1)
          
          if echo "$PLAN_OUTPUT" | grep -q "No changes"; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "::notice::No changes detected. Skipping apply."
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "::notice::Changes detected. Proceeding with apply."
            
            # Extract service changes from plan for visibility
            if echo "$PLAN_OUTPUT" | grep -q "aws_ecs_service.services\["; then
              echo ""
              echo "::notice::üìã Service Changes Detected:"
              
              CREATES=$(echo "$PLAN_OUTPUT" | grep -c "aws_ecs_service.services\[.*\] will be created" 2>/dev/null || echo "0")
              DESTROYS=$(echo "$PLAN_OUTPUT" | grep -c "aws_ecs_service.services\[.*\] will be destroyed" 2>/dev/null || echo "0")
              REPLACES=$(echo "$PLAN_OUTPUT" | grep -c "aws_ecs_service.services\[.*\] must be replaced" 2>/dev/null || echo "0")
              
              # Ensure we have valid integers (strip any whitespace/newlines and default to 0)
              CREATES=$(echo "$CREATES" | tr -d '[:space:]' || echo "0")
              DESTROYS=$(echo "$DESTROYS" | tr -d '[:space:]' || echo "0")
              REPLACES=$(echo "$REPLACES" | tr -d '[:space:]' || echo "0")
              
              if [ "${CREATES:-0}" -gt 0 ] 2>/dev/null; then
                echo "::notice::  ‚ûï Services to CREATE: $CREATES"
              fi
              
              if [ "${DESTROYS:-0}" -gt 0 ] 2>/dev/null; then
                echo "::warning::  ‚ùå Services to DESTROY: $DESTROYS"
                echo "::warning::‚ö†Ô∏è This may be due to service key changes (e.g., 'api' ‚Üí 'legacy::api')"
              fi
              
              if [ "${REPLACES:-0}" -gt 0 ] 2>/dev/null; then
                echo "::warning::  üîÑ Services to REPLACE: $REPLACES"
              fi
            fi
          fi

      - name: Save State Snapshot Before Apply
        id: save_state_snapshot
        if: inputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          echo "::notice::Creating state snapshot before apply for targeted cleanup..."
          
          # Get list of resources in state BEFORE apply
          STATE_BEFORE=$(terraform -chdir="$PLAN_DIR" state list -no-color 2>/dev/null || echo "")
          
          # Save to file for cleanup step
          echo "$STATE_BEFORE" > /tmp/state_before_apply.txt || true
          
          # Also get plan to see what will be created
          if [ -f "$PLAN_DIR/tfplan" ]; then
            PLAN_OUTPUT=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 || echo "")
            
            # Extract resources that will be created
            CREATED_RESOURCES=$(echo "$PLAN_OUTPUT" | grep "will be created" | \
              sed 's/^[[:space:]]*# //' | sed 's/ will be created$//' || echo "")
            
            echo "$CREATED_RESOURCES" > /tmp/resources_to_create.txt || true
          fi
          
          echo "state_snapshot_created=true" >> $GITHUB_OUTPUT

      - name: Idempotency Check Before Apply
        id: idempotency_check
        if: inputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          echo "::notice::üîç Performing idempotency checks before apply..."
          
          if [ ! -f "$PLAN_DIR/tfplan" ]; then
            echo "::warning::Plan file not found - skipping idempotency check"
            exit 0
          fi
          
          PLAN_OUTPUT=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 || echo "")
          
          # Get resources that plan wants to create
          RESOURCES_TO_CREATE=$(echo "$PLAN_OUTPUT" | grep "will be created" | \
            sed 's/^[[:space:]]*# //' | sed 's/ will be created$//' || echo "")
          
          # Get resources that plan wants to replace (critical for Service Discovery)
          RESOURCES_TO_REPLACE=$(echo "$PLAN_OUTPUT" | grep "must be replaced" | \
            sed 's/^[[:space:]]*# //' | sed 's/ must be replaced$//' || echo "")
          
          CONFLICTS=0
          
          # Check resources that will be created
          if [ -n "$RESOURCES_TO_CREATE" ]; then
            echo "::notice::Checking resources to be created..."
            while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              # Check if resource already exists in state
              if terraform -chdir="$PLAN_DIR" state show "$resource" >/dev/null 2>&1; then
                echo "::error::‚ùå Resource '$resource' is in state but plan wants to create it"
                echo "::error::This indicates state drift. Resource should be updated, not created."
                CONFLICTS=$((CONFLICTS + 1))
              fi
            done <<< "$RESOURCES_TO_CREATE"
          fi
          
          # Check resources that will be replaced (especially Service Discovery)
          if [ -n "$RESOURCES_TO_REPLACE" ]; then
            echo "::notice::Checking resources to be replaced..."
            while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              # For Service Discovery services, check if replacement already exists in AWS
              if echo "$resource" | grep -q "aws_service_discovery_service"; then
                # Extract service key from resource path
                SERVICE_KEY=$(echo "$resource" | sed 's/.*\["\(.*\)"\]/\1/')
                
                # Get namespace ID from state
                NAMESPACE_ID=$(terraform -chdir="$PLAN_DIR" state show 'module.ecs_fargate.aws_service_discovery_private_dns_namespace.this' 2>/dev/null | \
                  grep -E '^\s+id\s+=' | awk '{print $3}' | tr -d '"' || echo "")
                
                if [ -n "$NAMESPACE_ID" ]; then
                  # Get expected sanitized service name (part after ::)
                  EXPECTED_NAME=$(echo "$SERVICE_KEY" | sed 's/.*::\(.*\)/\1/' | tr '[:upper:]' '[:lower:]')
                  
                  # Check if service with this name already exists in AWS
                  EXISTING_SERVICE=$(aws servicediscovery list-services \
                    --filters Name=NAMESPACE_ID,Values="$NAMESPACE_ID" \
                    --query "Services[?Name=='$EXPECTED_NAME'].[Name,Id]" \
                    --output text 2>/dev/null || echo "")
                  
                  if [ -n "$EXISTING_SERVICE" ]; then
                    SERVICE_ID=$(echo "$EXISTING_SERVICE" | awk '{print $2}')
                    echo "::error::‚ùå Service Discovery service '$SERVICE_KEY' will be replaced"
                    echo "::error::   Replacement service '$EXPECTED_NAME' already exists in AWS (ID: $SERVICE_ID)"
                    echo "::error::   This will cause 'ServiceAlreadyExists' error during apply"
                    echo "::error::   The validation script should have imported this, but it may have failed"
                    echo "::error::   Consider running: terraform import '$resource' '$SERVICE_ID'"
                    CONFLICTS=$((CONFLICTS + 1))
                  fi
                fi
              fi
            done <<< "$RESOURCES_TO_REPLACE"
          fi
          
          if [ $CONFLICTS -gt 0 ]; then
            echo "::error::‚ùå Idempotency check failed: $CONFLICTS conflict(s) detected"
            echo "::error::Resources that will be created/replaced already exist in AWS"
            echo "::error::State validation should have caught this. Review state and plan."
            echo "::error::"
            echo "::error::To fix:"
            echo "::error::  1. Run the 'Comprehensive State Validation' step again"
            echo "::error::  2. Or manually import the resources: terraform import 'resource.address' 'aws-id'"
            exit 1
          else
            if [ -z "$RESOURCES_TO_CREATE" ] && [ -z "$RESOURCES_TO_REPLACE" ]; then
              echo "::notice::‚úì No resources to create or replace - idempotency check passed"
            else
              echo "::notice::‚úì Idempotency check passed - no conflicts detected"
            fi
          fi

      - name: Comprehensive Pre-Apply Validation
        id: comprehensive_validation
        if: inputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          if [ -f "scripts/comprehensive-pre-apply-validation.sh" ]; then
            chmod +x scripts/comprehensive-pre-apply-validation.sh
            scripts/comprehensive-pre-apply-validation.sh "$PLAN_DIR" "${{ inputs.environment }}" "${{ inputs.module_path }}" || {
              EXIT_CODE=$?
              if [ $EXIT_CODE -eq 1 ]; then
                echo "::error::Comprehensive validation found critical errors"
                echo "::error::Please review and fix the errors before proceeding"
                echo "validation_failed=true" >> $GITHUB_OUTPUT
              else
                echo "validation_failed=false" >> $GITHUB_OUTPUT
              fi
              exit 0  # Don't fail workflow, but set output
            }
            echo "validation_failed=false" >> $GITHUB_OUTPUT
          else
            echo "::warning::Comprehensive validation script not found, skipping"
            echo "validation_failed=false" >> $GITHUB_OUTPUT
          fi

      - name: Verify Plan is Still Valid
        id: verify_plan_stale
        if: inputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          echo "::notice::üîç Verifying plan is still valid (checking for state changes since plan)..."
          
          if [ ! -f "$PLAN_DIR/tfplan" ]; then
            echo "::error::Plan file not found - cannot verify"
            exit 1
          fi
          
          # Refresh state to check if anything changed
          # Use refresh-only to avoid making changes
          VAR_FILES="${{ steps.build_var_files.outputs.var_files }}"
          REFRESH_ARGS="-refresh-only -auto-approve -no-color"
          
          if [ -n "$VAR_FILES" ]; then
            for var_file in $VAR_FILES; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                REFRESH_ARGS="$REFRESH_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          REFRESH_OUTPUT=$(terraform -chdir="$PLAN_DIR" apply $REFRESH_ARGS 2>&1 || echo "")
          
          # Check if refresh detected any changes
          if echo "$REFRESH_OUTPUT" | grep -q "No changes"; then
            echo "::notice::‚úì Plan is still valid - no state changes detected"
            echo "plan_stale=false" >> $GITHUB_OUTPUT
          else
            echo "::error::‚ùå Plan is stale - state has changed since plan was created"
            echo "::error::State changes detected:"
            echo "$REFRESH_OUTPUT" | grep -E "(will be|must be|updated in-place)" | head -10 || true
            echo ""
            echo "::error::To fix:"
            echo "::error::  1. Re-run the workflow to create a fresh plan"
            echo "::error::  2. Or manually run: terraform plan"
            echo "plan_stale=true" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Terraform Apply
        id: apply
        if: |
          inputs.action == 'apply' && 
          steps.plan.outcome == 'success' && 
          steps.plan_changes.outputs.has_changes == 'true' &&
          (steps.verify_plan_stale.outcome == 'success' || steps.verify_plan_stale.outputs.plan_stale != 'true') &&
          steps.comprehensive_validation.outputs.validation_failed != 'true' &&
          steps.idempotency_check.outcome == 'success'
        uses: ./.github/actions/terraform-apply
        with:
          terraform_path: DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}
          plan_file: "tfplan"
          auto_approve: "true"

      - name: Verify Target Group Health Check Paths (Post-Apply)
        id: verify_health_checks
        if: inputs.action == 'apply' && steps.apply.outcome == 'success' && inputs.module_path == '04-ecs-fargate'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          echo "::notice::Verifying target group health check paths after apply..."
          
          if [ -f "scripts/verify-target-group-health-checks.sh" ]; then
            chmod +x scripts/verify-target-group-health-checks.sh
            export TERRAFORM_DIR="$PLAN_DIR"
            export ENVIRONMENT="${{ inputs.environment }}"
            
            if scripts/verify-target-group-health-checks.sh; then
              echo "::notice::‚úì All target group health check paths verified successfully"
              echo "verification_passed=true" >> $GITHUB_OUTPUT
            else
              echo "::warning::‚ö† State drift detected after apply. Target groups may not match configuration."
              echo "verification_passed=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "::notice::Verification script not found, skipping..."
            echo "verification_passed=skipped" >> $GITHUB_OUTPUT
          fi

      - name: Targeted Cleanup on Apply Failure
        if: inputs.action == 'apply' && steps.apply.outcome == 'failure' && inputs.environment != 'production'
        continue-on-error: true
        run: |
          echo "::error::Terraform apply failed. Attempting targeted cleanup of resources created in this run..."
          
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          
          # Get state before apply
          STATE_BEFORE=""
          if [ -f /tmp/state_before_apply.txt ]; then
            STATE_BEFORE=$(cat /tmp/state_before_apply.txt)
          fi
          
          # Get current state
          STATE_AFTER=$(terraform -chdir="$PLAN_DIR" state list -no-color 2>/dev/null || echo "")
          
          # Find resources that were created in this run (in state_after but not in state_before)
          NEWLY_CREATED=""
          if [ -n "$STATE_AFTER" ]; then
            while IFS= read -r resource; do
              if [ -z "$STATE_BEFORE" ] || ! echo "$STATE_BEFORE" | grep -q "^${resource}$"; then
                NEWLY_CREATED="${NEWLY_CREATED}${resource}"$'\n'
              fi
            done <<< "$STATE_AFTER"
          fi
          
          # Also check plan file for resources that were supposed to be created
          PLANNED_CREATES=""
          if [ -f /tmp/resources_to_create.txt ]; then
            PLANNED_CREATES=$(cat /tmp/resources_to_create.txt)
          fi
          
          if [ -z "$NEWLY_CREATED" ] && [ -z "$PLANNED_CREATES" ]; then
            echo "::notice::No new resources were created in this run. No cleanup needed."
            exit 0
          fi
          
          echo "::notice::Resources created in this failed run:"
          if [ -n "$NEWLY_CREATED" ]; then
            echo "$NEWLY_CREATED" | while read -r resource; do
              [ -n "$resource" ] && echo "::notice::  - $resource"
            done
          fi
          
          # Build destroy command with var files
          DESTROY_ARGS="-auto-approve"
          VAR_FILES="${{ steps.build_var_files.outputs.var_files }}"
          if [ -n "$VAR_FILES" ]; then
            for var_file in $VAR_FILES; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                DESTROY_ARGS="$DESTROY_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          # For targeted cleanup, we'll use terraform destroy but with a filter
          # Since Terraform doesn't support selective destroy easily, we'll:
          # 1. Try to remove newly created resources from state (if they're truly orphaned)
          # 2. Or destroy only if it's safe (non-production, and resources are clearly orphaned)
          
          echo "::notice::Attempting targeted cleanup..."
          
          # Option 1: Try to remove orphaned resources from state (if they don't exist in AWS)
          REMOVED_FROM_STATE=0
          if [ -n "$NEWLY_CREATED" ]; then
            echo "$NEWLY_CREATED" | while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              # Check if resource actually exists in AWS by trying to refresh it
              REFRESH_OUTPUT=$(terraform -chdir="$PLAN_DIR" state show "$resource" 2>&1 || echo "")
              if echo "$REFRESH_OUTPUT" | grep -q "doesn't exist" || echo "$REFRESH_OUTPUT" | grep -q "not found"; then
                echo "::notice::Removing orphaned resource from state: $resource"
                terraform -chdir="$PLAN_DIR" state rm "$resource" 2>&1 || true
                REMOVED_FROM_STATE=$((REMOVED_FROM_STATE + 1))
              fi
            done
          fi
          
          # Option 2: If resources exist in AWS, we need to destroy them properly
          # But only if they're truly orphaned (not part of working infrastructure)
          if [ -n "$NEWLY_CREATED" ] && [ "$REMOVED_FROM_STATE" -eq 0 ]; then
            echo "::warning::Resources exist in AWS. Checking if they're safe to destroy..."
            
            # For ECS Fargate, be very careful - only destroy if they're clearly failed
            # Check if any ECS services are in failed state
            FAILED_SERVICES=""
            while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              if echo "$resource" | grep -q "aws_ecs_service"; then
                # Extract service name and check status
                SERVICE_NAME=$(echo "$resource" | sed 's/.*\["\(.*\)"\]/\1/')
                # Service might be in failed state - check AWS
                # For now, we'll be conservative and not auto-destroy ECS services
                echo "::warning::ECS service detected: $SERVICE_NAME - manual cleanup recommended"
              fi
            done <<< "$NEWLY_CREATED"
            
            # For non-critical resources (like Service Discovery), we can be more aggressive
            echo "::notice::For safety, manual cleanup is recommended for resources created in this run"
            echo "::notice::Review the resources listed above and destroy manually if needed"
          fi
          
          # Cleanup temp files
          rm -f /tmp/state_before_apply.txt /tmp/resources_to_create.txt
          
          echo "::notice::Targeted cleanup analysis completed"

      - name: Production apply failure notification
        if: inputs.action == 'apply' && steps.apply.outcome == 'failure' && inputs.environment == 'production'
        run: |
          echo "::error::Terraform apply failed in PRODUCTION environment."
          echo "::error::Automatic cleanup is disabled for production. Manual intervention required."
          echo "::error::Please review the Terraform state and decide on appropriate action:"
          echo "::error::  1. Review what resources were created before the failure"
          echo "::error::  2. Manually destroy partially created resources if needed"
          echo "::error::  3. Fix the issue and retry the deployment"
          exit 1

      - name: No Changes Detected
        if: inputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'false'
        run: |
          echo "::notice::No changes to apply. Infrastructure is already up to date."
          echo "## ‚úÖ No Changes Required" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The Terraform plan showed no changes. Your infrastructure matches the configuration." >> $GITHUB_STEP_SUMMARY

      - name: Plan Failed
        if: inputs.action == 'apply' && steps.plan.outcome == 'failure'
        run: |
          echo "::error::Terraform plan failed. Apply cannot proceed."
          exit 1

      - name: Terraform Destroy (Production Protection)
        if: inputs.action == 'destroy' && inputs.environment == 'production'
        run: |
          echo "::error::Destroy action is blocked for production environment for safety."
          exit 1

      - name: Build var files list for destroy
        id: build_var_files_destroy
        if: inputs.action == 'destroy' && inputs.environment != 'production'
        run: |
          VAR_FILES=""
          if [ "${{ steps.check_tfvars.outputs.tfvars_exists }}" == "true" ]; then
            VAR_FILES="terraform.tfvars"
          fi
          if [ "${{ steps.check_services_tfvars.outputs.services_tfvars_exists }}" == "true" ]; then
            if [ -n "$VAR_FILES" ]; then
              VAR_FILES="$VAR_FILES services.generated.tfvars"
            else
              VAR_FILES="services.generated.tfvars"
            fi
          fi
          echo "var_files=$VAR_FILES" >> $GITHUB_OUTPUT

      - name: Terraform Destroy
        if: inputs.action == 'destroy' && inputs.environment != 'production'
        run: |
          PLAN_DIR="DEVOPS/live/${{ inputs.environment }}/${{ inputs.module_path }}"
          DESTROY_ARGS="-auto-approve"
          if [ -n "${{ steps.build_var_files_destroy.outputs.var_files }}" ]; then
            for var_file in ${{ steps.build_var_files_destroy.outputs.var_files }}; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                DESTROY_ARGS="$DESTROY_ARGS -var-file=$var_file"
              fi
            done
          fi
          terraform -chdir="$PLAN_DIR" destroy $DESTROY_ARGS
