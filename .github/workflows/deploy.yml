name: Unified Deployment Pipeline (deploy)

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target Environment'
        required: true
        default: 'dev'
        type: choice
        options: [dev, staging, production]
      module_path:
        description: 'Terraform module to deploy (e.g., 01-vpc, 04-ecs-fargate)'
        required: true
        type: string
      action:
        description: 'Terraform action (plan, apply, or destroy)'
        required: true
        default: 'plan'
        type: choice
        options: [plan, apply, destroy]
      update_images:
        description: 'Update ECS service images (true) or do infra-only (false). Applies only to 04-ecs-fargate.'
        required: false
        type: boolean
        default: true
      image_tag:
        description: 'Docker image tag for app deployments (optional, commit SHA)'
        required: false
        type: string
      application:
        description: 'Application to deploy (for app deployments, e.g., "test-app")'
        required: false
        default: 'all'
        type: string

  workflow_run:
    workflows: ["CI/CD Pipeline"]
    branches: [main]
    types:
      - completed
    # Note: workflow_run triggers on ANY completion (success/failure/cancelled)
    # We check the conclusion inside the job and exit early if CI failed
    # This ensures we only deploy when images are actually built

concurrency:
  # Use dynamic group based on trigger type - supports both workflow_dispatch and workflow_run
  group: deploy-${{ github.event_name == 'workflow_dispatch' && github.event.inputs.environment || 'dev' }}-${{ github.event_name == 'workflow_dispatch' && github.event.inputs.module_path || '04-ecs-fargate' }}
  cancel-in-progress: false  # Keep false for resilience - don't cancel in-progress deployments

jobs:
  deploy:
    name: ${{ github.event_name == 'workflow_dispatch' && format('{0} - {1}', github.event.inputs.action, github.event.inputs.module_path) || 'Deploy Application' }}
    runs-on: ubuntu-latest
    # Only run if CI workflow succeeded (for workflow_run triggers)
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
    
    permissions:
      id-token: write
      contents: read
      actions: read
      packages: read

    environment:
      name: ${{ github.event.inputs.environment || 'dev' }}

    steps:
      - name: Checkout CI repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        if: |
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.action != 'destroy'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install uv
        if: |
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.action != 'destroy'
        uses: astral-sh/setup-uv@v6
        with:
          version: "0.8.7"

      - name: Cache uv
        if: |
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.action != 'destroy'
        uses: actions/cache@v4
        with:
          path: ~/.cache/uv
          key: ${{ runner.os }}-uv-${{ hashFiles('**/uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Install backend dependencies
        if: |
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.action != 'destroy'
        working-directory: applications/test-app/backend
        run: |
          uv sync --frozen --extra dev

      - name: Run backend tests
        if: |
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.action != 'destroy'
        working-directory: applications/test-app/backend
        run: |
          uv run python -m pytest tests/test_main.py

      - name: Set up Node
        if: |
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.action != 'destroy'
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: 'npm'
          cache-dependency-path: applications/test-app/frontend/package-lock.json

      - name: Install frontend dependencies
        if: |
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.action != 'destroy'
        working-directory: applications/test-app/frontend
        run: npm ci

      - name: Run frontend tests
        if: |
          github.event_name == 'workflow_dispatch' &&
          github.event.inputs.action != 'destroy'
        working-directory: applications/test-app/frontend
        env:
          CI: "true"
        run: npm run test:ci

      - name: Verify CI workflow succeeded (skip if failed/cancelled)
        if: github.event_name == 'workflow_run'
        run: |
          CI_WORKFLOW_STATUS="${{ github.event.workflow_run.conclusion }}"
          CI_WORKFLOW_ID="${{ github.event.workflow_run.id }}"
          echo "::notice::CI workflow ID: $CI_WORKFLOW_ID"
          echo "::notice::CI workflow conclusion: $CI_WORKFLOW_STATUS"
          if [ "$CI_WORKFLOW_STATUS" != "success" ]; then
            echo "::warning::CI workflow status: $CI_WORKFLOW_STATUS - skipping deployment"
            echo "::warning::Deployment only runs when CI workflow succeeds"
            echo "::warning::CI workflow URL: https://github.com/${{ github.repository }}/actions/runs/$CI_WORKFLOW_ID"
            # Exit with code 0 to mark job as successful (but skipped)
            # This prevents the workflow from showing as failed when CI fails
            exit 0
          fi
          echo "::notice::‚úì CI workflow succeeded - proceeding with deployment"

      - name: Download artifacts from CI workflow
        if: |
          github.event_name == 'workflow_run' &&
          github.event.workflow_run.conclusion == 'success'
        uses: actions/download-artifact@v4
        with:
          name: build-version
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}
          path: .
        continue-on-error: true

      - name: Download built images list from CI workflow
        if: |
          github.event_name == 'workflow_run' &&
          github.event.workflow_run.conclusion == 'success'
        uses: actions/download-artifact@v4
        with:
          name: built-images
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}
          path: .
        continue-on-error: true

      - name: Check if images were built (skip deployment if not)
        if: |
          github.event_name == 'workflow_run' &&
          github.event.workflow_run.conclusion == 'success'
        id: check_images_built
        run: |
          CI_WORKFLOW_ID="${{ github.event.workflow_run.id }}"
          
          # Check if build-version.txt exists (proves images were built)
          if [ ! -f "build-version.txt" ]; then
            echo "::notice::build-version.txt artifact not found from CI workflow"
            echo "::notice::This means no images were built (likely only workflow/config files changed)"
            echo "::notice::Skipping deployment - no ECS resources to deploy"
            echo "::notice::CI workflow run: https://github.com/${{ github.repository }}/actions/runs/$CI_WORKFLOW_ID"
            echo "images_built=false" >> $GITHUB_OUTPUT
            exit 0  # Exit successfully - this is expected, not an error
          fi
          
          # Also check if built-images.txt has any actual images
          if [ -f "built-images.txt" ]; then
            IMAGE_COUNT=$(grep -v '^#' built-images.txt | grep -v '^$' | wc -l | tr -d ' ')
            if [ "$IMAGE_COUNT" -eq 0 ]; then
              echo "::notice::built-images.txt is empty - no images were built"
              echo "::notice::Skipping deployment - no ECS resources to deploy"
              echo "images_built=false" >> $GITHUB_OUTPUT
              exit 0  # Exit successfully - this is expected
            fi
          else
            # built-images.txt missing - assume no images built
            echo "::notice::built-images.txt not found - assuming no images were built"
            echo "::notice::Skipping deployment - no ECS resources to deploy"
            echo "images_built=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          BUILD_VERSION=$(cat build-version.txt)
          echo "::notice::‚úì Build version artifact found: $BUILD_VERSION"
          echo "::notice::‚úì Images were built successfully in CI workflow"
          echo "images_built=true" >> $GITHUB_OUTPUT

      - name: Set up dynamic variables
        if: |
          github.event_name == 'workflow_dispatch' ||
          (github.event_name == 'workflow_run' && steps.check_images_built.outputs.images_built == 'true')
        id: vars
        run: |
          # This step only runs if:
          # - Manual trigger (workflow_dispatch), OR
          # - Automatic trigger (workflow_run) AND images were built
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "::notice::Manual workflow dispatch trigger"
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
            echo "module_path=${{ github.event.inputs.module_path }}" >> $GITHUB_OUTPUT
            echo "action=${{ github.event.inputs.action }}" >> $GITHUB_OUTPUT
            # Whether to update images or run infra-only (ECS module only)
            UPDATE_IMAGES_INPUT="${{ github.event.inputs.update_images }}"
            if [ -z "$UPDATE_IMAGES_INPUT" ] || [ "$UPDATE_IMAGES_INPUT" = "null" ]; then
              UPDATE_IMAGES_INPUT="true"
            fi
            echo "update_images=$UPDATE_IMAGES_INPUT" >> $GITHUB_OUTPUT
            # Use provided image_tag or fallback to generated tag (idempotent - same commit = same tag)
            # For manual dispatch, generate tag format matching CI workflow logic (dynamic, scalable)
            IMAGE_TAG="${{ github.event.inputs.image_tag }}"
            if [ "$UPDATE_IMAGES_INPUT" = "true" ]; then
              if [ -z "$IMAGE_TAG" ] || [ "$IMAGE_TAG" = "" ]; then
                # Generate tag format matching CI workflow logic (supports tags, main, and other branches)
                SHORT_SHA=$(git rev-parse --short HEAD)
                if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
                  IMAGE_TAG="${{ github.ref_name }}"
                elif [[ "${{ github.ref }}" == refs/heads/main ]]; then
                  IMAGE_TAG="main-${SHORT_SHA}"
                else
                  IMAGE_TAG="dev-${SHORT_SHA}"
                fi
                echo "::notice::Generated image tag: $IMAGE_TAG"
              fi
              echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
            else
              # Infra-only mode: image_tag is not required/used
              echo "image_tag=" >> $GITHUB_OUTPUT
            fi
            # Use provided application or default to 'all' (scalable - supports multi-app)
            APPLICATION="${{ github.event.inputs.application }}"
            if [ -z "$APPLICATION" ] || [ "$APPLICATION" = "" ]; then
              APPLICATION="all"
            fi
            echo "application=$APPLICATION" >> $GITHUB_OUTPUT
          else
            echo "::notice::Automated workflow run trigger"
            echo "environment=dev" >> $GITHUB_OUTPUT # Or determine dynamically if needed
            echo "module_path=04-ecs-fargate" >> $GITHUB_OUTPUT
            echo "action=apply" >> $GITHUB_OUTPUT
            echo "update_images=true" >> $GITHUB_OUTPUT
            # Use build version from CI workflow artifact (single source of truth)
            # This ensures tag format always matches what CI built
            if [ -f "build-version.txt" ]; then
              IMAGE_TAG=$(cat build-version.txt)
              echo "::notice::Using build version from CI workflow: $IMAGE_TAG"
            else
              # Fallback: Generate tag format matching CI workflow logic (dynamic, scalable)
              SHORT_SHA=$(git rev-parse --short HEAD)
              if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
                IMAGE_TAG="${{ github.ref_name }}"
              elif [[ "${{ github.ref }}" == refs/heads/main ]]; then
                IMAGE_TAG="main-${SHORT_SHA}"
              else
                IMAGE_TAG="dev-${SHORT_SHA}"
              fi
              echo "::notice::Generated image tag (fallback): $IMAGE_TAG"
            fi
            echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
            echo "application=all" >> $GITHUB_OUTPUT
          fi

      - name: Debug - Check repository secrets
        run: |
          echo "::notice::Checking repository secrets..."
          DEVOPS_REPO_OWNER="${{ secrets.DEVOPS_REPO_OWNER }}"
          DEVOPS_REPO_NAME="${{ secrets.DEVOPS_REPO_NAME }}"
          
          if [ -n "$DEVOPS_REPO_OWNER" ]; then
            echo "DEVOPS_REPO_OWNER exists: yes"
            echo "DEVOPS_REPO_OWNER length: ${#DEVOPS_REPO_OWNER}"
          else
            echo "DEVOPS_REPO_OWNER exists: no"
            echo "DEVOPS_REPO_OWNER length: 0"
          fi
          
          if [ -n "$DEVOPS_REPO_NAME" ]; then
            echo "DEVOPS_REPO_NAME exists: yes"
            echo "DEVOPS_REPO_NAME length: ${#DEVOPS_REPO_NAME}"
          else
            echo "DEVOPS_REPO_NAME exists: no"
            echo "DEVOPS_REPO_NAME length: 0"
          fi
          
          REPO_OWNER="${DEVOPS_REPO_OWNER:-orshalit}"
          REPO_NAME="${DEVOPS_REPO_NAME:-projectdevops}"
          REPO_FORMAT="${REPO_OWNER}/${REPO_NAME}"
          echo "Computed repo format: $REPO_FORMAT"
          
          if [ -z "$DEVOPS_REPO_OWNER" ] || [ -z "$DEVOPS_REPO_NAME" ]; then
            echo "::error::Repository secrets are missing or empty!"
            echo "::error::DEVOPS_REPO_OWNER: '${DEVOPS_REPO_OWNER:-<empty>}'"
            echo "::error::DEVOPS_REPO_NAME: '${DEVOPS_REPO_NAME:-<empty>}'"
            echo "::error::Please set DEVOPS_REPO_OWNER and DEVOPS_REPO_NAME secrets in repository settings"
            exit 1
          fi

      - name: Terraform Setup
        uses: ./.github/actions/terraform-setup
        with:
          devops_repo: ${{ secrets.DEVOPS_REPO_OWNER || 'orshalit' }}/${{ secrets.DEVOPS_REPO_NAME || 'projectdevops' }}
          devops_repo_key: ${{ secrets.DEVOPS_REPO_KEY }}
          aws_role_arn: ${{ secrets.AWS_ROLE_ARN }}
          aws_region: ${{ secrets.AWS_REGION }}
          terraform_version: '1.6.0'
        env:
          DEVOPS_REPO_OWNER: ${{ secrets.DEVOPS_REPO_OWNER || 'orshalit' }}
          DEVOPS_REPO_NAME: ${{ secrets.DEVOPS_REPO_NAME || 'projectdevops' }}

      - name: Cache Dhall binaries
        id: cache-dhall
        uses: actions/cache@v4
        with:
          path: dhall/cache/binaries
          key: dhall-binaries-${{ runner.os }}-1.41.2
          restore-keys: |
            dhall-binaries-${{ runner.os }}-

      - name: Update Dhall imports to use local DEVOPS checkout
        run: |
          echo "::notice::DEVOPS repository is checked out to ./DEVOPS"
          echo "::notice::Verifying Dhall imports resolve to local DEVOPS checkout..."
          
          # Verify DEVOPS checkout
          if [ ! -f "DEVOPS/config/types/Service.dhall" ]; then
            echo "::error::DEVOPS/config/types/Service.dhall not found!"
            echo "::error::DEVOPS checkout may have failed"
            ls -la DEVOPS/ || echo "DEVOPS directory not found"
            exit 1
          fi
          echo "‚úì DEVOPS/config/types/Service.dhall found"
          echo "‚úì Dhall imports rely on relative paths; no rewrite needed."

      - name: Install Dhall, dhall-json, and jq
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          chmod +x scripts/install-dhall-with-fallback.sh
          scripts/install-dhall-with-fallback.sh

      - name: Generate services.generated.json from Dhall (Terraform JSON format)
        id: generate_services_json
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/04-ecs-fargate"
          SERVICES_JSON="$PLAN_DIR/services.generated.json"
          SERVICES_TFVARS_JSON_DHALL="dhall/services.tfvarsJSON.dhall"
          
          echo "::notice::Generating services.generated.json from Dhall (Terraform JSON format)..."
          
          # Check if Dhall converter exists
          if [ ! -f "$SERVICES_TFVARS_JSON_DHALL" ]; then
            echo "::error::Dhall JSON converter not found at $SERVICES_TFVARS_JSON_DHALL"
            echo "::error::This file should exist in the CI repository"
            echo "::error::Ensure the CI repository has been checked out and contains Dhall service definitions."
            exit 1
          fi
          
          # Generate Terraform-compatible JSON (with "services" key)
          # Uses dhall-to-json for native JSON output - no string templates!
          mkdir -p "$PLAN_DIR"
          echo "::notice::Converting Dhall to Terraform JSON format (validates types automatically)..."
          dhall-to-json --file "$SERVICES_TFVARS_JSON_DHALL" > "$SERVICES_JSON" || {
            echo "::error::Failed to generate Terraform JSON from Dhall"
            echo "::error::Checking Dhall file..."
            dhall type --file "$SERVICES_TFVARS_JSON_DHALL" || true
            exit 1
          }
          
          if [ ! -f "$SERVICES_JSON" ] || [ ! -s "$SERVICES_JSON" ]; then
            echo "::error::Failed to generate services.generated.json"
            echo "::error::Check the Dhall file for syntax or type errors"
            exit 1
          fi
          
          # Validate JSON structure
          if ! jq empty "$SERVICES_JSON" 2>/dev/null; then
            echo "::error::Generated JSON is invalid"
            cat "$SERVICES_JSON"
            exit 1
          fi
          
          echo "::notice::‚úì Generated $SERVICES_JSON (Terraform JSON format)"
          echo "::group::Generated services.generated.json (preview)"
          cat "$SERVICES_JSON" | jq '.services | keys | length' | xargs -I {} echo "Services count: {}"
          cat "$SERVICES_JSON" | jq '.services | keys' | head -20
          echo "::endgroup::"
          echo "services_json_generated=true" >> $GITHUB_OUTPUT

      - name: Generate Terraform Config from Dhall
        id: generate_config
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply' || steps.vars.outputs.action == 'destroy'
        run: |
          ENV="${{ steps.vars.outputs.environment }}"
          MODULE_PATH="${{ steps.vars.outputs.module_path }}"
          MODULE_NAME=$(basename "$MODULE_PATH")
          OUTPUT_FILE="DEVOPS/live/${ENV}/${MODULE_PATH}/terraform.tfvars.json"

          echo "::notice::Generating config for ${ENV}/${MODULE_NAME}..."

          # Check if module-specific converter exists (preferred method)
          MODULE_TFVARS_DHALL="DEVOPS/config/environments/${ENV}/${MODULE_NAME}.tfvars.dhall"
          if [ -f "$MODULE_TFVARS_DHALL" ]; then
            echo "::notice::Using module-specific converter: $MODULE_TFVARS_DHALL"
            dhall-to-json --file "$MODULE_TFVARS_DHALL" > "${OUTPUT_FILE}" || {
              echo "::error::Failed to convert $MODULE_TFVARS_DHALL to JSON"
              exit 1
            }
          else
            echo "::notice::Module-specific converter not found, extracting from dev.dhall..."
            # Fallback: Extract module config directly from dev.dhall JSON
            ENV_JSON=$(dhall-to-json --file DEVOPS/config/environments/${ENV}.dhall) || {
              echo "::error::Failed to convert DEVOPS/config/environments/${ENV}.dhall to JSON"
              exit 1
            }
            
            # Check if module exists
            MODULE_EXISTS=$(echo "$ENV_JSON" | jq -r "has(\"${MODULE_NAME}\")")
            if [ "$MODULE_EXISTS" != "true" ]; then
              echo "::error::Module '${MODULE_NAME}' not found in DEVOPS/config/environments/${ENV}.dhall"
              echo "::error::Available modules:"
              echo "$ENV_JSON" | jq -r 'keys[]' | sed 's/^/  - /' || true
              exit 1
            fi
            
            # Extract module config
            MODULE_CONFIG=$(echo "$ENV_JSON" | jq -c ".[\"${MODULE_NAME}\"]")
            
            # Validate that we got a valid object
            if [ -z "$MODULE_CONFIG" ] || [ "$MODULE_CONFIG" = "null" ]; then
              echo "::error::Module '${MODULE_NAME}' exists but has null value"
              exit 1
            fi
            
            echo "$MODULE_CONFIG" > "${OUTPUT_FILE}"
          fi
          
          # Validate the output file is valid JSON
          if ! jq empty "${OUTPUT_FILE}" 2>/dev/null; then
            echo "::error::Generated terraform.tfvars.json is not valid JSON"
            cat "${OUTPUT_FILE}"
            exit 1
          fi

          echo "::notice::‚úì Generated ${OUTPUT_FILE}"
          echo "::group::Generated terraform.tfvars.json"
          cat "${OUTPUT_FILE}"
          echo "::endgroup::"
          echo "var_file_path=${OUTPUT_FILE}" >> $GITHUB_OUTPUT

      - name: Terraform Format Check and Auto-Fix
        id: fmt
        if: |
          steps.vars.outputs.environment != '' &&
          steps.vars.outputs.module_path != '' &&
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply' || steps.vars.outputs.action == 'destroy')
        run: |
          TERRAFORM_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Checking Terraform formatting in $TERRAFORM_DIR..."
          
          # First, check formatting without modifying files (faster feedback)
          if terraform fmt -check -recursive "$TERRAFORM_DIR" 2>&1; then
            echo "::notice::‚úì All Terraform files are properly formatted"
            echo "needs_formatting=false" >> $GITHUB_OUTPUT
            echo "changes_made=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "::warning::Formatting issues detected. Auto-formatting files..."
          
          # Format all Terraform files
          terraform fmt -recursive "$TERRAFORM_DIR"
          
          # Check if any files were modified by comparing before/after
          cd DEVOPS
          
          # Check git status for changes in the terraform directory
          if git diff --quiet "live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"; then
            echo "::notice::‚úì Files were auto-formatted and are now correct"
            echo "needs_formatting=false" >> $GITHUB_OUTPUT
            echo "changes_made=false" >> $GITHUB_OUTPUT
          else
            echo "needs_formatting=true" >> $GITHUB_OUTPUT
            echo "changes_made=true" >> $GITHUB_OUTPUT
            
            # Show what changed
            echo "::notice::‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
            echo "::notice::‚ïë  üîß AUTO-FORMATTING TERRAFORM FILES  üîß  ‚ïë"
            echo "::notice::‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
            echo ""
            echo "::notice::The following files were auto-formatted:"
            git status --short "live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}" | while read -r line; do
              echo "::notice::  ‚Ä¢ $line"
            done
            echo ""
            echo "::group::Show formatted changes"
            git diff "live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
            echo "::endgroup::"
          fi

      - name: Note formatting changes (no commit during deploy)
        if: steps.fmt.outputs.needs_formatting == 'true' && steps.fmt.outputs.changes_made == 'true'
        run: |
          echo "::notice::Files were auto-formatted but NOT committed during deployment"
          echo "::notice::This prevents stale plans caused by state changes from commits"
          echo "::notice::To commit formatting:"
          echo "::notice::  1. Use the 'Terraform Format Check' workflow with auto-fix enabled"
          echo "::notice::  2. Or run 'terraform fmt -recursive' locally and commit manually"

      - name: Continue deployment (formatting fixed)
        if: steps.fmt.outputs.needs_formatting == 'true' && steps.fmt.outputs.changes_made == 'true'
        run: |
          echo "::notice::‚úì Terraform formatting issues have been auto-fixed"
          echo "::notice::Deployment will continue with properly formatted files"

      - name: Verify DEVOPS Repository Version
        id: verify_devops
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        run: |
          cd DEVOPS
          echo "::notice::DEVOPS Repository Information:"
          echo "::notice::  Branch: $(git rev-parse --abbrev-ref HEAD)"
          echo "::notice::  Commit: $(git rev-parse HEAD)"
          echo "::notice::  Date: $(git log -1 --format=%ci)"
          echo "::notice::  Message: $(git log -1 --format=%s)"
          
          # Store commit hash for reference
          echo "commit_hash=$(git rev-parse HEAD)" >> $GITHUB_OUTPUT
          
          # Verify we're on main branch
          CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)
          if [ "$CURRENT_BRANCH" != "main" ]; then
            echo "::warning::DEVOPS repo is on branch '$CURRENT_BRANCH', not 'main'"
          fi
          
          # Check if repo is clean (no uncommitted changes)
          if ! git diff-index --quiet HEAD --; then
            echo "::warning::DEVOPS repo has uncommitted changes"
            git status --short
          fi

      - name: Terraform Init
        id: init
        run: terraform -chdir=DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }} init -upgrade

      - name: Validate Variable Declarations
        id: validate_vars
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          VAR_FILE="$PLAN_DIR/variables.tf"
          
          echo "::notice::Validating variable declarations for ECS Fargate module..."
          
          # Check if variables.tf exists
          if [ ! -f "$VAR_FILE" ]; then
            echo "::error::variables.tf not found at $VAR_FILE"
            exit 1
          fi
          
          # This workflow no longer injects global_image_tag for 04-ecs-fargate.
          # Image tags are resolved per-service by patching services.generated.json before plan/apply.
          REQUIRED_VAR="services"
          
          echo "::notice::Checking for required variable declaration: $REQUIRED_VAR"
          
          # Check if variable is declared in variables.tf
          if ! grep -qE "^\s*variable\s+\"$REQUIRED_VAR\"" "$VAR_FILE"; then
            echo "::error::‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
            echo "::error::‚ïë  ‚ö†Ô∏è  MISSING VARIABLE DECLARATION  ‚ö†Ô∏è  ‚ïë"
            echo "::error::‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
            echo ""
            echo "::error::Variable '$REQUIRED_VAR' is required (services.generated.json is passed to Terraform) but is not declared in:"
            echo "::error::  $VAR_FILE"
            echo ""
            echo "::error::DEVOPS Repository Commit: ${{ steps.verify_devops.outputs.commit_hash }}"
            echo ""
            echo "::error::Current variables.tf content (first 50 lines):"
            head -50 "$VAR_FILE" | sed 's/^/::error::  /' || true
            echo ""
            echo "::error::Solution:"
            echo "::error::  1. Ensure DEVOPS repository has the latest code with variable declaration"
            echo "::error::  2. Add variable declaration to $VAR_FILE:"
            echo "::error::     variable \"$REQUIRED_VAR\" { ... }"
            echo "::error::  3. Commit and push to DEVOPS repository"
            echo "::error::  4. Re-run this workflow"
            exit 1
          else
            echo "::notice::‚úì Variable '$REQUIRED_VAR' is properly declared in $VAR_FILE"
          fi

      - name: Check for terraform.tfvars
        id: check_tfvars
        run: |
          if [ -f "DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}/terraform.tfvars" ]; then
            echo "tfvars_exists=true" >> $GITHUB_OUTPUT
          else
            echo "tfvars_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Check for services.generated.json
        id: check_services_json
        run: |
          if [ -f "DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}/services.generated.json" ]; then
            echo "services_json_exists=true" >> $GITHUB_OUTPUT
          else
            echo "services_json_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Resolve per-service image tags + verify updated images (ECS)
        id: ecs_image_tags
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate'
        uses: ./.github/actions/ecs-service-image-tags
        with:
          plan_dir: DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}
          services_json: DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}/services.generated.json
          event_name: ${{ github.event_name }}
          application: ${{ steps.vars.outputs.application }}
          update_images: ${{ steps.vars.outputs.update_images }}
          desired_tag: ${{ steps.vars.outputs.image_tag }}
          built_images_file: built-images.txt
          verify_images: 'true'
          github_token: ${{ secrets.GITHUB_TOKEN }}

      - name: Check if services is empty
        id: check_services_empty
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          # Check if services.generated.json exists and has non-empty services
          if [ -f "$PLAN_DIR/services.generated.json" ]; then
            # Check if services map is empty using jq
            SERVICE_COUNT=$(jq '.services | length' "$PLAN_DIR/services.generated.json" 2>/dev/null || echo "0")
            if [ "$SERVICE_COUNT" -eq 0 ]; then
              echo "services_empty=true" >> $GITHUB_OUTPUT
              echo "::notice::Services map is empty, will exclude from var files to avoid Terraform crash"
            else
              echo "services_empty=false" >> $GITHUB_OUTPUT
              echo "::notice::Services map has $SERVICE_COUNT service(s)"
            fi
          else
            echo "services_empty=true" >> $GITHUB_OUTPUT
            echo "::notice::services.generated.json not found"
          fi

      - name: Terraform Validate
        id: validate
        # Skip validate for ECS Fargate module entirely to avoid Terraform crash
        # This is a known Terraform bug with marked values in conditional expressions
        # terraform plan will catch all validation errors anyway
        if: steps.vars.outputs.module_path != '04-ecs-fargate'
        run: terraform -chdir=DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }} validate -no-color || echo "Terraform validate failed (non-blocking); relying on plan for validation."

      - name: Build var files list
        id: build_var_files
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          VAR_FILES=""
          
          # Always use the generated terraform.tfvars.json
          VAR_FILES="terraform.tfvars.json"
          
          # For ECS module, also include services.generated.json if it exists
          if [ "${{ steps.vars.outputs.module_path }}" == "04-ecs-fargate" ] && [ -f "$PLAN_DIR/services.generated.json" ]; then
            # Check if services map is empty (avoid Terraform crash when services = {})
            SERVICE_COUNT=$(jq '.services | length' "$PLAN_DIR/services.generated.json" 2>/dev/null || echo "0")
            if [ "$SERVICE_COUNT" -gt 0 ]; then
              VAR_FILES="$VAR_FILES services.generated.json"
              echo "services_empty=false" >> $GITHUB_OUTPUT
            else
              echo "::notice::Services map is empty (services = {}), services.generated.json will not be included."
              echo "services_empty=true" >> $GITHUB_OUTPUT
            fi
          else
            echo "services_empty=true" >> $GITHUB_OUTPUT
          fi
          
          echo "var_files=$VAR_FILES" >> $GITHUB_OUTPUT

      - name: Check for existing services in state
        id: check_state_services
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.check_services_empty.outputs.services_empty == 'true' &&
          steps.vars.outputs.module_path == '04-ecs-fargate'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Checking Terraform state for existing services..."
          
          # Initialize Terraform to access state (without var files to avoid crash)
          cd "$PLAN_DIR"
          terraform init -no-color -input=false > /dev/null 2>&1 || true
          
          # Check if state has any ECS services
          # Use terraform state list to find services (safe even with empty services config)
          if terraform state list -no-color 2>/dev/null | grep -q "aws_ecs_service.services\["; then
            EXISTING_SERVICES=$(terraform state list -no-color 2>/dev/null | grep "aws_ecs_service.services\[" || echo "")
            
            echo ""
            echo "::error::‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
            echo "::error::‚ïë  ‚ö†Ô∏è  DEPLOYMENT BLOCKED: DANGEROUS CONFIGURATION DETECTED  ‚ö†Ô∏è  ‚ïë"
            echo "::error::‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
            echo ""
            echo "::error::Problem: services = {} (empty) but Terraform state has existing services!"
            echo ""
            echo "::error::If you proceed, Terraform will DESTROY all existing services:"
            echo "$EXISTING_SERVICES" | while read -r service; do
              echo "::error::  ‚ùå $service"
            done
            echo ""
            echo "::error::Why this is blocked:"
            echo "::error::  ‚Ä¢ Config says: 'deploy zero services' (services = [])"
            echo "::error::  ‚Ä¢ State says: 'these services exist'"
            echo "::error::  ‚Ä¢ Terraform will destroy existing services to match empty config"
            echo "::error::  ‚Ä¢ This is almost certainly a mistake, not intentional"
            echo ""
            echo "::error::Solution - Generate services configuration first:"
            echo "::error::  1. Go to: Actions ‚Üí 'Generate Service Config'"
            echo "::error::  2. Run the workflow to generate services.generated.json from Dhall"
            echo "::error::  3. Review and merge the PR in DEVOPS repository"
            echo "::error::  4. Then run this deployment workflow again"
            echo ""
            echo "::error::This ensures services in config match your Dhall definitions."
            exit 1
          else
            echo "::notice::‚úì No existing services in state"
            echo "::notice::‚úì Safe to proceed with empty services config (fresh deployment)"
          fi

      - name: Comprehensive State Validation and Auto-Import
        id: validate_state
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate' &&
          steps.check_services_empty.outputs.services_empty != 'true'
        continue-on-error: true
        timeout-minutes: 5
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          if [ -f "scripts/validate-and-import-state.sh" ]; then
            chmod +x scripts/validate-and-import-state.sh
            scripts/validate-and-import-state.sh "$PLAN_DIR" "${{ steps.vars.outputs.environment }}" "${{ steps.vars.outputs.module_path }}" || {
              EXIT_CODE=$?
              if [ $EXIT_CODE -eq 1 ]; then
                echo "::error::State validation found resources that couldn't be imported"
                echo "::error::These will cause 'already exists' errors. Manual intervention may be needed."
              fi
              exit 0  # Don't fail workflow, but warn
            }
          else
            echo "::warning::State validation script not found, skipping comprehensive validation"
          fi

      - name: Terraform Refresh State (Pre-Plan Sync)
        id: refresh
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        timeout-minutes: 5
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          REFRESH_ARGS="-no-color"
          
          # Check if state exists - if not, skip refresh (fresh deployment)
          if ! terraform -chdir="$PLAN_DIR" state list -no-color >/dev/null 2>&1; then
            echo "::notice::No Terraform state found - skipping refresh (fresh deployment)"
            echo "::notice::This is expected when starting with a new/empty state"
            exit 0
          fi
          
          # Use the same var files logic as plan (which excludes empty services.generated.json)
          # This prevents Terraform crash when services = {} (empty services map)
          VAR_FILES="${{ steps.build_var_files.outputs.var_files }}"
          
          if [ -n "$VAR_FILES" ]; then
            for var_file in $VAR_FILES; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
              REFRESH_ARGS="$REFRESH_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          echo "::notice::üîÑ Refreshing Terraform state to sync with AWS (preventing state drift)..."
          if [ "${{ steps.check_services_empty.outputs.services_empty }}" == "true" ]; then
            echo "::notice::Services map is empty - refreshing infrastructure only (services excluded to avoid crash)"
          else
            echo "::notice::Refreshing all resources including services"
          fi
          
          # Use refresh-only mode to update state without making changes
          # This syncs state with actual AWS resources
          # Fail on errors - configuration errors should be caught here
          timeout 300 terraform -chdir="$PLAN_DIR" apply -refresh-only -auto-approve $REFRESH_ARGS 2>&1 | tee /tmp/refresh_output.txt || {
            REFRESH_EXITCODE=$?
            if [ $REFRESH_EXITCODE -eq 124 ]; then
              echo "::error::State refresh timed out after 5 minutes"
              exit 124
            else
              # Check if refresh-only failed because there are no changes (exit code 0 from apply -refresh-only)
              if grep -q "No changes" /tmp/refresh_output.txt 2>/dev/null; then
                echo "::notice::‚úì State is already in sync with AWS (no changes needed)"
                exit 0
              else
                echo "::error::State refresh failed with exit code $REFRESH_EXITCODE"
                echo "::error::Refresh output:"
                cat /tmp/refresh_output.txt 2>/dev/null || true
                echo "::error::This indicates a configuration error that must be fixed before proceeding"
                exit $REFRESH_EXITCODE
              fi
            fi
          }
          
          # Check if refresh-only made any changes
          STATE_UPDATED=false
          if grep -q "No changes" /tmp/refresh_output.txt 2>/dev/null; then
            echo "::notice::‚úì State is already in sync with AWS"
          else
            echo "::notice::‚úì State refreshed - updated to match AWS resources"
            STATE_UPDATED=true
            # Show what was updated
            if grep -q "updated in-place" /tmp/refresh_output.txt 2>/dev/null; then
              echo "::notice::Resources updated in state:"
              grep "updated in-place" /tmp/refresh_output.txt | head -5 || true
            fi
            # Check if any resources were added to state (imported via refresh)
            if grep -q "has been imported" /tmp/refresh_output.txt 2>/dev/null; then
              echo "::notice::Resources imported during refresh:"
              grep "has been imported" /tmp/refresh_output.txt | head -5 || true
            fi
          fi
          
          # If state was updated, invalidate any existing plan file
          # This ensures plan is recreated with fresh state
          if [ "$STATE_UPDATED" = "true" ] && [ -f "$PLAN_DIR/tfplan" ]; then
            echo "::notice::‚ö† State was updated - existing plan file will be invalidated"
            echo "::notice::Plan will be recreated with fresh state"
            rm -f "$PLAN_DIR/tfplan" || true
          fi
          
          rm -f /tmp/refresh_output.txt

      - name: Detect State Drift (Target Groups)
        id: detect_drift
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          # Only check for ECS Fargate module
          if [[ "${{ steps.vars.outputs.module_path }}" == "04-ecs-fargate" ]]; then
            echo "::notice::Target group health check validation is included in comprehensive pre-apply validation"
          else
            echo "::notice::State drift detection only runs for ECS Fargate module"
          fi

      - name: Check for service key mismatches (pre-plan)
        id: check_service_keys
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.check_services_empty.outputs.services_empty != 'true' &&
          steps.vars.outputs.module_path == '04-ecs-fargate'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Checking for service key mismatches between state and config..."
          
          # Initialize Terraform to access state
          cd "$PLAN_DIR"
          terraform init -no-color -input=false > /dev/null 2>&1 || true
          
          # Get services from state
          STATE_SERVICES=$(terraform state list -no-color 2>/dev/null | grep "aws_ecs_service.services\[" | sed 's/aws_ecs_service.services\["\(.*\)"\]/\1/' || echo "")
          
          # Get services from config (parse services.generated.json)
          if [ -f "$PLAN_DIR/services.generated.json" ]; then
            CONFIG_SERVICES=$(jq -r '.services | keys[]' "$PLAN_DIR/services.generated.json" 2>/dev/null || echo "")
          else
            CONFIG_SERVICES=""
          fi
          
          if [ -n "$STATE_SERVICES" ] && [ -n "$CONFIG_SERVICES" ]; then
            # Find services in state but not in config (will be destroyed)
            SERVICES_TO_DESTROY=""
            SERVICES_TO_CREATE=""
            SERVICES_TO_UPDATE=""
            
            for state_svc in $STATE_SERVICES; do
              if ! echo "$CONFIG_SERVICES" | grep -q "^$state_svc$"; then
                SERVICES_TO_DESTROY="$SERVICES_TO_DESTROY $state_svc"
              else
                SERVICES_TO_UPDATE="$SERVICES_TO_UPDATE $state_svc"
              fi
            done
            
            for config_svc in $CONFIG_SERVICES; do
              if ! echo "$STATE_SERVICES" | grep -q "^$config_svc$"; then
                SERVICES_TO_CREATE="$SERVICES_TO_CREATE $config_svc"
              fi
            done
            
            if [ -n "$SERVICES_TO_DESTROY" ]; then
              echo "::warning::‚ö†Ô∏è Service key mismatch detected!"
              echo "::warning::The following services in state will be DESTROYED (not in new config):"
              for svc in $SERVICES_TO_DESTROY; do
                echo "::warning::  ‚ùå $svc"
              done
              echo ""
              echo "::warning::Possible reasons:"
              echo "::warning::  ‚Ä¢ Service key changed (e.g., 'api' ‚Üí 'legacy::api')"
              echo "::warning::  ‚Ä¢ Service removed from YAML definitions"
              echo "::warning::  ‚Ä¢ Service renamed"
              echo ""
              echo "::warning::To preserve services, consider:"
              echo "::warning::  1. State migration: terraform state mv 'aws_ecs_service.services[\"old-key\"]' 'aws_ecs_service.services[\"new-key\"]'"
              echo "::warning::  2. Or add the service back to YAML if it was accidentally removed"
            else
              echo "::notice::‚úì All services in state match config keys"
            fi
            
            if [ -n "$SERVICES_TO_CREATE" ]; then
              echo "::notice::The following NEW services will be CREATED:"
              for svc in $SERVICES_TO_CREATE; do
                echo "::notice::  ‚ûï $svc"
              done
            fi
            
            if [ -n "$SERVICES_TO_UPDATE" ]; then
              echo "::notice::The following services will be UPDATED (if config changed):"
              for svc in $SERVICES_TO_UPDATE; do
                echo "::notice::  üîÑ $svc"
              done
            fi
          else
            echo "::notice::Cannot compare - state or config services list is empty"
          fi

      - name: Pre-Plan State Validation Summary
        id: pre_plan_summary
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate' &&
          steps.check_services_empty.outputs.services_empty != 'true'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::üìä State Validation Summary:"
          
          # Get expected service keys
          if [ ! -f "$PLAN_DIR/services.generated.json" ]; then
            echo "::notice::No services.generated.json found"
            exit 0
          fi
          
          EXPECTED_KEYS=$(jq -r '.services | keys[]' "$PLAN_DIR/services.generated.json" 2>/dev/null || echo "")
          
          if [ -z "$EXPECTED_KEYS" ]; then
            echo "::notice::No service keys found in config"
            exit 0
          fi
          
          # Check which services are in state
          STATE_SERVICES=$(terraform -chdir="$PLAN_DIR" state list 2>/dev/null | \
            grep 'module.ecs_fargate.aws_service_discovery_service.services\["' | \
            sed 's/.*\["\(.*\)"\]/\1/' || echo "")
          
          STATE_ECS=$(terraform -chdir="$PLAN_DIR" state list 2>/dev/null | \
            grep 'module.ecs_fargate.aws_ecs_service.services\["' | \
            sed 's/.*\["\(.*\)"\]/\1/' || echo "")
          
          MISSING_SD=""
          MISSING_ECS=""
          
          for tf_key in $EXPECTED_KEYS; do
            if ! echo "$STATE_SERVICES" | grep -q "^$tf_key$"; then
              MISSING_SD="$MISSING_SD $tf_key"
            fi
            if ! echo "$STATE_ECS" | grep -q "^$tf_key$"; then
              MISSING_ECS="$MISSING_ECS $tf_key"
            fi
          done
          
          if [ -n "$MISSING_SD" ] || [ -n "$MISSING_ECS" ]; then
            if [ -n "$MISSING_SD" ]; then
              echo "::warning::‚ö†Ô∏è Service Discovery services NOT in state:"
              for svc in $MISSING_SD; do
                echo "::warning::  - $svc"
              done
            fi
            if [ -n "$MISSING_ECS" ]; then
              echo "::notice::‚ÑπÔ∏è ECS services NOT in state (will be created):"
              for svc in $MISSING_ECS; do
                echo "::notice::  - $svc"
              done
            fi
          else
            echo "::notice::‚úì All expected resources are in state"
          fi

      - name: Terraform Plan
        id: plan
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        uses: ./.github/actions/terraform-plan
        with:
          terraform_path: DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}
          var_files: ${{ steps.build_var_files.outputs.var_files }}
          # No global image tag injection; image tags are resolved per-service via services.generated.json
          extra_vars: ''
          plan_file: "tfplan"
        continue-on-error: true

      - name: Debug Plan - Show Replacement Reasons
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          if [ -f "$PLAN_DIR/tfplan" ]; then
            echo "::group::Debug: Resources Being Replaced"
            echo "Checking for resources that need replacement..."
            terraform -chdir="$PLAN_DIR" show -no-color tfplan | grep -A 10 "must be replaced" || echo "No replacements detected"
            echo "::endgroup::"
            
            echo "::group::Debug: Detailed Plan Output"
            terraform -chdir="$PLAN_DIR" show -no-color tfplan || true
            echo "::endgroup::"
          else
            echo "::notice::Plan file not available for debugging"
          fi

      - name: Add Plan to Summary
        if: steps.vars.outputs.action == 'plan' || (steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success')
        run: |
          echo "## Terraform Plan Output" >> $GITHUB_STEP_SUMMARY
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          if [ -f "$PLAN_DIR/tfplan" ]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            terraform -chdir="$PLAN_DIR" show -no-color tfplan >> $GITHUB_STEP_SUMMARY 2>&1
            echo '```' >> $GITHUB_STEP_SUMMARY
            
            # Add replacement analysis to summary
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## üîç Replacement Analysis" >> $GITHUB_STEP_SUMMARY
            REPLACEMENTS=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 | grep -c "must be replaced" || echo "0")
            if [ "$REPLACEMENTS" -gt "0" ]; then
              echo "‚ö†Ô∏è **Warning:** $REPLACEMENTS resource(s) will be replaced" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "Resources being replaced:" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 | grep -B 2 -A 5 "must be replaced" >> $GITHUB_STEP_SUMMARY || true
              echo '```' >> $GITHUB_STEP_SUMMARY
            else
              echo "‚úÖ No replacements detected" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "Plan file not available (tfplan not found)" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check if plan has changes
        id: plan_changes
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          PLAN_OUTPUT=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1)
          
          if echo "$PLAN_OUTPUT" | grep -q "No changes"; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "::notice::No changes detected. Skipping apply."
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "::notice::Changes detected. Proceeding with apply."
            
            # Extract service changes from plan for visibility
            if echo "$PLAN_OUTPUT" | grep -q "aws_ecs_service.services\["; then
              echo ""
              echo "::notice::üìã Service Changes Detected:"
              
              CREATES=$(echo "$PLAN_OUTPUT" | grep -c "aws_ecs_service.services\[.*\] will be created" 2>/dev/null || echo "0")
              DESTROYS=$(echo "$PLAN_OUTPUT" | grep -c "aws_ecs_service.services\[.*\] will be destroyed" 2>/dev/null || echo "0")
              REPLACES=$(echo "$PLAN_OUTPUT" | grep -c "aws_ecs_service.services\[.*\] must be replaced" 2>/dev/null || echo "0")
              
              # Ensure we have valid integers (strip any whitespace/newlines and default to 0)
              CREATES=$(echo "$CREATES" | tr -d '[:space:]' || echo "0")
              DESTROYS=$(echo "$DESTROYS" | tr -d '[:space:]' || echo "0")
              REPLACES=$(echo "$REPLACES" | tr -d '[:space:]' || echo "0")
              
              if [ "${CREATES:-0}" -gt 0 ] 2>/dev/null; then
                echo "::notice::  ‚ûï Services to CREATE: $CREATES"
              fi
              
              if [ "${DESTROYS:-0}" -gt 0 ] 2>/dev/null; then
                echo "::warning::  ‚ùå Services to DESTROY: $DESTROYS"
                echo "::warning::‚ö†Ô∏è This may be due to service key changes (e.g., 'api' ‚Üí 'legacy::api')"
              fi
              
              if [ "${REPLACES:-0}" -gt 0 ] 2>/dev/null; then
                echo "::warning::  üîÑ Services to REPLACE: $REPLACES"
              fi
            fi
          fi

      - name: Save State Snapshot Before Apply
        id: save_state_snapshot
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Creating state snapshot before apply for targeted cleanup..."
          
          # Get list of resources in state BEFORE apply
          STATE_BEFORE=$(terraform -chdir="$PLAN_DIR" state list -no-color 2>/dev/null || echo "")
          
          # Save to file for cleanup step
          echo "$STATE_BEFORE" > /tmp/state_before_apply.txt || true
          
          # Also get plan to see what will be created
          if [ -f "$PLAN_DIR/tfplan" ]; then
            PLAN_OUTPUT=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 || echo "")
            
            # Extract resources that will be created
            CREATED_RESOURCES=$(echo "$PLAN_OUTPUT" | grep "will be created" | \
              sed 's/^[[:space:]]*# //' | sed 's/ will be created$//' || echo "")
            
            echo "$CREATED_RESOURCES" > /tmp/resources_to_create.txt || true
          fi

      - name: Idempotency Check Before Apply
        id: idempotency_check
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::üîç Performing idempotency checks before apply..."
          
          if [ ! -f "$PLAN_DIR/tfplan" ]; then
            echo "::warning::Plan file not found - skipping idempotency check"
            exit 0
          fi
          
          PLAN_OUTPUT=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 || echo "")
          
          # Get resources that plan wants to create
          RESOURCES_TO_CREATE=$(echo "$PLAN_OUTPUT" | grep "will be created" | \
            sed 's/^[[:space:]]*# //' | sed 's/ will be created$//' || echo "")
          
          # Get resources that plan wants to replace (critical for Service Discovery)
          RESOURCES_TO_REPLACE=$(echo "$PLAN_OUTPUT" | grep "must be replaced" | \
            sed 's/^[[:space:]]*# //' | sed 's/ must be replaced$//' || echo "")
          
          CONFLICTS=0
          
          # Check resources that will be created
          if [ -n "$RESOURCES_TO_CREATE" ]; then
            echo "::notice::Checking resources to be created..."
            while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              # Check if resource already exists in state
              if terraform -chdir="$PLAN_DIR" state show "$resource" >/dev/null 2>&1; then
                echo "::error::‚ùå Resource '$resource' is in state but plan wants to create it"
                echo "::error::This indicates state drift. Resource should be updated, not created."
                CONFLICTS=$((CONFLICTS + 1))
              fi
            done <<< "$RESOURCES_TO_CREATE"
          fi
          
          # Check resources that will be replaced (especially Service Discovery)
          if [ -n "$RESOURCES_TO_REPLACE" ]; then
            echo "::notice::Checking resources to be replaced..."
            while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              # For Service Discovery services, check if replacement already exists in AWS
              if echo "$resource" | grep -q "aws_service_discovery_service"; then
                # Extract service key from resource path
                SERVICE_KEY=$(echo "$resource" | sed 's/.*\["\(.*\)"\]/\1/')
                
                # Get namespace ID from state
                NAMESPACE_ID=$(terraform -chdir="$PLAN_DIR" state show 'module.ecs_fargate.aws_service_discovery_private_dns_namespace.this' 2>/dev/null | \
                  grep -E '^\s+id\s+=' | awk '{print $3}' | tr -d '"' || echo "")
                
                if [ -n "$NAMESPACE_ID" ]; then
                  # Get expected sanitized service name (part after ::)
                  EXPECTED_NAME=$(echo "$SERVICE_KEY" | sed 's/.*::\(.*\)/\1/' | tr '[:upper:]' '[:lower:]')
                  
                  # Check if service with this name already exists in AWS
                  EXISTING_SERVICE=$(aws servicediscovery list-services \
                    --filters Name=NAMESPACE_ID,Values="$NAMESPACE_ID" \
                    --query "Services[?Name=='$EXPECTED_NAME'].[Name,Id]" \
                    --output text 2>/dev/null || echo "")
                  
                  if [ -n "$EXISTING_SERVICE" ]; then
                    SERVICE_ID=$(echo "$EXISTING_SERVICE" | awk '{print $2}')
                    echo "::error::‚ùå Service Discovery service '$SERVICE_KEY' will be replaced"
                    echo "::error::   Replacement service '$EXPECTED_NAME' already exists in AWS (ID: $SERVICE_ID)"
                    echo "::error::   This will cause 'ServiceAlreadyExists' error during apply"
                    echo "::error::   The validation script should have imported this, but it may have failed"
                    echo "::error::   Consider running: terraform import '$resource' '$SERVICE_ID'"
                    CONFLICTS=$((CONFLICTS + 1))
                  fi
                fi
              fi
            done <<< "$RESOURCES_TO_REPLACE"
          fi
          
          if [ $CONFLICTS -gt 0 ]; then
            echo "::error::‚ùå Idempotency check failed: $CONFLICTS conflict(s) detected"
            echo "::error::Resources that will be created/replaced already exist in AWS"
            echo "::error::State validation should have caught this. Review state and plan."
            echo "::error::"
            echo "::error::To fix:"
            echo "::error::  1. Run the 'Comprehensive State Validation' step again"
            echo "::error::  2. Or manually import the resources: terraform import 'resource.address' 'aws-id'"
            exit 1
          else
            if [ -z "$RESOURCES_TO_CREATE" ] && [ -z "$RESOURCES_TO_REPLACE" ]; then
              echo "::notice::‚úì No resources to create or replace - idempotency check passed"
            else
              echo "::notice::‚úì Idempotency check passed - no conflicts detected"
            fi
          fi

      - name: Comprehensive Pre-Apply Validation
        id: comprehensive_validation
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          if [ -f "scripts/comprehensive-pre-apply-validation.sh" ]; then
            chmod +x scripts/comprehensive-pre-apply-validation.sh
            scripts/comprehensive-pre-apply-validation.sh "$PLAN_DIR" "${{ steps.vars.outputs.environment }}" "${{ steps.vars.outputs.module_path }}" || {
              EXIT_CODE=$?
              if [ $EXIT_CODE -eq 1 ]; then
                echo "::error::Comprehensive validation found critical errors"
                echo "::error::Please review and fix the errors before proceeding"
                echo "validation_failed=true" >> $GITHUB_OUTPUT
              else
                echo "validation_failed=false" >> $GITHUB_OUTPUT
              fi
              exit 0  # Don't fail workflow, but set output
            }
            echo "validation_failed=false" >> $GITHUB_OUTPUT
          else
            echo "::warning::Comprehensive validation script not found, skipping"
            echo "validation_failed=false" >> $GITHUB_OUTPUT
          fi

      - name: Check Terraform State Lock (if DynamoDB locking enabled)
        id: check_state_lock
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        continue-on-error: true
        timeout-minutes: 1
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::üîí Checking for Terraform state locks..."
          
          # Check if DynamoDB locking is configured in backend
          if grep -q "dynamodb_table" "$PLAN_DIR/backend.tf" 2>/dev/null || grep -q "dynamodb_table" "$PLAN_DIR/backend.hcl" 2>/dev/null; then
            echo "::notice::DynamoDB state locking is configured"
            
            # Try to list locks (terraform will error if locked)
            # Note: There's no direct "check lock" command, but we can try a safe operation
            # that will fail if locked: terraform state list (read-only, but still requires lock)
            if terraform -chdir="$PLAN_DIR" state list -no-color >/dev/null 2>&1; then
              echo "::notice::‚úì No state locks detected (state is accessible)"
              echo "state_locked=false" >> $GITHUB_OUTPUT
            else
              LOCK_ERROR=$(terraform -chdir="$PLAN_DIR" state list -no-color 2>&1 || true)
              if echo "$LOCK_ERROR" | grep -qi "lock\|locked"; then
                echo "::warning::‚ö† State is locked by another operation"
                echo "::warning::Lock error: $LOCK_ERROR"
                echo "::warning::Waiting for lock to be released or checking for stale locks..."
                echo "state_locked=true" >> $GITHUB_OUTPUT
              else
                echo "::notice::State check completed (may be fresh deployment with no state)"
                echo "state_locked=false" >> $GITHUB_OUTPUT
              fi
            fi
          else
            echo "::notice::DynamoDB state locking is NOT configured (S3 backend only)"
            echo "::notice::State locking relies on S3 object locking, which is less reliable"
            echo "::notice::Consider enabling DynamoDB locking for better concurrency control"
            echo "::notice::See: DEVOPS/live/dev/01-vpc/README-BACKEND.md"
            echo "state_locked=false" >> $GITHUB_OUTPUT
          fi

      - name: Always Re-plan Before Apply (Prevent Stale Plans)
        id: replan_before_apply
        if: |
          steps.vars.outputs.action == 'apply' && 
          steps.plan.outcome == 'success' && 
          steps.plan_changes.outputs.has_changes == 'true'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::üîÑ Creating fresh plan right before apply (prevents stale plan errors)..."
          echo "::notice::This ensures the plan matches the current state, even if state changed since initial plan"
          
          # Remove old plan file
          rm -f "$PLAN_DIR/tfplan"
          
          # Build plan arguments (same as original plan)
          VAR_FILES="${{ steps.build_var_files.outputs.var_files }}"
          PLAN_ARGS="-out=tfplan -no-color"
          
          if [ -n "$VAR_FILES" ]; then
            for var_file in $VAR_FILES; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                PLAN_ARGS="$PLAN_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          # Create fresh plan
          terraform -chdir="$PLAN_DIR" plan $PLAN_ARGS || {
            echo "::error::Fresh plan creation failed"
            echo "::error::This may indicate state conflicts or configuration issues"
            exit 1
          }
          
          echo "::notice::‚úì Fresh plan created successfully"
          echo "replanned=true" >> $GITHUB_OUTPUT
          
          # Show plan summary
          echo "::group::Fresh Plan Summary"
          terraform -chdir="$PLAN_DIR" show -no-color tfplan | head -50 || true
          echo "::endgroup::"

      - name: Terraform Apply
        id: apply
        if: |
          steps.vars.outputs.action == 'apply' && 
          steps.plan.outcome == 'success' && 
          steps.plan_changes.outputs.has_changes == 'true' &&
          steps.replan_before_apply.outcome == 'success' &&
          steps.replan_before_apply.outputs.replanned == 'true' &&
          steps.comprehensive_validation.outputs.validation_failed != 'true' &&
          steps.idempotency_check.outcome == 'success'
        uses: ./.github/actions/terraform-apply
        with:
          terraform_path: DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}
          plan_file: "tfplan"
          auto_approve: "true"

      - name: Generate and Set Secret Values
        id: generate_secrets
        if: |
          steps.vars.outputs.action == 'apply' && 
          steps.apply.outcome == 'success' && 
          steps.vars.outputs.module_path == '05-database'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::üîê Generating secure secrets for database module..."
          
          if [ -d "DEVOPS" ]; then
            cd DEVOPS
          else
            echo "::error::DEVOPS checkout directory not found at ./DEVOPS"
            exit 1
          fi

          if [ -f "scripts/generate-secrets.sh" ]; then
            chmod +x scripts/generate-secrets.sh
            if scripts/generate-secrets.sh "${{ steps.vars.outputs.environment }}" "live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"; then
              echo "::notice::‚úì Secret generation completed successfully"
              echo "secrets_generated=true" >> $GITHUB_OUTPUT
            else
              echo "::warning::‚ö† Secret generation script failed (non-blocking)"
              echo "::warning::Secrets may need to be set manually"
              echo "secrets_generated=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "::warning::Secret generation script not found at DEVOPS/scripts/generate-secrets.sh"
            echo "::warning::Skipping secret generation - secrets may need to be set manually"
            echo "secrets_generated=false" >> $GITHUB_OUTPUT
          fi

      - name: Verify DynamoDB Table Connectivity (Post-Apply)
        id: verify_dynamodb
        if: |
          steps.vars.outputs.action == 'apply' && 
          steps.apply.outcome == 'success' && 
          steps.vars.outputs.module_path == '05-database'
        continue-on-error: true
        env:
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
          ENVIRONMENT: ${{ steps.vars.outputs.environment }}
        run: |
          echo "::notice::üîç Verifying DynamoDB table connectivity after deployment..."
          
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/05-database"
          TABLE_NAME=""
          
          # Method 1: Get table name(s) from Terraform outputs (most reliable)
          echo "::group::Discovering table name from Terraform outputs"
          cd "$PLAN_DIR"
          
          # Try to get table_names output (map of table names)
          TABLE_NAMES_JSON=$(terraform output -json table_names 2>/dev/null || echo "{}")
          
          if [ "$TABLE_NAMES_JSON" != "{}" ] && [ -n "$TABLE_NAMES_JSON" ]; then
            # Count how many tables we have
            TABLE_COUNT=$(echo "$TABLE_NAMES_JSON" | jq 'length' 2>/dev/null || echo "0")
            echo "Found $TABLE_COUNT table(s) in Terraform output"
            
            # Extract table names (handle both single and multiple tables)
            if [ "$TABLE_COUNT" -eq 1 ]; then
              # Single table - get the value directly
              TABLE_NAME=$(echo "$TABLE_NAMES_JSON" | jq -r 'to_entries[0].value // empty' 2>/dev/null || echo "")
            elif [ "$TABLE_COUNT" -gt 1 ]; then
              # Multiple tables - prefer "greetings" table, otherwise first one
              TABLE_NAME=$(echo "$TABLE_NAMES_JSON" | jq -r '.greetings // to_entries[0].value // empty' 2>/dev/null || echo "")
              echo "Multiple tables found, using: $TABLE_NAME"
            fi
            
            if [ -n "$TABLE_NAME" ] && [ "$TABLE_NAME" != "null" ]; then
              echo "‚úì Found table name from Terraform output: $TABLE_NAME"
              
              # Show all tables for reference
              echo "All tables in Terraform output:"
              echo "$TABLE_NAMES_JSON" | jq -r 'to_entries[] | "  - \(.key): \(.value)"' 2>/dev/null || echo "  (unable to parse)"
            fi
          else
            echo "‚ö† Terraform output 'table_names' not available or empty"
          fi
          
          cd - > /dev/null
          echo "::endgroup::"
          
          # Method 2: Try SSM Parameter Store (if Terraform output failed)
          if [ -z "$TABLE_NAME" ] || [ "$TABLE_NAME" = "null" ]; then
            echo "::group::Discovering table name from SSM Parameter Store"
            SSM_PARAM="/${{ steps.vars.outputs.environment }}/dynamodb/greetings/table_name"
            TABLE_NAME=$(aws ssm get-parameter \
              --name "$SSM_PARAM" \
              --region "$AWS_REGION" \
              --query 'Parameter.Value' \
              --output text 2>/dev/null || echo "")
            
            if [ -n "$TABLE_NAME" ]; then
              echo "‚úì Found table name from SSM: $TABLE_NAME"
            else
              echo "‚ö† SSM parameter not found: $SSM_PARAM"
            fi
            echo "::endgroup::"
          fi
          
          # Method 3: List all DynamoDB tables and find matching one
          if [ -z "$TABLE_NAME" ] || [ "$TABLE_NAME" = "null" ]; then
            echo "::group::Discovering table name from AWS DynamoDB"
            echo "Listing all DynamoDB tables in region $AWS_REGION..."
            
            # List all tables and find one matching the environment pattern
            EXPECTED_PATTERN="${{ steps.vars.outputs.environment }}-greetings"
            ALL_TABLES=$(aws dynamodb list-tables --region "$AWS_REGION" --query 'TableNames[]' --output text 2>/dev/null || echo "")
            
            if [ -n "$ALL_TABLES" ]; then
              # Check if expected table exists
              if echo "$ALL_TABLES" | grep -q "^$EXPECTED_PATTERN$"; then
                TABLE_NAME="$EXPECTED_PATTERN"
                echo "‚úì Found table matching pattern: $TABLE_NAME"
              else
                # Get first table that contains the environment name
                TABLE_NAME=$(echo "$ALL_TABLES" | tr '\t' '\n' | grep -E "^${{ steps.vars.outputs.environment }}-" | head -1 || echo "")
                if [ -n "$TABLE_NAME" ]; then
                  echo "‚úì Found table with environment prefix: $TABLE_NAME"
                else
                  # Last resort: get first table
                  TABLE_NAME=$(echo "$ALL_TABLES" | tr '\t' '\n' | head -1 || echo "")
                  if [ -n "$TABLE_NAME" ]; then
                    echo "‚ö† Using first available table: $TABLE_NAME (may not be correct)"
                  fi
                fi
              fi
            else
              echo "‚ö† No tables found in DynamoDB"
            fi
            echo "::endgroup::"
          fi
          
          # Final fallback: Use naming convention
          if [ -z "$TABLE_NAME" ] || [ "$TABLE_NAME" = "null" ]; then
            TABLE_NAME="${{ steps.vars.outputs.environment }}-greetings"
            echo "::warning::Using default table name (naming convention): $TABLE_NAME"
          fi
          
          echo "::notice::Using table name: $TABLE_NAME"
          
          # Verify table exists and is accessible
          echo "::group::DynamoDB Table Verification"
          echo "Verifying table: $TABLE_NAME in region: $AWS_REGION"
          
          if aws dynamodb describe-table \
            --table-name "$TABLE_NAME" \
            --region "$AWS_REGION" \
            > /dev/null 2>&1; then
            echo "‚úì Table exists and is accessible"
            
            # Get comprehensive table details
            TABLE_DETAILS=$(aws dynamodb describe-table \
              --table-name "$TABLE_NAME" \
              --region "$AWS_REGION" 2>/dev/null || echo "{}")
            
            TABLE_STATUS=$(echo "$TABLE_DETAILS" | jq -r '.Table.TableStatus // "UNKNOWN"' 2>/dev/null || echo "UNKNOWN")
            TABLE_ARN=$(echo "$TABLE_DETAILS" | jq -r '.Table.TableArn // ""' 2>/dev/null || echo "")
            ITEM_COUNT=$(echo "$TABLE_DETAILS" | jq -r '.Table.ItemCount // 0' 2>/dev/null || echo "0")
            TABLE_SIZE=$(echo "$TABLE_DETAILS" | jq -r '.Table.TableSizeBytes // 0' 2>/dev/null || echo "0")
            BILLING_MODE=$(echo "$TABLE_DETAILS" | jq -r '.Table.BillingModeSummary.BillingMode // "UNKNOWN"' 2>/dev/null || echo "UNKNOWN")
            GSI_COUNT=$(echo "$TABLE_DETAILS" | jq -r '.Table.GlobalSecondaryIndexes | length // 0' 2>/dev/null || echo "0")
            
            echo ""
            echo "üìä Table Details:"
            echo "  Name: $TABLE_NAME"
            echo "  Status: $TABLE_STATUS"
            echo "  ARN: $TABLE_ARN"
            echo "  Item Count: $ITEM_COUNT"
            echo "  Size: $TABLE_SIZE bytes"
            echo "  Billing Mode: $BILLING_MODE"
            echo "  Global Secondary Indexes: $GSI_COUNT"
            
            if [ "$TABLE_STATUS" = "ACTIVE" ]; then
              echo ""
              echo "‚úì Table is ACTIVE and ready for operations"
              
              # Test basic write operation (optional - creates a test item)
              TEST_ITEM_ID="test-verify-$(date +%s)-$$"
              TEST_CREATED_AT=$(date -u +%Y-%m-%dT%H:%M:%SZ)
              echo "Testing write operation with test item: $TEST_ITEM_ID"
              
              if aws dynamodb put-item \
                --table-name "$TABLE_NAME" \
                --item "{\"id\":{\"S\":\"$TEST_ITEM_ID\"},\"created_at\":{\"S\":\"$TEST_CREATED_AT\"},\"user_name\":{\"S\":\"test-user\"},\"message\":{\"S\":\"Integration test verification\"}}" \
                --region "$AWS_REGION" \
                > /dev/null 2>&1; then
                echo "‚úì Write operation successful"
                
                # Test read operation
                if aws dynamodb get-item \
                  --table-name "$TABLE_NAME" \
                  --key "{\"id\":{\"S\":\"$TEST_ITEM_ID\"},\"created_at\":{\"S\":\"$TEST_CREATED_AT\"}}" \
                  --region "$AWS_REGION" \
                  --query 'Item.message.S' \
                  --output text \
                  > /dev/null 2>&1; then
                  echo "‚úì Read operation successful"
                  
                  # Clean up test item
                  aws dynamodb delete-item \
                    --table-name "$TABLE_NAME" \
                    --key "{\"id\":{\"S\":\"$TEST_ITEM_ID\"},\"created_at\":{\"S\":\"$TEST_CREATED_AT\"}}" \
                    --region "$AWS_REGION" \
                    > /dev/null 2>&1 || true
                  echo "‚úì Test item cleaned up"
                else
                  echo "‚ö† Read operation failed (non-critical - may be due to IAM permissions)"
                fi
              else
                echo "‚ö† Write operation failed (may be due to IAM permissions - non-critical)"
              fi
              
              echo "::notice::‚úì DynamoDB table verification completed successfully"
              echo "verification_passed=true" >> $GITHUB_OUTPUT
            else
              echo "::warning::‚ö† Table exists but is not ACTIVE (Status: $TABLE_STATUS)"
              echo "verification_passed=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "::error::‚úó Table '$TABLE_NAME' not found or not accessible"
            echo "::error::Please verify the table was created successfully"
            echo "verification_passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          echo "::endgroup::"

      - name: Verify Target Group Health Check Paths (Post-Apply)
        id: verify_health_checks
        if: steps.vars.outputs.action == 'apply' && steps.apply.outcome == 'success' && steps.vars.outputs.module_path == '04-ecs-fargate'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Verifying target group health check paths after apply..."
          
          if [ -f "scripts/verify-target-group-health-checks.sh" ]; then
            chmod +x scripts/verify-target-group-health-checks.sh
            export TERRAFORM_DIR="$PLAN_DIR"
            export ENVIRONMENT="${{ steps.vars.outputs.environment }}"
            
            if scripts/verify-target-group-health-checks.sh; then
              echo "::notice::‚úì All target group health check paths verified successfully"
            else
              echo "::warning::‚ö† State drift detected after apply. Target groups may not match configuration."
            fi
          else
            echo "::notice::Verification script not found, skipping..."
          fi

      - name: Verify Service Health and Connectivity
        id: verify_service_health
        if: steps.vars.outputs.action == 'apply' && steps.apply.outcome == 'success' && steps.vars.outputs.module_path == '04-ecs-fargate'
        env:
          VERIFY_MAX_WAIT_SECONDS: "900"
          VERIFY_INTERVAL_SECONDS: "15"
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Verifying service health and connectivity after deployment..."
          
          if [ -d "DEVOPS" ]; then
            cd DEVOPS
          else
            echo "::error::DEVOPS checkout directory not found at ./DEVOPS"
            exit 1
          fi

          if [ ! -f "scripts/verify-service-health.sh" ]; then
            echo "::error::Service health verification script not found at DEVOPS/scripts/verify-service-health.sh"
            exit 1
          fi

          chmod +x scripts/verify-service-health.sh

          # This step MUST gate deployment success on real health.
          scripts/verify-service-health.sh "${{ steps.vars.outputs.environment }}" "${{ steps.vars.outputs.module_path }}" "${{ secrets.AWS_REGION }}"

      - name: Targeted Cleanup on Apply Failure
        if: steps.vars.outputs.action == 'apply' && steps.apply.outcome == 'failure' && steps.vars.outputs.environment != 'production'
        continue-on-error: true
        run: |
          echo "::error::Terraform apply failed. Attempting targeted cleanup of resources created in this run..."
          
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          # Get state before apply
          STATE_BEFORE=""
          if [ -f /tmp/state_before_apply.txt ]; then
            STATE_BEFORE=$(cat /tmp/state_before_apply.txt)
          fi
          
          # Get current state
          STATE_AFTER=$(terraform -chdir="$PLAN_DIR" state list -no-color 2>/dev/null || echo "")
          
          # Find resources that were created in this run (in state_after but not in state_before)
          NEWLY_CREATED=""
          if [ -n "$STATE_AFTER" ]; then
            while IFS= read -r resource; do
              if [ -z "$STATE_BEFORE" ] || ! echo "$STATE_BEFORE" | grep -q "^${resource}$"; then
                NEWLY_CREATED="${NEWLY_CREATED}${resource}"$'\n'
              fi
            done <<< "$STATE_AFTER"
          fi
          
          # Also check plan file for resources that were supposed to be created
          PLANNED_CREATES=""
          if [ -f /tmp/resources_to_create.txt ]; then
            PLANNED_CREATES=$(cat /tmp/resources_to_create.txt)
          fi
          
          if [ -z "$NEWLY_CREATED" ] && [ -z "$PLANNED_CREATES" ]; then
            echo "::notice::No new resources were created in this run. No cleanup needed."
            exit 0
          fi
          
          echo "::notice::Resources created in this failed run:"
          if [ -n "$NEWLY_CREATED" ]; then
            echo "$NEWLY_CREATED" | while read -r resource; do
              [ -n "$resource" ] && echo "::notice::  - $resource"
            done
          fi
          
          # Build destroy command with var files
          DESTROY_ARGS="-auto-approve"
          VAR_FILES="${{ steps.build_var_files.outputs.var_files }}"
          if [ -n "$VAR_FILES" ]; then
            for var_file in $VAR_FILES; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                DESTROY_ARGS="$DESTROY_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          # For targeted cleanup, we'll use terraform destroy but with a filter
          # Since Terraform doesn't support selective destroy easily, we'll:
          # 1. Try to remove newly created resources from state (if they're truly orphaned)
          # 2. Or destroy only if it's safe (non-production, and resources are clearly orphaned)
          
          echo "::notice::Attempting targeted cleanup..."
          
          # Option 1: Try to remove orphaned resources from state (if they don't exist in AWS)
          REMOVED_FROM_STATE=0
          if [ -n "$NEWLY_CREATED" ]; then
            echo "$NEWLY_CREATED" | while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              # Check if resource actually exists in AWS by trying to refresh it
              REFRESH_OUTPUT=$(terraform -chdir="$PLAN_DIR" state show "$resource" 2>&1 || echo "")
              if echo "$REFRESH_OUTPUT" | grep -q "doesn't exist" || echo "$REFRESH_OUTPUT" | grep -q "not found"; then
                echo "::notice::Removing orphaned resource from state: $resource"
                terraform -chdir="$PLAN_DIR" state rm "$resource" 2>&1 || true
                REMOVED_FROM_STATE=$((REMOVED_FROM_STATE + 1))
              fi
            done
          fi
          
          # Option 2: If resources exist in AWS, we need to destroy them properly
          # But only if they're truly orphaned (not part of working infrastructure)
          if [ -n "$NEWLY_CREATED" ] && [ "$REMOVED_FROM_STATE" -eq 0 ]; then
            echo "::warning::Resources exist in AWS. Checking if they're safe to destroy..."
            
            # For ECS Fargate, be very careful - only destroy if they're clearly failed
            # Check if any ECS services are in failed state
            FAILED_SERVICES=""
            while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              if echo "$resource" | grep -q "aws_ecs_service"; then
                # Extract service name and check status
                SERVICE_NAME=$(echo "$resource" | sed 's/.*\["\(.*\)"\]/\1/')
                # Service might be in failed state - check AWS
                # For now, we'll be conservative and not auto-destroy ECS services
                echo "::warning::ECS service detected: $SERVICE_NAME - manual cleanup recommended"
              fi
            done <<< "$NEWLY_CREATED"
            
            # For non-critical resources (like Service Discovery), we can be more aggressive
            echo "::notice::For safety, manual cleanup is recommended for resources created in this run"
            echo "::notice::Review the resources listed above and destroy manually if needed"
          fi
          
          # Cleanup temp files
          rm -f /tmp/state_before_apply.txt /tmp/resources_to_create.txt
          
          echo "::notice::Targeted cleanup analysis completed"

      - name: Production apply failure notification
        if: steps.vars.outputs.action == 'apply' && steps.apply.outcome == 'failure' && steps.vars.outputs.environment == 'production'
        run: |
          echo "::error::Terraform apply failed in PRODUCTION environment."
          echo "::error::Automatic cleanup is disabled for production. Manual intervention required."
          echo "::error::Please review the Terraform state and decide on appropriate action:"
          echo "::error::  1. Review what resources were created before the failure"
          echo "::error::  2. Manually destroy partially created resources if needed"
          echo "::error::  3. Fix the issue and retry the deployment"
          exit 1

      - name: No Changes Detected
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'false'
        run: |
          echo "::notice::No changes to apply. Infrastructure is already up to date."
          echo "## ‚úÖ No Changes Required" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The Terraform plan showed no changes. Your infrastructure matches the configuration." >> $GITHUB_STEP_SUMMARY

      - name: Plan Failed
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'failure'
        run: |
          echo "::error::Terraform plan failed. Apply cannot proceed."
          exit 1

      - name: Terraform Destroy (Production Protection)
        if: steps.vars.outputs.action == 'destroy' && steps.vars.outputs.environment == 'production'
        run: |
          echo "::error::Destroy action is blocked for production environment for safety."
          exit 1

      - name: Build var files list for destroy
        id: build_var_files_destroy
        if: steps.vars.outputs.action == 'destroy' && steps.vars.outputs.environment != 'production'
        run: |
          VAR_FILES=""
          if [ "${{ steps.check_tfvars.outputs.tfvars_exists }}" == "true" ]; then
            VAR_FILES="terraform.tfvars"
          fi
          # For destroy, include services.generated.json even if empty (needed for proper cleanup)
          if [ "${{ steps.check_services_json.outputs.services_json_exists }}" == "true" ]; then
            if [ -n "$VAR_FILES" ]; then
              VAR_FILES="$VAR_FILES services.generated.json"
            else
              VAR_FILES="services.generated.json"
            fi
          fi
          echo "var_files=$VAR_FILES" >> $GITHUB_OUTPUT

      - name: Terraform Destroy
        if: steps.vars.outputs.action == 'destroy' && steps.vars.outputs.environment != 'production'
        shell: bash
        run: |
          set +e  # Disable exit on error from the start for lock handling
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          DESTROY_ARGS="-auto-approve"
          if [ -n "${{ steps.build_var_files_destroy.outputs.var_files }}" ]; then
            for var_file in ${{ steps.build_var_files_destroy.outputs.var_files }}; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                DESTROY_ARGS="$DESTROY_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          # Ensure Terraform is initialized before destroy (idempotency)
          echo "::notice::[$(date +'%H:%M:%S')] Initializing Terraform..."
          terraform -chdir="$PLAN_DIR" init -upgrade -no-color || {
            echo "::error::Terraform init failed before destroy"
            exit 1
          }
          
          # Verify state exists before destroying (resilience)
          echo "::notice::[$(date +'%H:%M:%S')] Verifying Terraform state exists..."
          if ! terraform -chdir="$PLAN_DIR" state list -no-color >/dev/null 2>&1; then
            echo "::notice::No Terraform state found - nothing to destroy"
            exit 0
          fi
          
          # Show what will be destroyed for visibility
          echo "::notice::[$(date +'%H:%M:%S')] Generating destroy plan to show what will be destroyed..."
          PLAN_OUTPUT=$(terraform -chdir="$PLAN_DIR" plan -destroy -refresh=false -input=false -no-color -var="services={}" -var="albs={}" -var="terraform_backend_bucket_name=devops-project-terraform" $DESTROY_ARGS 2>&1)
          PLAN_EXIT=$?
          
          if [ $PLAN_EXIT -eq 0 ]; then
            echo "$PLAN_OUTPUT" | head -100
            echo "::notice::[$(date +'%H:%M:%S')] Destroy plan generated successfully"
          else
            echo "$PLAN_OUTPUT"
            echo "::warning::[$(date +'%H:%M:%S')] Destroy plan had warnings/errors, but continuing..."
          fi
          
          # Add empty defaults for required variables since we're destroying from state
          DESTROY_ARGS="$DESTROY_ARGS -var=services={} -var=albs={} -var=terraform_backend_bucket_name=devops-project-terraform"
          
          # Create temp file for destroy output to check for lock errors
          TEMP_DESTROY_LOG=$(mktemp)
          trap "rm -f $TEMP_DESTROY_LOG" EXIT
          
          echo "::notice::[$(date +'%H:%M:%S')] Starting terraform destroy (this may take several minutes)..."
          echo "::notice::[$(date +'%H:%M:%S')] Destroy output will stream in real-time below..."
          
          # Execute destroy with real-time output streaming
          terraform -chdir="$PLAN_DIR" destroy -refresh=false -input=false -no-color $DESTROY_ARGS 2>&1 | tee "$TEMP_DESTROY_LOG"
          DESTROY_EXIT=${PIPESTATUS[0]}
          
          # Check for state lock errors
          if [ $DESTROY_EXIT -ne 0 ]; then
            if grep -qi "Error acquiring the state lock" "$TEMP_DESTROY_LOG" || grep -qi "Lock Info:" "$TEMP_DESTROY_LOG"; then
              echo "::warning::[$(date +'%H:%M:%S')] State lock detected. Attempting to extract lock ID..."
              
              # Try multiple patterns to extract lock ID
              LOCK_ID=""
              
              # Pattern 1: Look for "Lock Info:" followed by ID
              LOCK_ID=$(grep -A 10 "Lock Info:" "$TEMP_DESTROY_LOG" | grep -i "ID:" | awk '{print $2}' | head -1)
              
              # Pattern 2: Look for lock ID in error message
              if [ -z "$LOCK_ID" ]; then
                LOCK_ID=$(grep -i "lock.*id" "$TEMP_DESTROY_LOG" | sed -n 's/.*[Ii][Dd][:]\?[[:space:]]*\([a-f0-9-]\{36\}\).*/\1/p' | head -1)
              fi
              
              # Pattern 3: Extract UUID-like strings near lock messages
              if [ -z "$LOCK_ID" ]; then
                LOCK_ID=$(grep -i "lock" "$TEMP_DESTROY_LOG" | grep -oE '[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}' | head -1)
              fi
              
              if [ -n "$LOCK_ID" ]; then
                echo "::notice::[$(date +'%H:%M:%S')] Found lock ID: $LOCK_ID. Attempting force-unlock..."
                terraform -chdir="$PLAN_DIR" force-unlock -force "$LOCK_ID" -no-color 2>&1
                UNLOCK_EXIT=$?
                
                if [ $UNLOCK_EXIT -eq 0 ]; then
                  echo "::notice::[$(date +'%H:%M:%S')] Lock released successfully. Retrying destroy..."
                  
                  # Retry destroy with same args
                  echo "::notice::[$(date +'%H:%M:%S')] Retrying terraform destroy..."
                  terraform -chdir="$PLAN_DIR" destroy -refresh=false -input=false -no-color $DESTROY_ARGS 2>&1 | tee "$TEMP_DESTROY_LOG"
                  DESTROY_EXIT=${PIPESTATUS[0]}
                else
                  echo "::error::[$(date +'%H:%M:%S')] Failed to force-unlock. Lock ID: $LOCK_ID"
                  echo "::error::Please manually delete the lock in DynamoDB or use: terraform force-unlock -force $LOCK_ID"
                fi
              else
                echo "::warning::[$(date +'%H:%M:%S')] Could not extract lock ID from output. Showing relevant lines:"
                grep -i "lock" "$TEMP_DESTROY_LOG" | head -20
                echo "::error::Please manually check and remove the state lock in DynamoDB"
              fi
            fi
          fi
          
          # Re-enable exit on error and check final result
          set -e
          if [ $DESTROY_EXIT -ne 0 ]; then
            echo "::error::[$(date +'%H:%M:%S')] Terraform destroy failed with exit code $DESTROY_EXIT"
            exit $DESTROY_EXIT
          else
            echo "::notice::[$(date +'%H:%M:%S')] Terraform destroy completed successfully!"
          fi
