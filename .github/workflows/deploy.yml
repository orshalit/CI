name: Unified Deployment Pipeline (deploy)

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target Environment'
        required: true
        default: 'dev'
        type: choice
        options: [dev, staging, production]
      module_path:
        description: 'Terraform module to deploy (e.g., 01-vpc, 04-ecs-fargate)'
        required: true
        type: string
      action:
        description: 'Terraform action (plan, apply, or destroy)'
        required: true
        default: 'plan'
        type: choice
        options: [plan, apply, destroy]
      image_tag:
        description: 'Docker image tag for app deployments (optional, commit SHA)'
        required: false
        type: string
      application:
        description: 'Application to deploy (for app deployments, e.g., "test-app")'
        required: false
        default: 'all'
        type: string

  workflow_run:
    workflows: ["CI/CD Pipeline"]
    branches: [main]
    types:
      - completed
    # Note: workflow_run triggers on ANY completion (success/failure/cancelled)
    # We check the conclusion inside the job and exit early if CI failed
    # This ensures we only deploy when images are actually built

concurrency:
  # Use dynamic group based on trigger type - supports both workflow_dispatch and workflow_run
  group: deploy-${{ github.event_name == 'workflow_dispatch' && github.event.inputs.environment || 'dev' }}-${{ github.event_name == 'workflow_dispatch' && github.event.inputs.module_path || '04-ecs-fargate' }}
  cancel-in-progress: false  # Keep false for resilience - don't cancel in-progress deployments

jobs:
  deploy:
    name: ${{ github.event_name == 'workflow_dispatch' && format('{0} - {1}', github.event.inputs.action, github.event.inputs.module_path) || 'Deploy Application' }}
    runs-on: ubuntu-latest
    # Only run if CI workflow succeeded (for workflow_run triggers)
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
    
    permissions:
      id-token: write
      contents: read

    environment:
      name: ${{ github.event.inputs.environment || 'dev' }}

    steps:
      - name: Checkout CI repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.action != 'destroy') ||
          (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install backend dependencies
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.action != 'destroy') ||
          (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
        working-directory: applications/test-app/backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt

      - name: Run backend tests
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.action != 'destroy') ||
          (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
        working-directory: applications/test-app/backend
        run: |
          pytest tests/test_main.py

      - name: Set up Node
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.action != 'destroy') ||
          (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: 'npm'
          cache-dependency-path: applications/test-app/frontend/package-lock.json

      - name: Install frontend dependencies
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.action != 'destroy') ||
          (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
        working-directory: applications/test-app/frontend
        run: npm ci

      - name: Run frontend tests
        if: |
          (github.event_name == 'workflow_dispatch' && github.event.inputs.action != 'destroy') ||
          (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
        working-directory: applications/test-app/frontend
        env:
          CI: "true"
        run: npm run test:ci

      - name: Verify CI workflow succeeded (skip if failed/cancelled)
        if: github.event_name == 'workflow_run'
        run: |
          CI_WORKFLOW_STATUS="${{ github.event.workflow_run.conclusion }}"
          CI_WORKFLOW_ID="${{ github.event.workflow_run.id }}"
          echo "::notice::CI workflow ID: $CI_WORKFLOW_ID"
          echo "::notice::CI workflow conclusion: $CI_WORKFLOW_STATUS"
          if [ "$CI_WORKFLOW_STATUS" != "success" ]; then
            echo "::warning::CI workflow status: $CI_WORKFLOW_STATUS - skipping deployment"
            echo "::warning::Deployment only runs when CI workflow succeeds"
            echo "::warning::CI workflow URL: https://github.com/${{ github.repository }}/actions/runs/$CI_WORKFLOW_ID"
            # Exit with code 0 to mark job as successful (but skipped)
            # This prevents the workflow from showing as failed when CI fails
            exit 0
          fi
          echo "::notice::âœ“ CI workflow succeeded - proceeding with deployment"

      - name: Download artifacts from CI workflow
        if: |
          github.event_name == 'workflow_run' &&
          github.event.workflow_run.conclusion == 'success'
        uses: actions/download-artifact@v4
        with:
          name: build-version
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}
          path: .
        continue-on-error: true

      - name: Download built images list from CI workflow
        if: |
          github.event_name == 'workflow_run' &&
          github.event.workflow_run.conclusion == 'success'
        uses: actions/download-artifact@v4
        with:
          name: built-images
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}
          path: .
        continue-on-error: true

      - name: Check if images were built (skip deployment if not)
        if: |
          github.event_name == 'workflow_run' &&
          github.event.workflow_run.conclusion == 'success'
        id: check_images_built
        run: |
          CI_WORKFLOW_ID="${{ github.event.workflow_run.id }}"
          
          # Check if build-version.txt exists (proves images were built)
          if [ ! -f "build-version.txt" ]; then
            echo "::notice::build-version.txt artifact not found from CI workflow"
            echo "::notice::This means no images were built (likely only workflow/config files changed)"
            echo "::notice::Skipping deployment - no ECS resources to deploy"
            echo "::notice::CI workflow run: https://github.com/${{ github.repository }}/actions/runs/$CI_WORKFLOW_ID"
            echo "images_built=false" >> $GITHUB_OUTPUT
            exit 0  # Exit successfully - this is expected, not an error
          fi
          
          # Also check if built-images.txt has any actual images
          if [ -f "built-images.txt" ]; then
            IMAGE_COUNT=$(grep -v '^#' built-images.txt | grep -v '^$' | wc -l | tr -d ' ')
            if [ "$IMAGE_COUNT" -eq 0 ]; then
              echo "::notice::built-images.txt is empty - no images were built"
              echo "::notice::Skipping deployment - no ECS resources to deploy"
              echo "images_built=false" >> $GITHUB_OUTPUT
              exit 0  # Exit successfully - this is expected
            fi
          else
            # built-images.txt missing - assume no images built
            echo "::notice::built-images.txt not found - assuming no images were built"
            echo "::notice::Skipping deployment - no ECS resources to deploy"
            echo "images_built=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          BUILD_VERSION=$(cat build-version.txt)
          echo "::notice::âœ“ Build version artifact found: $BUILD_VERSION"
          echo "::notice::âœ“ Images were built successfully in CI workflow"
          echo "images_built=true" >> $GITHUB_OUTPUT

      - name: Set up dynamic variables
        if: |
          github.event_name == 'workflow_dispatch' ||
          (github.event_name == 'workflow_run' && steps.check_images_built.outputs.images_built == 'true')
        id: vars
        run: |
          # This step only runs if:
          # - Manual trigger (workflow_dispatch), OR
          # - Automatic trigger (workflow_run) AND images were built
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "::notice::Manual workflow dispatch trigger"
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
            echo "module_path=${{ github.event.inputs.module_path }}" >> $GITHUB_OUTPUT
            echo "action=${{ github.event.inputs.action }}" >> $GITHUB_OUTPUT
            # Use provided image_tag or fallback to generated tag (idempotent - same commit = same tag)
            # For manual dispatch, generate tag format matching CI workflow logic (dynamic, scalable)
            IMAGE_TAG="${{ github.event.inputs.image_tag }}"
            if [ -z "$IMAGE_TAG" ] || [ "$IMAGE_TAG" = "" ]; then
              # Generate tag format matching CI workflow logic (supports tags, main, and other branches)
              SHORT_SHA=$(git rev-parse --short HEAD)
              if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
                IMAGE_TAG="${{ github.ref_name }}"
              elif [[ "${{ github.ref }}" == refs/heads/main ]]; then
                IMAGE_TAG="main-${SHORT_SHA}"
              else
                IMAGE_TAG="dev-${SHORT_SHA}"
              fi
              echo "::notice::Generated image tag: $IMAGE_TAG"
            fi
            echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
            # Use provided application or default to 'all' (scalable - supports multi-app)
            APPLICATION="${{ github.event.inputs.application }}"
            if [ -z "$APPLICATION" ] || [ "$APPLICATION" = "" ]; then
              APPLICATION="all"
            fi
            echo "application=$APPLICATION" >> $GITHUB_OUTPUT
          else
            echo "::notice::Automated workflow run trigger"
            echo "environment=dev" >> $GITHUB_OUTPUT # Or determine dynamically if needed
            echo "module_path=04-ecs-fargate" >> $GITHUB_OUTPUT
            echo "action=apply" >> $GITHUB_OUTPUT
            # Use build version from CI workflow artifact (single source of truth)
            # This ensures tag format always matches what CI built
            if [ -f "build-version.txt" ]; then
              IMAGE_TAG=$(cat build-version.txt)
              echo "::notice::Using build version from CI workflow: $IMAGE_TAG"
            else
              # Fallback: Generate tag format matching CI workflow logic (dynamic, scalable)
              SHORT_SHA=$(git rev-parse --short HEAD)
              if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
                IMAGE_TAG="${{ github.ref_name }}"
              elif [[ "${{ github.ref }}" == refs/heads/main ]]; then
                IMAGE_TAG="main-${SHORT_SHA}"
              else
                IMAGE_TAG="dev-${SHORT_SHA}"
              fi
              echo "::notice::Generated image tag (fallback): $IMAGE_TAG"
            fi
            echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
            echo "application=all" >> $GITHUB_OUTPUT
          fi

      - name: Debug - Check repository secrets
        run: |
          echo "::notice::Checking repository secrets..."
          DEVOPS_REPO_OWNER="${{ secrets.DEVOPS_REPO_OWNER }}"
          DEVOPS_REPO_NAME="${{ secrets.DEVOPS_REPO_NAME }}"
          
          if [ -n "$DEVOPS_REPO_OWNER" ]; then
            echo "DEVOPS_REPO_OWNER exists: yes"
            echo "DEVOPS_REPO_OWNER length: ${#DEVOPS_REPO_OWNER}"
          else
            echo "DEVOPS_REPO_OWNER exists: no"
            echo "DEVOPS_REPO_OWNER length: 0"
          fi
          
          if [ -n "$DEVOPS_REPO_NAME" ]; then
            echo "DEVOPS_REPO_NAME exists: yes"
            echo "DEVOPS_REPO_NAME length: ${#DEVOPS_REPO_NAME}"
          else
            echo "DEVOPS_REPO_NAME exists: no"
            echo "DEVOPS_REPO_NAME length: 0"
          fi
          
          REPO_OWNER="${DEVOPS_REPO_OWNER:-orshalit}"
          REPO_NAME="${DEVOPS_REPO_NAME:-projectdevops}"
          REPO_FORMAT="${REPO_OWNER}/${REPO_NAME}"
          echo "Computed repo format: $REPO_FORMAT"
          
          if [ -z "$DEVOPS_REPO_OWNER" ] || [ -z "$DEVOPS_REPO_NAME" ]; then
            echo "::error::Repository secrets are missing or empty!"
            echo "::error::DEVOPS_REPO_OWNER: '${DEVOPS_REPO_OWNER:-<empty>}'"
            echo "::error::DEVOPS_REPO_NAME: '${DEVOPS_REPO_NAME:-<empty>}'"
            echo "::error::Please set DEVOPS_REPO_OWNER and DEVOPS_REPO_NAME secrets in repository settings"
            exit 1
          fi

      - name: Terraform Setup
        uses: ./.github/actions/terraform-setup
        with:
          devops_repo: ${{ secrets.DEVOPS_REPO_OWNER || 'orshalit' }}/${{ secrets.DEVOPS_REPO_NAME || 'projectdevops' }}
          devops_repo_key: ${{ secrets.DEVOPS_REPO_KEY }}
          aws_role_arn: ${{ secrets.AWS_ROLE_ARN }}
          aws_region: ${{ secrets.AWS_REGION }}
          terraform_version: '1.6.0'
        env:
          DEVOPS_REPO_OWNER: ${{ secrets.DEVOPS_REPO_OWNER || 'orshalit' }}
          DEVOPS_REPO_NAME: ${{ secrets.DEVOPS_REPO_NAME || 'projectdevops' }}

      - name: Cache Dhall binaries
        id: cache-dhall
        uses: actions/cache@v4
        with:
          path: dhall/cache/binaries
          key: dhall-binaries-${{ runner.os }}-1.41.2
          restore-keys: |
            dhall-binaries-${{ runner.os }}-

      - name: Update Dhall imports to use local DEVOPS checkout
        run: |
          echo "::notice::DEVOPS repository is checked out to ./DEVOPS"
          echo "::notice::Verifying Dhall imports resolve to local DEVOPS checkout..."
          
          # Verify DEVOPS checkout
          if [ ! -f "DEVOPS/config/types/Service.dhall" ]; then
            echo "::error::DEVOPS/config/types/Service.dhall not found!"
            echo "::error::DEVOPS checkout may have failed"
            ls -la DEVOPS/ || echo "DEVOPS directory not found"
            exit 1
          fi
          echo "âœ“ DEVOPS/config/types/Service.dhall found"
          echo "âœ“ Dhall imports rely on relative paths; no rewrite needed."

      - name: Install Dhall, dhall-json, and jq
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          chmod +x scripts/install-dhall-with-fallback.sh
          scripts/install-dhall-with-fallback.sh

      - name: Generate services.generated.json from Dhall (Terraform JSON format)
        id: generate_services_json
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/04-ecs-fargate"
          SERVICES_JSON="$PLAN_DIR/services.generated.json"
          SERVICES_TFVARS_JSON_DHALL="dhall/services.tfvarsJSON.dhall"
          
          echo "::notice::Generating services.generated.json from Dhall (Terraform JSON format)..."
          
          # Check if Dhall converter exists
          if [ ! -f "$SERVICES_TFVARS_JSON_DHALL" ]; then
            echo "::error::Dhall JSON converter not found at $SERVICES_TFVARS_JSON_DHALL"
            echo "::error::This file should exist in the CI repository"
            echo "::error::Ensure the CI repository has been checked out and contains Dhall service definitions."
            exit 1
          fi
          
          # Generate Terraform-compatible JSON (with "services" key)
          # Uses dhall-to-json for native JSON output - no string templates!
          mkdir -p "$PLAN_DIR"
          echo "::notice::Converting Dhall to Terraform JSON format (validates types automatically)..."
          dhall-to-json --file "$SERVICES_TFVARS_JSON_DHALL" > "$SERVICES_JSON" || {
            echo "::error::Failed to generate Terraform JSON from Dhall"
            echo "::error::Checking Dhall file..."
            dhall type --file "$SERVICES_TFVARS_JSON_DHALL" || true
            exit 1
          }
          
          if [ ! -f "$SERVICES_JSON" ] || [ ! -s "$SERVICES_JSON" ]; then
            echo "::error::Failed to generate services.generated.json"
            echo "::error::Check the Dhall file for syntax or type errors"
            exit 1
          fi
          
          # Validate JSON structure
          if ! jq empty "$SERVICES_JSON" 2>/dev/null; then
            echo "::error::Generated JSON is invalid"
            cat "$SERVICES_JSON"
            exit 1
          fi
          
          echo "::notice::âœ“ Generated $SERVICES_JSON (Terraform JSON format)"
          echo "::group::Generated services.generated.json (preview)"
          cat "$SERVICES_JSON" | jq '.services | keys | length' | xargs -I {} echo "Services count: {}"
          cat "$SERVICES_JSON" | jq '.services | keys' | head -20
          echo "::endgroup::"
          echo "services_json_generated=true" >> $GITHUB_OUTPUT

      - name: Verify Image Exists
        id: verify_image
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate' &&
          steps.vars.outputs.image_tag != '' &&
          steps.vars.outputs.image_tag != 'null'
        run: |
          IMAGE_TAG="${{ steps.vars.outputs.image_tag }}"
          SERVICES_JSON="DEVOPS/live/${{ steps.vars.outputs.environment }}/04-ecs-fargate/services.generated.json"
          
          if [ ! -f "$SERVICES_JSON" ]; then
            echo "::error::Services JSON file not found at $SERVICES_JSON after generation step."
            echo "::error::This should not happen - the generation step should have created it."
            exit 1
          fi

          # Login to GHCR first to inspect manifests
          echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin || {
            echo "::error::Failed to login to GHCR.io. Ensure GITHUB_TOKEN has 'packages:read' permission."
            exit 1
          }

          # Extract container_image values from the JSON file using jq
          # JSON structure: { "services": { "key": { "container_image": "..." } } }
          IMAGE_NAMES=$(jq -r '.services[].container_image' "$SERVICES_JSON")

          if [ -z "$IMAGE_NAMES" ]; then
            echo "::error::Could not extract any container_image from $SERVICES_JSON."
            exit 1
          fi

          echo "::notice::Verifying existence of the following images with tag: $IMAGE_TAG"
          echo "$IMAGE_NAMES"

          ALL_IMAGES_EXIST=true
          for IMG_NAME in $IMAGE_NAMES; do
            # container_image already contains full path (e.g., ghcr.io/owner/test-app-backend)
            FULL_IMAGE_PATH="${IMG_NAME}:${IMAGE_TAG}"
            echo "::group::Checking for $FULL_IMAGE_PATH"
            if docker manifest inspect "$FULL_IMAGE_PATH" > /dev/null 2>&1; then
              echo "::notice::âœ“ Docker image $FULL_IMAGE_PATH found."
            else
              echo "::error::âŒ Docker image $FULL_IMAGE_PATH NOT found in GitHub Container Registry."
              ALL_IMAGES_EXIST=false
            fi
            echo "::endgroup::"
          done

          if [ "$ALL_IMAGES_EXIST" = "false" ]; then
            echo "::error::One or more required Docker images are missing for tag '$IMAGE_TAG'."
            echo "::error::This deployment cannot proceed. This is expected if the CI pipeline only built a subset of images."
            echo "::error::Please check the CI/CD Pipeline workflow run for commit ${{ github.sha }} to see which images were built."
            echo "::error::CI Workflow URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id || github.run_id }}"
            exit 1
          fi

          echo "::notice::âœ“ All required Docker images found in registry."
          echo "image_check_passed=true" >> $GITHUB_OUTPUT

      - name: Generate Terraform Config from Dhall
        id: generate_config
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply' || steps.vars.outputs.action == 'destroy'
        run: |
          ENV="${{ steps.vars.outputs.environment }}"
          MODULE_PATH="${{ steps.vars.outputs.module_path }}"
          MODULE_NAME=$(basename "$MODULE_PATH")
          OUTPUT_FILE="DEVOPS/live/${ENV}/${MODULE_PATH}/terraform.tfvars.json"

          echo "::notice::Generating config for ${ENV}/${MODULE_NAME}..."

          # Check if module-specific converter exists (preferred method)
          MODULE_TFVARS_DHALL="DEVOPS/config/environments/${ENV}/${MODULE_NAME}.tfvars.dhall"
          if [ -f "$MODULE_TFVARS_DHALL" ]; then
            echo "::notice::Using module-specific converter: $MODULE_TFVARS_DHALL"
            dhall-to-json --file "$MODULE_TFVARS_DHALL" > "${OUTPUT_FILE}" || {
              echo "::error::Failed to convert $MODULE_TFVARS_DHALL to JSON"
              exit 1
            }
          else
            echo "::notice::Module-specific converter not found, extracting from dev.dhall..."
            # Fallback: Extract module config directly from dev.dhall JSON
            ENV_JSON=$(dhall-to-json --file DEVOPS/config/environments/${ENV}.dhall) || {
              echo "::error::Failed to convert DEVOPS/config/environments/${ENV}.dhall to JSON"
              exit 1
            }
            
            # Check if module exists
            MODULE_EXISTS=$(echo "$ENV_JSON" | jq -r "has(\"${MODULE_NAME}\")")
            if [ "$MODULE_EXISTS" != "true" ]; then
              echo "::error::Module '${MODULE_NAME}' not found in DEVOPS/config/environments/${ENV}.dhall"
              echo "::error::Available modules:"
              echo "$ENV_JSON" | jq -r 'keys[]' | sed 's/^/  - /' || true
              exit 1
            fi
            
            # Extract module config
            MODULE_CONFIG=$(echo "$ENV_JSON" | jq -c ".[\"${MODULE_NAME}\"]")
            
            # Validate that we got a valid object
            if [ -z "$MODULE_CONFIG" ] || [ "$MODULE_CONFIG" = "null" ]; then
              echo "::error::Module '${MODULE_NAME}' exists but has null value"
              exit 1
            fi
            
            echo "$MODULE_CONFIG" > "${OUTPUT_FILE}"
          fi
          
          # Validate the output file is valid JSON
          if ! jq empty "${OUTPUT_FILE}" 2>/dev/null; then
            echo "::error::Generated terraform.tfvars.json is not valid JSON"
            cat "${OUTPUT_FILE}"
            exit 1
          fi

          echo "::notice::âœ“ Generated ${OUTPUT_FILE}"
          echo "::group::Generated terraform.tfvars.json"
          cat "${OUTPUT_FILE}"
          echo "::endgroup::"
          echo "var_file_path=${OUTPUT_FILE}" >> $GITHUB_OUTPUT

      - name: Terraform Format Check and Auto-Fix
        id: fmt
        if: |
          steps.vars.outputs.environment != '' &&
          steps.vars.outputs.module_path != '' &&
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply' || steps.vars.outputs.action == 'destroy')
        run: |
          TERRAFORM_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Checking Terraform formatting in $TERRAFORM_DIR..."
          
          # First, check formatting without modifying files (faster feedback)
          if terraform fmt -check -recursive "$TERRAFORM_DIR" 2>&1; then
            echo "::notice::âœ“ All Terraform files are properly formatted"
            echo "needs_formatting=false" >> $GITHUB_OUTPUT
            echo "changes_made=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "::warning::Formatting issues detected. Auto-formatting files..."
          
          # Format all Terraform files
          terraform fmt -recursive "$TERRAFORM_DIR"
          
          # Check if any files were modified by comparing before/after
          cd DEVOPS
          
          # Check git status for changes in the terraform directory
          if git diff --quiet "live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"; then
            echo "::notice::âœ“ Files were auto-formatted and are now correct"
            echo "needs_formatting=false" >> $GITHUB_OUTPUT
            echo "changes_made=false" >> $GITHUB_OUTPUT
          else
            echo "needs_formatting=true" >> $GITHUB_OUTPUT
            echo "changes_made=true" >> $GITHUB_OUTPUT
            
            # Show what changed
            echo "::notice::â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
            echo "::notice::â•‘  ðŸ”§ AUTO-FORMATTING TERRAFORM FILES  ðŸ”§  â•‘"
            echo "::notice::â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo ""
            echo "::notice::The following files were auto-formatted:"
            git status --short "live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}" | while read -r line; do
              echo "::notice::  â€¢ $line"
            done
            echo ""
            echo "::group::Show formatted changes"
            git diff "live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
            echo "::endgroup::"
          fi

      - name: Note formatting changes (no commit during deploy)
        if: steps.fmt.outputs.needs_formatting == 'true' && steps.fmt.outputs.changes_made == 'true'
        run: |
          echo "::notice::Files were auto-formatted but NOT committed during deployment"
          echo "::notice::This prevents stale plans caused by state changes from commits"
          echo "::notice::To commit formatting:"
          echo "::notice::  1. Use the 'Terraform Format Check' workflow with auto-fix enabled"
          echo "::notice::  2. Or run 'terraform fmt -recursive' locally and commit manually"

      - name: Continue deployment (formatting fixed)
        if: steps.fmt.outputs.needs_formatting == 'true' && steps.fmt.outputs.changes_made == 'true'
        run: |
          echo "::notice::âœ“ Terraform formatting issues have been auto-fixed"
          echo "::notice::Deployment will continue with properly formatted files"

      - name: Verify DEVOPS Repository Version
        id: verify_devops
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        run: |
          cd DEVOPS
          echo "::notice::DEVOPS Repository Information:"
          echo "::notice::  Branch: $(git rev-parse --abbrev-ref HEAD)"
          echo "::notice::  Commit: $(git rev-parse HEAD)"
          echo "::notice::  Date: $(git log -1 --format=%ci)"
          echo "::notice::  Message: $(git log -1 --format=%s)"
          
          # Store commit hash for reference
          echo "commit_hash=$(git rev-parse HEAD)" >> $GITHUB_OUTPUT
          
          # Verify we're on main branch
          CURRENT_BRANCH=$(git rev-parse --abbrev-ref HEAD)
          if [ "$CURRENT_BRANCH" != "main" ]; then
            echo "::warning::DEVOPS repo is on branch '$CURRENT_BRANCH', not 'main'"
          fi
          
          # Check if repo is clean (no uncommitted changes)
          if ! git diff-index --quiet HEAD --; then
            echo "::warning::DEVOPS repo has uncommitted changes"
            git status --short
          fi

      - name: Terraform Init
        id: init
        run: terraform -chdir=DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }} init -upgrade

      - name: Validate Variable Declarations
        id: validate_vars
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          VAR_FILE="$PLAN_DIR/variables.tf"
          
          echo "::notice::Validating variable declarations for ECS Fargate module..."
          
          # Check if variables.tf exists
          if [ ! -f "$VAR_FILE" ]; then
            echo "::error::variables.tf not found at $VAR_FILE"
            exit 1
          fi
          
          # For ECS Fargate deployments, we always pass global_image_tag via extra_vars
          # Check if image_tag is set (it should always be, either from input or github.sha)
          IMAGE_TAG_VALUE="${{ steps.vars.outputs.image_tag }}"
          REQUIRED_VAR="global_image_tag"
          
          if [ -n "$IMAGE_TAG_VALUE" ] && [ "$IMAGE_TAG_VALUE" != "null" ] && [ "$IMAGE_TAG_VALUE" != "" ]; then
            echo "::notice::Image tag will be passed as: $REQUIRED_VAR=$IMAGE_TAG_VALUE"
            echo "::notice::Checking for required variable declaration: $REQUIRED_VAR"
            
            # Check if variable is declared in variables.tf
            if ! grep -qE "^\s*variable\s+\"$REQUIRED_VAR\"" "$VAR_FILE"; then
              echo "::error::â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
              echo "::error::â•‘  âš ï¸  MISSING VARIABLE DECLARATION  âš ï¸  â•‘"
              echo "::error::â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
              echo ""
              echo "::error::Variable '$REQUIRED_VAR' is being passed via extra_vars but is not declared in:"
              echo "::error::  $VAR_FILE"
              echo ""
              echo "::error::The workflow will pass: $REQUIRED_VAR=$IMAGE_TAG_VALUE"
              echo "::error::But Terraform configuration doesn't declare this variable."
              echo ""
              echo "::error::DEVOPS Repository Commit: ${{ steps.verify_devops.outputs.commit_hash }}"
              echo ""
              echo "::error::Current variables.tf content (first 50 lines):"
              head -50 "$VAR_FILE" | sed 's/^/::error::  /' || true
              echo ""
              echo "::error::Solution:"
              echo "::error::  1. Ensure DEVOPS repository has the latest code with variable declaration"
              echo "::error::  2. Add variable declaration to $VAR_FILE:"
              echo "::error::     variable \"$REQUIRED_VAR\" {"
              echo "::error::       description = \"Optional override tag applied to all services\""
              echo "::error::       type        = string"
              echo "::error::       default     = null"
              echo "::error::     }"
              echo "::error::  3. Commit and push to DEVOPS repository"
              echo "::error::  4. Re-run this workflow"
              exit 1
            else
              echo "::notice::âœ“ Variable '$REQUIRED_VAR' is properly declared in $VAR_FILE"
            fi
          else
            echo "::warning::Image tag is empty or null, skipping variable validation"
          fi

      - name: Check for terraform.tfvars
        id: check_tfvars
        run: |
          if [ -f "DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}/terraform.tfvars" ]; then
            echo "tfvars_exists=true" >> $GITHUB_OUTPUT
          else
            echo "tfvars_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Check for services.generated.json
        id: check_services_json
        run: |
          if [ -f "DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}/services.generated.json" ]; then
            echo "services_json_exists=true" >> $GITHUB_OUTPUT
          else
            echo "services_json_exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Filter services by application and built images (modular & resilient)
        id: filter_services
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          APPLICATION="${{ steps.vars.outputs.application }}"
          SERVICES_JSON="$PLAN_DIR/services.generated.json"
          
          # Load built images list (modular: only deploy services whose images were built)
          # GitHub Actions merges artifacts with the same name from matrix jobs
          BUILT_IMAGES_FILE="built-images.txt"
          BUILT_IMAGES=""
          IMAGE_COUNT=0
          
          if [ -f "$BUILT_IMAGES_FILE" ]; then
            BUILT_IMAGES=$(grep -v '^#' "$BUILT_IMAGES_FILE" | grep -v '^$' | sort -u || echo "")
            IMAGE_COUNT=$(echo "$BUILT_IMAGES" | grep -v '^$' | wc -l | tr -d ' ')
          fi
          
          # Fail-safe behavior: Different handling for workflow_run vs workflow_dispatch
          # workflow_run: CI workflow should have built images - fail if missing/empty
          # workflow_dispatch: Manual deployment - allow fallback with warnings
          if [ "${{ github.event_name }}" == "workflow_run" ]; then
            if [ ! -f "$BUILT_IMAGES_FILE" ] || [ "$IMAGE_COUNT" -eq 0 ]; then
              echo "::error::Built images list required for workflow_run deployment"
              echo "::error::CI workflow should have built images. This indicates:"
              echo "::error::  1. All builds failed (check CI workflow logs)"
              echo "::error::  2. Artifact download failed (retry deployment)"
              echo "::error::  3. CI workflow didn't build any images (fix CI workflow)"
              echo "::error::"
              echo "::error::Deployment cannot proceed without knowing which images were built."
              echo "::error::This prevents deploying services with missing images."
              exit 1
            fi
            echo "::notice::Found $IMAGE_COUNT built image(s) from CI workflow:"
            echo "$BUILT_IMAGES" | sed 's/^/  - /'
          else
            # workflow_dispatch (manual deployment)
            if [ ! -f "$BUILT_IMAGES_FILE" ] || [ "$IMAGE_COUNT" -eq 0 ]; then
              echo "::warning::Built images list not found or empty (manual deployment)"
              echo "::warning::This will deploy all services using existing images in registry"
              echo "::warning::âš ï¸  WARNING: Images may be outdated if CI workflow didn't run"
              echo "::warning::"
              echo "::warning::To deploy with fresh images:"
              echo "::warning::  1. Run CI workflow first to build images"
              echo "::warning::  2. Then run this deployment workflow"
              echo "::warning::"
              echo "::warning::Proceeding with deployment of all services..."
            else
              echo "::notice::Found $IMAGE_COUNT built image(s):"
              echo "$BUILT_IMAGES" | sed 's/^/  - /'
            fi
          fi
          
          # Filter services.generated.json by application first
          # Note: services.generated.json has structure {services: {...}}
          FILTERED_JSON=$(mktemp)
          if [ "$APPLICATION" != "all" ] && [ -n "$APPLICATION" ]; then
            echo "::notice::Filtering by application: $APPLICATION"
            jq --arg app "$APPLICATION" '.services | to_entries | map(select(.value.application == $app)) | from_entries' \
              "$SERVICES_JSON" > "$FILTERED_JSON" || {
              echo "::error::Failed to filter services by application"
              exit 1
            }
          else
            echo "::notice::Application filter: all (deploying all applications)"
            jq '.services' "$SERVICES_JSON" > "$FILTERED_JSON"
          fi
          
          # Now filter by built images (modular: only services whose images exist)
          # For workflow_run: We already validated images exist above (would have failed)
          # For workflow_dispatch: May proceed without images (manual override)
          if [ "$IMAGE_COUNT" -gt 0 ] && [ -n "$BUILT_IMAGES" ]; then
            echo "::notice::Filtering services to only those whose images were built (modular deployment)"
            
            # Match image_repo against built images
            # image_repo format: "ghcr.io/owner/image-name" 
            # built images format: "image-name" (one per line)
            # Extract last part of image_repo (after last /) and match
            
            FILTERED_JSON_TEMP=$(mktemp)
            # Build jq filter expression
            FILTER_PARTS=""
            for IMG in $BUILT_IMAGES; do
              if [ -z "$FILTER_PARTS" ]; then
                FILTER_PARTS="(.value.container_image | split(\"/\") | .[-1] == \"$IMG\")"
              else
                FILTER_PARTS="$FILTER_PARTS or (.value.container_image | split(\"/\") | .[-1] == \"$IMG\")"
              fi
            done
            
            jq "to_entries | map(select($FILTER_PARTS)) | from_entries" "$FILTERED_JSON" > "$FILTERED_JSON_TEMP" || {
              echo "::error::Failed to filter services by built images"
              exit 1
            }
            
            FILTERED_COUNT=$(jq 'length' "$FILTERED_JSON_TEMP" 2>/dev/null || echo "0")
            BASE_COUNT=$(jq 'length' "$FILTERED_JSON" 2>/dev/null || echo "0")
            
            if [ "$FILTERED_COUNT" -eq 0 ] && [ "$BASE_COUNT" -gt 0 ]; then
              echo "::error::No services match built images!"
              echo "::error::Built images:"
              echo "$BUILT_IMAGES" | sed 's/^/  - /'
              echo "::error::Available services (container_image):"
              jq -r '.[].container_image' "$FILTERED_JSON" | sed 's/^/  - /' || echo "  (none)"
              exit 1
            fi
            
            echo "::notice::âœ“ Modular filtering: $FILTERED_COUNT/$BASE_COUNT service(s) match built images"
            mv "$FILTERED_JSON_TEMP" "$FILTERED_JSON"
          else
            # No built images available - fallback behavior
            # For workflow_run: This should never happen (we fail above)
            # For workflow_dispatch: User explicitly triggered, allow fallback
            FILTERED_COUNT=$(jq 'length' "$FILTERED_JSON" 2>/dev/null || echo "0")
            if [ "${{ github.event_name }}" == "workflow_run" ]; then
              # This should never execute (we fail above), but safety check
              echo "::error::Internal error: workflow_run should have failed above"
              exit 1
            else
              echo "::warning::No built images available - deploying all services (not modular)"
              echo "::warning::This is expected for manual deployments without CI workflow"
              echo "::warning::Services will use existing images in registry (may be outdated)"
            fi
          fi
          
          # Convert filtered JSON to Terraform format (with "services" key)
          FILTERED_FILE="$PLAN_DIR/services.generated.json.filtered"
          FILTERED_COUNT=$(jq 'length' "$FILTERED_JSON" 2>/dev/null || echo "0")
          
          if [ "$FILTERED_COUNT" -eq 0 ]; then
            echo "::error::No services to deploy after filtering"
            exit 1
          fi
          
          # Wrap filtered services in "services" key for Terraform
          jq '{services: .}' "$FILTERED_JSON" > "$FILTERED_FILE" || {
            echo "::error::Failed to convert filtered JSON to Terraform format"
            exit 1
          }
          
          FILTERED_KEYS=$(jq -r '.services | keys[]' "$FILTERED_FILE" 2>/dev/null || echo "")
          
          # Validate the filtered file
          if ! jq empty "$FILTERED_FILE" 2>/dev/null; then
            echo "::error::Filtered JSON file is invalid"
            cat "$FILTERED_FILE"
            exit 1
          fi
          
          echo "filtered_services_file=services.generated.json.filtered" >> $GITHUB_OUTPUT
          echo "services_count=$FILTERED_COUNT" >> $GITHUB_OUTPUT
          
          echo "::group::Services to deploy (modular)"
          echo "$FILTERED_KEYS" | sed 's/^/  - /'
          echo "::endgroup::"
          
          echo "::notice::âœ“ Created filtered services JSON with $FILTERED_COUNT service(s)"
          echo "::group::Filtered services preview"
          cat "$FILTERED_FILE" | jq '.services | keys' | head -20
          echo "::endgroup::"

      - name: Check if services is empty
        id: check_services_empty
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          # Check if services.generated.json exists and has non-empty services
          if [ -f "$PLAN_DIR/services.generated.json" ]; then
            # Check if services map is empty using jq
            SERVICE_COUNT=$(jq '.services | length' "$PLAN_DIR/services.generated.json" 2>/dev/null || echo "0")
            if [ "$SERVICE_COUNT" -eq 0 ]; then
              echo "services_empty=true" >> $GITHUB_OUTPUT
              echo "::notice::Services map is empty, will exclude from var files to avoid Terraform crash"
            else
              echo "services_empty=false" >> $GITHUB_OUTPUT
              echo "::notice::Services map has $SERVICE_COUNT service(s)"
            fi
          else
            echo "services_empty=true" >> $GITHUB_OUTPUT
            echo "::notice::services.generated.json not found"
          fi

      - name: Terraform Validate
        id: validate
        # Skip validate for ECS Fargate module entirely to avoid Terraform crash
        # This is a known Terraform bug with marked values in conditional expressions
        # terraform plan will catch all validation errors anyway
        if: steps.vars.outputs.module_path != '04-ecs-fargate'
        run: terraform -chdir=DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }} validate -no-color || echo "Terraform validate failed (non-blocking); relying on plan for validation."

      - name: Build var files list
        id: build_var_files
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          VAR_FILES=""
          
          # Always use the generated terraform.tfvars.json
          VAR_FILES="terraform.tfvars.json"
          
          # For ECS module, also include services.generated.json if it exists
          if [ "${{ steps.vars.outputs.module_path }}" == "04-ecs-fargate" ] && [ -f "$PLAN_DIR/services.generated.json" ]; then
            # Check if services list is empty
            if jq -e '. | length > 0' "$PLAN_DIR/services.generated.json" > /dev/null; then
               VAR_FILES="$VAR_FILES services.generated.json"
               echo "services_empty=false" >> $GITHUB_OUTPUT
            else
               echo "::notice::Services JSON is empty, will not be included."
               echo "services_empty=true" >> $GITHUB_OUTPUT
            fi
          else
            echo "services_empty=true" >> $GITHUB_OUTPUT
          fi
          
          echo "var_files=$VAR_FILES" >> $GITHUB_OUTPUT

      - name: Check for existing services in state
        id: check_state_services
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.check_services_empty.outputs.services_empty == 'true' &&
          steps.vars.outputs.module_path == '04-ecs-fargate'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Checking Terraform state for existing services..."
          
          # Initialize Terraform to access state (without var files to avoid crash)
          cd "$PLAN_DIR"
          terraform init -no-color -input=false > /dev/null 2>&1 || true
          
          # Check if state has any ECS services
          # Use terraform state list to find services (safe even with empty services config)
          if terraform state list -no-color 2>/dev/null | grep -q "aws_ecs_service.services\["; then
            EXISTING_SERVICES=$(terraform state list -no-color 2>/dev/null | grep "aws_ecs_service.services\[" || echo "")
            
            echo ""
            echo "::error::â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
            echo "::error::â•‘  âš ï¸  DEPLOYMENT BLOCKED: DANGEROUS CONFIGURATION DETECTED  âš ï¸  â•‘"
            echo "::error::â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo ""
            echo "::error::Problem: services = {} (empty) but Terraform state has existing services!"
            echo ""
            echo "::error::If you proceed, Terraform will DESTROY all existing services:"
            echo "$EXISTING_SERVICES" | while read -r service; do
              echo "::error::  âŒ $service"
            done
            echo ""
            echo "::error::Why this is blocked:"
            echo "::error::  â€¢ Config says: 'deploy zero services' (services = [])"
            echo "::error::  â€¢ State says: 'these services exist'"
            echo "::error::  â€¢ Terraform will destroy existing services to match empty config"
            echo "::error::  â€¢ This is almost certainly a mistake, not intentional"
            echo ""
            echo "::error::Solution - Generate services configuration first:"
            echo "::error::  1. Go to: Actions â†’ 'Generate Service Config'"
            echo "::error::  2. Run the workflow to generate services.generated.json from Dhall"
            echo "::error::  3. Review and merge the PR in DEVOPS repository"
            echo "::error::  4. Then run this deployment workflow again"
            echo ""
            echo "::error::This ensures services in config match your Dhall definitions."
            exit 1
          else
            echo "::notice::âœ“ No existing services in state"
            echo "::notice::âœ“ Safe to proceed with empty services config (fresh deployment)"
          fi

      - name: Comprehensive State Validation and Auto-Import
        id: validate_state
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate' &&
          steps.check_services_empty.outputs.services_empty != 'true'
        continue-on-error: true
        timeout-minutes: 5
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          if [ -f "scripts/validate-and-import-state.sh" ]; then
            chmod +x scripts/validate-and-import-state.sh
            scripts/validate-and-import-state.sh "$PLAN_DIR" "${{ steps.vars.outputs.environment }}" "${{ steps.vars.outputs.module_path }}" || {
              EXIT_CODE=$?
              if [ $EXIT_CODE -eq 1 ]; then
                echo "::error::State validation found resources that couldn't be imported"
                echo "::error::These will cause 'already exists' errors. Manual intervention may be needed."
              fi
              exit 0  # Don't fail workflow, but warn
            }
          else
            echo "::warning::State validation script not found, skipping comprehensive validation"
          fi

      - name: Terraform Refresh State (Pre-Plan Sync)
        id: refresh
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        timeout-minutes: 5
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          REFRESH_ARGS="-no-color"
          
          # Check if state exists - if not, skip refresh (fresh deployment)
          if ! terraform -chdir="$PLAN_DIR" state list -no-color >/dev/null 2>&1; then
            echo "::notice::No Terraform state found - skipping refresh (fresh deployment)"
            echo "::notice::This is expected when starting with a new/empty state"
            exit 0
          fi
          
          # Use the same var files logic as plan (which excludes empty services.generated.json)
          # This prevents Terraform crash when services = {} (empty services map)
          VAR_FILES="${{ steps.build_var_files.outputs.var_files }}"
          
          if [ -n "$VAR_FILES" ]; then
            for var_file in $VAR_FILES; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
              REFRESH_ARGS="$REFRESH_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          echo "::notice::ðŸ”„ Refreshing Terraform state to sync with AWS (preventing state drift)..."
          if [ "${{ steps.check_services_empty.outputs.services_empty }}" == "true" ]; then
            echo "::notice::Services map is empty - refreshing infrastructure only (services excluded to avoid crash)"
          else
            echo "::notice::Refreshing all resources including services"
          fi
          
          # Use refresh-only mode to update state without making changes
          # This syncs state with actual AWS resources
          # Fail on errors - configuration errors should be caught here
          timeout 300 terraform -chdir="$PLAN_DIR" apply -refresh-only -auto-approve $REFRESH_ARGS 2>&1 | tee /tmp/refresh_output.txt || {
            REFRESH_EXITCODE=$?
            if [ $REFRESH_EXITCODE -eq 124 ]; then
              echo "::error::State refresh timed out after 5 minutes"
              exit 124
            else
              # Check if refresh-only failed because there are no changes (exit code 0 from apply -refresh-only)
              if grep -q "No changes" /tmp/refresh_output.txt 2>/dev/null; then
                echo "::notice::âœ“ State is already in sync with AWS (no changes needed)"
                exit 0
              else
                echo "::error::State refresh failed with exit code $REFRESH_EXITCODE"
                echo "::error::Refresh output:"
                cat /tmp/refresh_output.txt 2>/dev/null || true
                echo "::error::This indicates a configuration error that must be fixed before proceeding"
                exit $REFRESH_EXITCODE
              fi
            fi
          }
          
          # Check if refresh-only made any changes
          STATE_UPDATED=false
          if grep -q "No changes" /tmp/refresh_output.txt 2>/dev/null; then
            echo "::notice::âœ“ State is already in sync with AWS"
          else
            echo "::notice::âœ“ State refreshed - updated to match AWS resources"
            STATE_UPDATED=true
            # Show what was updated
            if grep -q "updated in-place" /tmp/refresh_output.txt 2>/dev/null; then
              echo "::notice::Resources updated in state:"
              grep "updated in-place" /tmp/refresh_output.txt | head -5 || true
            fi
            # Check if any resources were added to state (imported via refresh)
            if grep -q "has been imported" /tmp/refresh_output.txt 2>/dev/null; then
              echo "::notice::Resources imported during refresh:"
              grep "has been imported" /tmp/refresh_output.txt | head -5 || true
            fi
          fi
          
          # If state was updated, invalidate any existing plan file
          # This ensures plan is recreated with fresh state
          if [ "$STATE_UPDATED" = "true" ] && [ -f "$PLAN_DIR/tfplan" ]; then
            echo "::notice::âš  State was updated - existing plan file will be invalidated"
            echo "::notice::Plan will be recreated with fresh state"
            rm -f "$PLAN_DIR/tfplan" || true
          fi
          
          rm -f /tmp/refresh_output.txt

      - name: Detect State Drift (Target Groups)
        id: detect_drift
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          # Only check for ECS Fargate module
          if [[ "${{ steps.vars.outputs.module_path }}" == "04-ecs-fargate" ]]; then
            echo "::notice::Target group health check validation is included in comprehensive pre-apply validation"
          else
            echo "::notice::State drift detection only runs for ECS Fargate module"
          fi

      - name: Check for service key mismatches (pre-plan)
        id: check_service_keys
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.check_services_empty.outputs.services_empty != 'true' &&
          steps.vars.outputs.module_path == '04-ecs-fargate'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Checking for service key mismatches between state and config..."
          
          # Initialize Terraform to access state
          cd "$PLAN_DIR"
          terraform init -no-color -input=false > /dev/null 2>&1 || true
          
          # Get services from state
          STATE_SERVICES=$(terraform state list -no-color 2>/dev/null | grep "aws_ecs_service.services\[" | sed 's/aws_ecs_service.services\["\(.*\)"\]/\1/' || echo "")
          
          # Get services from config (parse services.generated.json)
          if [ -f "$PLAN_DIR/services.generated.json" ]; then
            CONFIG_SERVICES=$(jq -r '.services | keys[]' "$PLAN_DIR/services.generated.json" 2>/dev/null || echo "")
          else
            CONFIG_SERVICES=""
          fi
          
          if [ -n "$STATE_SERVICES" ] && [ -n "$CONFIG_SERVICES" ]; then
            # Find services in state but not in config (will be destroyed)
            SERVICES_TO_DESTROY=""
            SERVICES_TO_CREATE=""
            SERVICES_TO_UPDATE=""
            
            for state_svc in $STATE_SERVICES; do
              if ! echo "$CONFIG_SERVICES" | grep -q "^$state_svc$"; then
                SERVICES_TO_DESTROY="$SERVICES_TO_DESTROY $state_svc"
              else
                SERVICES_TO_UPDATE="$SERVICES_TO_UPDATE $state_svc"
              fi
            done
            
            for config_svc in $CONFIG_SERVICES; do
              if ! echo "$STATE_SERVICES" | grep -q "^$config_svc$"; then
                SERVICES_TO_CREATE="$SERVICES_TO_CREATE $config_svc"
              fi
            done
            
            if [ -n "$SERVICES_TO_DESTROY" ]; then
              echo "::warning::âš ï¸ Service key mismatch detected!"
              echo "::warning::The following services in state will be DESTROYED (not in new config):"
              for svc in $SERVICES_TO_DESTROY; do
                echo "::warning::  âŒ $svc"
              done
              echo ""
              echo "::warning::Possible reasons:"
              echo "::warning::  â€¢ Service key changed (e.g., 'api' â†’ 'legacy::api')"
              echo "::warning::  â€¢ Service removed from YAML definitions"
              echo "::warning::  â€¢ Service renamed"
              echo ""
              echo "::warning::To preserve services, consider:"
              echo "::warning::  1. State migration: terraform state mv 'aws_ecs_service.services[\"old-key\"]' 'aws_ecs_service.services[\"new-key\"]'"
              echo "::warning::  2. Or add the service back to YAML if it was accidentally removed"
            else
              echo "::notice::âœ“ All services in state match config keys"
            fi
            
            if [ -n "$SERVICES_TO_CREATE" ]; then
              echo "::notice::The following NEW services will be CREATED:"
              for svc in $SERVICES_TO_CREATE; do
                echo "::notice::  âž• $svc"
              done
            fi
            
            if [ -n "$SERVICES_TO_UPDATE" ]; then
              echo "::notice::The following services will be UPDATED (if config changed):"
              for svc in $SERVICES_TO_UPDATE; do
                echo "::notice::  ðŸ”„ $svc"
              done
            fi
          else
            echo "::notice::Cannot compare - state or config services list is empty"
          fi

      - name: Pre-Plan State Validation Summary
        id: pre_plan_summary
        if: |
          (steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply') &&
          steps.vars.outputs.module_path == '04-ecs-fargate' &&
          steps.check_services_empty.outputs.services_empty != 'true'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::ðŸ“Š State Validation Summary:"
          
          # Get expected service keys
          if [ ! -f "$PLAN_DIR/services.generated.json" ]; then
            echo "::notice::No services.generated.json found"
            exit 0
          fi
          
          EXPECTED_KEYS=$(jq -r '.services | keys[]' "$PLAN_DIR/services.generated.json" 2>/dev/null || echo "")
          
          if [ -z "$EXPECTED_KEYS" ]; then
            echo "::notice::No service keys found in config"
            exit 0
          fi
          
          # Check which services are in state
          STATE_SERVICES=$(terraform -chdir="$PLAN_DIR" state list 2>/dev/null | \
            grep 'module.ecs_fargate.aws_service_discovery_service.services\["' | \
            sed 's/.*\["\(.*\)"\]/\1/' || echo "")
          
          STATE_ECS=$(terraform -chdir="$PLAN_DIR" state list 2>/dev/null | \
            grep 'module.ecs_fargate.aws_ecs_service.services\["' | \
            sed 's/.*\["\(.*\)"\]/\1/' || echo "")
          
          MISSING_SD=""
          MISSING_ECS=""
          
          for tf_key in $EXPECTED_KEYS; do
            if ! echo "$STATE_SERVICES" | grep -q "^$tf_key$"; then
              MISSING_SD="$MISSING_SD $tf_key"
            fi
            if ! echo "$STATE_ECS" | grep -q "^$tf_key$"; then
              MISSING_ECS="$MISSING_ECS $tf_key"
            fi
          done
          
          if [ -n "$MISSING_SD" ] || [ -n "$MISSING_ECS" ]; then
            if [ -n "$MISSING_SD" ]; then
              echo "::warning::âš ï¸ Service Discovery services NOT in state:"
              for svc in $MISSING_SD; do
                echo "::warning::  - $svc"
              done
            fi
            if [ -n "$MISSING_ECS" ]; then
              echo "::notice::â„¹ï¸ ECS services NOT in state (will be created):"
              for svc in $MISSING_ECS; do
                echo "::notice::  - $svc"
              done
            fi
          else
            echo "::notice::âœ“ All expected resources are in state"
          fi

      - name: Terraform Plan
        id: plan
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        uses: ./.github/actions/terraform-plan
        with:
          terraform_path: DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}
          var_files: ${{ steps.build_var_files.outputs.var_files }}
          # Only pass global_image_tag for ECS Fargate module (dynamic based on module_path)
          # Use conditional expression: if module_path is 04-ecs-fargate, pass image tag, otherwise empty string
          extra_vars: ${{ steps.vars.outputs.module_path == '04-ecs-fargate' && format('global_image_tag={0}', steps.vars.outputs.image_tag) || '' }}
          plan_file: "tfplan"
        continue-on-error: true

      - name: Debug Plan - Show Replacement Reasons
        if: steps.vars.outputs.action == 'plan' || steps.vars.outputs.action == 'apply'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          if [ -f "$PLAN_DIR/tfplan" ]; then
            echo "::group::Debug: Resources Being Replaced"
            echo "Checking for resources that need replacement..."
            terraform -chdir="$PLAN_DIR" show -no-color tfplan | grep -A 10 "must be replaced" || echo "No replacements detected"
            echo "::endgroup::"
            
            echo "::group::Debug: Detailed Plan Output"
            terraform -chdir="$PLAN_DIR" show -no-color tfplan || true
            echo "::endgroup::"
          else
            echo "::notice::Plan file not available for debugging"
          fi

      - name: Add Plan to Summary
        if: steps.vars.outputs.action == 'plan' || (steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success')
        run: |
          echo "## Terraform Plan Output" >> $GITHUB_STEP_SUMMARY
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          if [ -f "$PLAN_DIR/tfplan" ]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            terraform -chdir="$PLAN_DIR" show -no-color tfplan >> $GITHUB_STEP_SUMMARY 2>&1
            echo '```' >> $GITHUB_STEP_SUMMARY
            
            # Add replacement analysis to summary
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ðŸ” Replacement Analysis" >> $GITHUB_STEP_SUMMARY
            REPLACEMENTS=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 | grep -c "must be replaced" || echo "0")
            if [ "$REPLACEMENTS" -gt "0" ]; then
              echo "âš ï¸ **Warning:** $REPLACEMENTS resource(s) will be replaced" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "Resources being replaced:" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 | grep -B 2 -A 5 "must be replaced" >> $GITHUB_STEP_SUMMARY || true
              echo '```' >> $GITHUB_STEP_SUMMARY
            else
              echo "âœ… No replacements detected" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "Plan file not available (tfplan not found)" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check if plan has changes
        id: plan_changes
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          PLAN_OUTPUT=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1)
          
          if echo "$PLAN_OUTPUT" | grep -q "No changes"; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "::notice::No changes detected. Skipping apply."
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "::notice::Changes detected. Proceeding with apply."
            
            # Extract service changes from plan for visibility
            if echo "$PLAN_OUTPUT" | grep -q "aws_ecs_service.services\["; then
              echo ""
              echo "::notice::ðŸ“‹ Service Changes Detected:"
              
              CREATES=$(echo "$PLAN_OUTPUT" | grep -c "aws_ecs_service.services\[.*\] will be created" 2>/dev/null || echo "0")
              DESTROYS=$(echo "$PLAN_OUTPUT" | grep -c "aws_ecs_service.services\[.*\] will be destroyed" 2>/dev/null || echo "0")
              REPLACES=$(echo "$PLAN_OUTPUT" | grep -c "aws_ecs_service.services\[.*\] must be replaced" 2>/dev/null || echo "0")
              
              # Ensure we have valid integers (strip any whitespace/newlines and default to 0)
              CREATES=$(echo "$CREATES" | tr -d '[:space:]' || echo "0")
              DESTROYS=$(echo "$DESTROYS" | tr -d '[:space:]' || echo "0")
              REPLACES=$(echo "$REPLACES" | tr -d '[:space:]' || echo "0")
              
              if [ "${CREATES:-0}" -gt 0 ] 2>/dev/null; then
                echo "::notice::  âž• Services to CREATE: $CREATES"
              fi
              
              if [ "${DESTROYS:-0}" -gt 0 ] 2>/dev/null; then
                echo "::warning::  âŒ Services to DESTROY: $DESTROYS"
                echo "::warning::âš ï¸ This may be due to service key changes (e.g., 'api' â†’ 'legacy::api')"
              fi
              
              if [ "${REPLACES:-0}" -gt 0 ] 2>/dev/null; then
                echo "::warning::  ðŸ”„ Services to REPLACE: $REPLACES"
              fi
            fi
          fi

      - name: Save State Snapshot Before Apply
        id: save_state_snapshot
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Creating state snapshot before apply for targeted cleanup..."
          
          # Get list of resources in state BEFORE apply
          STATE_BEFORE=$(terraform -chdir="$PLAN_DIR" state list -no-color 2>/dev/null || echo "")
          
          # Save to file for cleanup step
          echo "$STATE_BEFORE" > /tmp/state_before_apply.txt || true
          
          # Also get plan to see what will be created
          if [ -f "$PLAN_DIR/tfplan" ]; then
            PLAN_OUTPUT=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 || echo "")
            
            # Extract resources that will be created
            CREATED_RESOURCES=$(echo "$PLAN_OUTPUT" | grep "will be created" | \
              sed 's/^[[:space:]]*# //' | sed 's/ will be created$//' || echo "")
            
            echo "$CREATED_RESOURCES" > /tmp/resources_to_create.txt || true
          fi

      - name: Idempotency Check Before Apply
        id: idempotency_check
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::ðŸ” Performing idempotency checks before apply..."
          
          if [ ! -f "$PLAN_DIR/tfplan" ]; then
            echo "::warning::Plan file not found - skipping idempotency check"
            exit 0
          fi
          
          PLAN_OUTPUT=$(terraform -chdir="$PLAN_DIR" show -no-color tfplan 2>&1 || echo "")
          
          # Get resources that plan wants to create
          RESOURCES_TO_CREATE=$(echo "$PLAN_OUTPUT" | grep "will be created" | \
            sed 's/^[[:space:]]*# //' | sed 's/ will be created$//' || echo "")
          
          # Get resources that plan wants to replace (critical for Service Discovery)
          RESOURCES_TO_REPLACE=$(echo "$PLAN_OUTPUT" | grep "must be replaced" | \
            sed 's/^[[:space:]]*# //' | sed 's/ must be replaced$//' || echo "")
          
          CONFLICTS=0
          
          # Check resources that will be created
          if [ -n "$RESOURCES_TO_CREATE" ]; then
            echo "::notice::Checking resources to be created..."
            while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              # Check if resource already exists in state
              if terraform -chdir="$PLAN_DIR" state show "$resource" >/dev/null 2>&1; then
                echo "::error::âŒ Resource '$resource' is in state but plan wants to create it"
                echo "::error::This indicates state drift. Resource should be updated, not created."
                CONFLICTS=$((CONFLICTS + 1))
              fi
            done <<< "$RESOURCES_TO_CREATE"
          fi
          
          # Check resources that will be replaced (especially Service Discovery)
          if [ -n "$RESOURCES_TO_REPLACE" ]; then
            echo "::notice::Checking resources to be replaced..."
            while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              # For Service Discovery services, check if replacement already exists in AWS
              if echo "$resource" | grep -q "aws_service_discovery_service"; then
                # Extract service key from resource path
                SERVICE_KEY=$(echo "$resource" | sed 's/.*\["\(.*\)"\]/\1/')
                
                # Get namespace ID from state
                NAMESPACE_ID=$(terraform -chdir="$PLAN_DIR" state show 'module.ecs_fargate.aws_service_discovery_private_dns_namespace.this' 2>/dev/null | \
                  grep -E '^\s+id\s+=' | awk '{print $3}' | tr -d '"' || echo "")
                
                if [ -n "$NAMESPACE_ID" ]; then
                  # Get expected sanitized service name (part after ::)
                  EXPECTED_NAME=$(echo "$SERVICE_KEY" | sed 's/.*::\(.*\)/\1/' | tr '[:upper:]' '[:lower:]')
                  
                  # Check if service with this name already exists in AWS
                  EXISTING_SERVICE=$(aws servicediscovery list-services \
                    --filters Name=NAMESPACE_ID,Values="$NAMESPACE_ID" \
                    --query "Services[?Name=='$EXPECTED_NAME'].[Name,Id]" \
                    --output text 2>/dev/null || echo "")
                  
                  if [ -n "$EXISTING_SERVICE" ]; then
                    SERVICE_ID=$(echo "$EXISTING_SERVICE" | awk '{print $2}')
                    echo "::error::âŒ Service Discovery service '$SERVICE_KEY' will be replaced"
                    echo "::error::   Replacement service '$EXPECTED_NAME' already exists in AWS (ID: $SERVICE_ID)"
                    echo "::error::   This will cause 'ServiceAlreadyExists' error during apply"
                    echo "::error::   The validation script should have imported this, but it may have failed"
                    echo "::error::   Consider running: terraform import '$resource' '$SERVICE_ID'"
                    CONFLICTS=$((CONFLICTS + 1))
                  fi
                fi
              fi
            done <<< "$RESOURCES_TO_REPLACE"
          fi
          
          if [ $CONFLICTS -gt 0 ]; then
            echo "::error::âŒ Idempotency check failed: $CONFLICTS conflict(s) detected"
            echo "::error::Resources that will be created/replaced already exist in AWS"
            echo "::error::State validation should have caught this. Review state and plan."
            echo "::error::"
            echo "::error::To fix:"
            echo "::error::  1. Run the 'Comprehensive State Validation' step again"
            echo "::error::  2. Or manually import the resources: terraform import 'resource.address' 'aws-id'"
            exit 1
          else
            if [ -z "$RESOURCES_TO_CREATE" ] && [ -z "$RESOURCES_TO_REPLACE" ]; then
              echo "::notice::âœ“ No resources to create or replace - idempotency check passed"
            else
              echo "::notice::âœ“ Idempotency check passed - no conflicts detected"
            fi
          fi

      - name: Comprehensive Pre-Apply Validation
        id: comprehensive_validation
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          if [ -f "scripts/comprehensive-pre-apply-validation.sh" ]; then
            chmod +x scripts/comprehensive-pre-apply-validation.sh
            scripts/comprehensive-pre-apply-validation.sh "$PLAN_DIR" "${{ steps.vars.outputs.environment }}" "${{ steps.vars.outputs.module_path }}" || {
              EXIT_CODE=$?
              if [ $EXIT_CODE -eq 1 ]; then
                echo "::error::Comprehensive validation found critical errors"
                echo "::error::Please review and fix the errors before proceeding"
                echo "validation_failed=true" >> $GITHUB_OUTPUT
              else
                echo "validation_failed=false" >> $GITHUB_OUTPUT
              fi
              exit 0  # Don't fail workflow, but set output
            }
            echo "validation_failed=false" >> $GITHUB_OUTPUT
          else
            echo "::warning::Comprehensive validation script not found, skipping"
            echo "validation_failed=false" >> $GITHUB_OUTPUT
          fi

      - name: Check Terraform State Lock (if DynamoDB locking enabled)
        id: check_state_lock
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'true'
        continue-on-error: true
        timeout-minutes: 1
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::ðŸ”’ Checking for Terraform state locks..."
          
          # Check if DynamoDB locking is configured in backend
          if grep -q "dynamodb_table" "$PLAN_DIR/backend.tf" 2>/dev/null || grep -q "dynamodb_table" "$PLAN_DIR/backend.hcl" 2>/dev/null; then
            echo "::notice::DynamoDB state locking is configured"
            
            # Try to list locks (terraform will error if locked)
            # Note: There's no direct "check lock" command, but we can try a safe operation
            # that will fail if locked: terraform state list (read-only, but still requires lock)
            if terraform -chdir="$PLAN_DIR" state list -no-color >/dev/null 2>&1; then
              echo "::notice::âœ“ No state locks detected (state is accessible)"
              echo "state_locked=false" >> $GITHUB_OUTPUT
            else
              LOCK_ERROR=$(terraform -chdir="$PLAN_DIR" state list -no-color 2>&1 || true)
              if echo "$LOCK_ERROR" | grep -qi "lock\|locked"; then
                echo "::warning::âš  State is locked by another operation"
                echo "::warning::Lock error: $LOCK_ERROR"
                echo "::warning::Waiting for lock to be released or checking for stale locks..."
                echo "state_locked=true" >> $GITHUB_OUTPUT
              else
                echo "::notice::State check completed (may be fresh deployment with no state)"
                echo "state_locked=false" >> $GITHUB_OUTPUT
              fi
            fi
          else
            echo "::notice::DynamoDB state locking is NOT configured (S3 backend only)"
            echo "::notice::State locking relies on S3 object locking, which is less reliable"
            echo "::notice::Consider enabling DynamoDB locking for better concurrency control"
            echo "::notice::See: DEVOPS/live/dev/01-vpc/README-BACKEND.md"
            echo "state_locked=false" >> $GITHUB_OUTPUT
          fi

      - name: Always Re-plan Before Apply (Prevent Stale Plans)
        id: replan_before_apply
        if: |
          steps.vars.outputs.action == 'apply' && 
          steps.plan.outcome == 'success' && 
          steps.plan_changes.outputs.has_changes == 'true'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::ðŸ”„ Creating fresh plan right before apply (prevents stale plan errors)..."
          echo "::notice::This ensures the plan matches the current state, even if state changed since initial plan"
          
          # Remove old plan file
          rm -f "$PLAN_DIR/tfplan"
          
          # Build plan arguments (same as original plan)
          VAR_FILES="${{ steps.build_var_files.outputs.var_files }}"
          PLAN_ARGS="-out=tfplan -no-color"
          
          if [ -n "$VAR_FILES" ]; then
            for var_file in $VAR_FILES; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                PLAN_ARGS="$PLAN_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          # Add global_image_tag if applicable
          if [ "${{ steps.vars.outputs.module_path }}" == "04-ecs-fargate" ] && [ -n "${{ steps.vars.outputs.image_tag }}" ]; then
            PLAN_ARGS="$PLAN_ARGS -var=global_image_tag=${{ steps.vars.outputs.image_tag }}"
          fi
          
          # Create fresh plan
          terraform -chdir="$PLAN_DIR" plan $PLAN_ARGS || {
            echo "::error::Fresh plan creation failed"
            echo "::error::This may indicate state conflicts or configuration issues"
            exit 1
          }
          
          echo "::notice::âœ“ Fresh plan created successfully"
          echo "replanned=true" >> $GITHUB_OUTPUT
          
          # Show plan summary
          echo "::group::Fresh Plan Summary"
          terraform -chdir="$PLAN_DIR" show -no-color tfplan | head -50 || true
          echo "::endgroup::"

      - name: Terraform Apply
        id: apply
        if: |
          steps.vars.outputs.action == 'apply' && 
          steps.plan.outcome == 'success' && 
          steps.plan_changes.outputs.has_changes == 'true' &&
          steps.replan_before_apply.outcome == 'success' &&
          steps.replan_before_apply.outputs.replanned == 'true' &&
          steps.comprehensive_validation.outputs.validation_failed != 'true' &&
          steps.idempotency_check.outcome == 'success'
        uses: ./.github/actions/terraform-apply
        with:
          terraform_path: DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}
          plan_file: "tfplan"
          auto_approve: "true"

      - name: Generate and Set Secret Values
        id: generate_secrets
        if: |
          steps.vars.outputs.action == 'apply' && 
          steps.apply.outcome == 'success' && 
          steps.vars.outputs.module_path == '05-database'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::ðŸ” Generating secure secrets for database module..."
          
          if [ -f "DEVOPS/scripts/generate-secrets.sh" ]; then
            chmod +x DEVOPS/scripts/generate-secrets.sh
            cd DEVOPS
            if scripts/generate-secrets.sh "${{ steps.vars.outputs.environment }}" "live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"; then
              echo "::notice::âœ“ Secret generation completed successfully"
              echo "secrets_generated=true" >> $GITHUB_OUTPUT
            else
              echo "::warning::âš  Secret generation script failed (non-blocking)"
              echo "::warning::Secrets may need to be set manually"
              echo "secrets_generated=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "::warning::Secret generation script not found at DEVOPS/scripts/generate-secrets.sh"
            echo "::warning::Skipping secret generation - secrets may need to be set manually"
            echo "secrets_generated=false" >> $GITHUB_OUTPUT
          fi

      - name: Verify DynamoDB Table Connectivity (Post-Apply)
        id: verify_dynamodb
        if: |
          steps.vars.outputs.action == 'apply' && 
          steps.apply.outcome == 'success' && 
          steps.vars.outputs.module_path == '05-database'
        continue-on-error: true
        env:
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
          ENVIRONMENT: ${{ steps.vars.outputs.environment }}
        run: |
          echo "::notice::ðŸ” Verifying DynamoDB table connectivity after deployment..."
          
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/05-database"
          TABLE_NAME=""
          
          # Method 1: Get table name(s) from Terraform outputs (most reliable)
          echo "::group::Discovering table name from Terraform outputs"
          cd "$PLAN_DIR"
          
          # Try to get table_names output (map of table names)
          TABLE_NAMES_JSON=$(terraform output -json table_names 2>/dev/null || echo "{}")
          
          if [ "$TABLE_NAMES_JSON" != "{}" ] && [ -n "$TABLE_NAMES_JSON" ]; then
            # Count how many tables we have
            TABLE_COUNT=$(echo "$TABLE_NAMES_JSON" | jq 'length' 2>/dev/null || echo "0")
            echo "Found $TABLE_COUNT table(s) in Terraform output"
            
            # Extract table names (handle both single and multiple tables)
            if [ "$TABLE_COUNT" -eq 1 ]; then
              # Single table - get the value directly
              TABLE_NAME=$(echo "$TABLE_NAMES_JSON" | jq -r 'to_entries[0].value // empty' 2>/dev/null || echo "")
            elif [ "$TABLE_COUNT" -gt 1 ]; then
              # Multiple tables - prefer "greetings" table, otherwise first one
              TABLE_NAME=$(echo "$TABLE_NAMES_JSON" | jq -r '.greetings // to_entries[0].value // empty' 2>/dev/null || echo "")
              echo "Multiple tables found, using: $TABLE_NAME"
            fi
            
            if [ -n "$TABLE_NAME" ] && [ "$TABLE_NAME" != "null" ]; then
              echo "âœ“ Found table name from Terraform output: $TABLE_NAME"
              
              # Show all tables for reference
              echo "All tables in Terraform output:"
              echo "$TABLE_NAMES_JSON" | jq -r 'to_entries[] | "  - \(.key): \(.value)"' 2>/dev/null || echo "  (unable to parse)"
            fi
          else
            echo "âš  Terraform output 'table_names' not available or empty"
          fi
          
          cd - > /dev/null
          echo "::endgroup::"
          
          # Method 2: Try SSM Parameter Store (if Terraform output failed)
          if [ -z "$TABLE_NAME" ] || [ "$TABLE_NAME" = "null" ]; then
            echo "::group::Discovering table name from SSM Parameter Store"
            SSM_PARAM="/${{ steps.vars.outputs.environment }}/dynamodb/greetings/table_name"
            TABLE_NAME=$(aws ssm get-parameter \
              --name "$SSM_PARAM" \
              --region "$AWS_REGION" \
              --query 'Parameter.Value' \
              --output text 2>/dev/null || echo "")
            
            if [ -n "$TABLE_NAME" ]; then
              echo "âœ“ Found table name from SSM: $TABLE_NAME"
            else
              echo "âš  SSM parameter not found: $SSM_PARAM"
            fi
            echo "::endgroup::"
          fi
          
          # Method 3: List all DynamoDB tables and find matching one
          if [ -z "$TABLE_NAME" ] || [ "$TABLE_NAME" = "null" ]; then
            echo "::group::Discovering table name from AWS DynamoDB"
            echo "Listing all DynamoDB tables in region $AWS_REGION..."
            
            # List all tables and find one matching the environment pattern
            EXPECTED_PATTERN="${{ steps.vars.outputs.environment }}-greetings"
            ALL_TABLES=$(aws dynamodb list-tables --region "$AWS_REGION" --query 'TableNames[]' --output text 2>/dev/null || echo "")
            
            if [ -n "$ALL_TABLES" ]; then
              # Check if expected table exists
              if echo "$ALL_TABLES" | grep -q "^$EXPECTED_PATTERN$"; then
                TABLE_NAME="$EXPECTED_PATTERN"
                echo "âœ“ Found table matching pattern: $TABLE_NAME"
              else
                # Get first table that contains the environment name
                TABLE_NAME=$(echo "$ALL_TABLES" | tr '\t' '\n' | grep -E "^${{ steps.vars.outputs.environment }}-" | head -1 || echo "")
                if [ -n "$TABLE_NAME" ]; then
                  echo "âœ“ Found table with environment prefix: $TABLE_NAME"
                else
                  # Last resort: get first table
                  TABLE_NAME=$(echo "$ALL_TABLES" | tr '\t' '\n' | head -1 || echo "")
                  if [ -n "$TABLE_NAME" ]; then
                    echo "âš  Using first available table: $TABLE_NAME (may not be correct)"
                  fi
                fi
              fi
            else
              echo "âš  No tables found in DynamoDB"
            fi
            echo "::endgroup::"
          fi
          
          # Final fallback: Use naming convention
          if [ -z "$TABLE_NAME" ] || [ "$TABLE_NAME" = "null" ]; then
            TABLE_NAME="${{ steps.vars.outputs.environment }}-greetings"
            echo "::warning::Using default table name (naming convention): $TABLE_NAME"
          fi
          
          echo "::notice::Using table name: $TABLE_NAME"
          
          # Verify table exists and is accessible
          echo "::group::DynamoDB Table Verification"
          echo "Verifying table: $TABLE_NAME in region: $AWS_REGION"
          
          if aws dynamodb describe-table \
            --table-name "$TABLE_NAME" \
            --region "$AWS_REGION" \
            > /dev/null 2>&1; then
            echo "âœ“ Table exists and is accessible"
            
            # Get comprehensive table details
            TABLE_DETAILS=$(aws dynamodb describe-table \
              --table-name "$TABLE_NAME" \
              --region "$AWS_REGION" 2>/dev/null || echo "{}")
            
            TABLE_STATUS=$(echo "$TABLE_DETAILS" | jq -r '.Table.TableStatus // "UNKNOWN"' 2>/dev/null || echo "UNKNOWN")
            TABLE_ARN=$(echo "$TABLE_DETAILS" | jq -r '.Table.TableArn // ""' 2>/dev/null || echo "")
            ITEM_COUNT=$(echo "$TABLE_DETAILS" | jq -r '.Table.ItemCount // 0' 2>/dev/null || echo "0")
            TABLE_SIZE=$(echo "$TABLE_DETAILS" | jq -r '.Table.TableSizeBytes // 0' 2>/dev/null || echo "0")
            BILLING_MODE=$(echo "$TABLE_DETAILS" | jq -r '.Table.BillingModeSummary.BillingMode // "UNKNOWN"' 2>/dev/null || echo "UNKNOWN")
            GSI_COUNT=$(echo "$TABLE_DETAILS" | jq -r '.Table.GlobalSecondaryIndexes | length // 0' 2>/dev/null || echo "0")
            
            echo ""
            echo "ðŸ“Š Table Details:"
            echo "  Name: $TABLE_NAME"
            echo "  Status: $TABLE_STATUS"
            echo "  ARN: $TABLE_ARN"
            echo "  Item Count: $ITEM_COUNT"
            echo "  Size: $TABLE_SIZE bytes"
            echo "  Billing Mode: $BILLING_MODE"
            echo "  Global Secondary Indexes: $GSI_COUNT"
            
            if [ "$TABLE_STATUS" = "ACTIVE" ]; then
              echo ""
              echo "âœ“ Table is ACTIVE and ready for operations"
              
              # Test basic write operation (optional - creates a test item)
              TEST_ITEM_ID="test-verify-$(date +%s)-$$"
              TEST_CREATED_AT=$(date -u +%Y-%m-%dT%H:%M:%SZ)
              echo "Testing write operation with test item: $TEST_ITEM_ID"
              
              if aws dynamodb put-item \
                --table-name "$TABLE_NAME" \
                --item "{\"id\":{\"S\":\"$TEST_ITEM_ID\"},\"created_at\":{\"S\":\"$TEST_CREATED_AT\"},\"user_name\":{\"S\":\"test-user\"},\"message\":{\"S\":\"Integration test verification\"}}" \
                --region "$AWS_REGION" \
                > /dev/null 2>&1; then
                echo "âœ“ Write operation successful"
                
                # Test read operation
                if aws dynamodb get-item \
                  --table-name "$TABLE_NAME" \
                  --key "{\"id\":{\"S\":\"$TEST_ITEM_ID\"},\"created_at\":{\"S\":\"$TEST_CREATED_AT\"}}" \
                  --region "$AWS_REGION" \
                  --query 'Item.message.S' \
                  --output text \
                  > /dev/null 2>&1; then
                  echo "âœ“ Read operation successful"
                  
                  # Clean up test item
                  aws dynamodb delete-item \
                    --table-name "$TABLE_NAME" \
                    --key "{\"id\":{\"S\":\"$TEST_ITEM_ID\"},\"created_at\":{\"S\":\"$TEST_CREATED_AT\"}}" \
                    --region "$AWS_REGION" \
                    > /dev/null 2>&1 || true
                  echo "âœ“ Test item cleaned up"
                else
                  echo "âš  Read operation failed (non-critical - may be due to IAM permissions)"
                fi
              else
                echo "âš  Write operation failed (may be due to IAM permissions - non-critical)"
              fi
              
              echo "::notice::âœ“ DynamoDB table verification completed successfully"
              echo "verification_passed=true" >> $GITHUB_OUTPUT
            else
              echo "::warning::âš  Table exists but is not ACTIVE (Status: $TABLE_STATUS)"
              echo "verification_passed=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "::error::âœ— Table '$TABLE_NAME' not found or not accessible"
            echo "::error::Please verify the table was created successfully"
            echo "verification_passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          echo "::endgroup::"

      - name: Verify Target Group Health Check Paths (Post-Apply)
        id: verify_health_checks
        if: steps.vars.outputs.action == 'apply' && steps.apply.outcome == 'success' && steps.vars.outputs.module_path == '04-ecs-fargate'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Verifying target group health check paths after apply..."
          
          if [ -f "scripts/verify-target-group-health-checks.sh" ]; then
            chmod +x scripts/verify-target-group-health-checks.sh
            export TERRAFORM_DIR="$PLAN_DIR"
            export ENVIRONMENT="${{ steps.vars.outputs.environment }}"
            
            if scripts/verify-target-group-health-checks.sh; then
              echo "::notice::âœ“ All target group health check paths verified successfully"
            else
              echo "::warning::âš  State drift detected after apply. Target groups may not match configuration."
            fi
          else
            echo "::notice::Verification script not found, skipping..."
          fi

      - name: Verify Service Health and Connectivity
        id: verify_service_health
        if: steps.vars.outputs.action == 'apply' && steps.apply.outcome == 'success' && steps.vars.outputs.module_path == '04-ecs-fargate'
        continue-on-error: true
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          echo "::notice::Verifying service health and connectivity after deployment..."
          
          if [ -f "DEVOPS/scripts/verify-service-health.sh" ]; then
            chmod +x DEVOPS/scripts/verify-service-health.sh
            cd DEVOPS
            if DEVOPS/scripts/verify-service-health.sh "${{ steps.vars.outputs.environment }}" "${{ steps.vars.outputs.module_path }}" "${{ secrets.AWS_REGION }}"; then
              echo "::notice::âœ“ All services are healthy and accessible"
            else
              echo "::warning::âš  Some services may be unhealthy or inaccessible. Check the output above for details."
              echo "::notice::This is often normal immediately after deployment - DNS propagation and container startup can take a few minutes."
            fi
          else
            echo "::notice::Service health verification script not found, skipping..."
          fi

      - name: Targeted Cleanup on Apply Failure
        if: steps.vars.outputs.action == 'apply' && steps.apply.outcome == 'failure' && steps.vars.outputs.environment != 'production'
        continue-on-error: true
        run: |
          echo "::error::Terraform apply failed. Attempting targeted cleanup of resources created in this run..."
          
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          
          # Get state before apply
          STATE_BEFORE=""
          if [ -f /tmp/state_before_apply.txt ]; then
            STATE_BEFORE=$(cat /tmp/state_before_apply.txt)
          fi
          
          # Get current state
          STATE_AFTER=$(terraform -chdir="$PLAN_DIR" state list -no-color 2>/dev/null || echo "")
          
          # Find resources that were created in this run (in state_after but not in state_before)
          NEWLY_CREATED=""
          if [ -n "$STATE_AFTER" ]; then
            while IFS= read -r resource; do
              if [ -z "$STATE_BEFORE" ] || ! echo "$STATE_BEFORE" | grep -q "^${resource}$"; then
                NEWLY_CREATED="${NEWLY_CREATED}${resource}"$'\n'
              fi
            done <<< "$STATE_AFTER"
          fi
          
          # Also check plan file for resources that were supposed to be created
          PLANNED_CREATES=""
          if [ -f /tmp/resources_to_create.txt ]; then
            PLANNED_CREATES=$(cat /tmp/resources_to_create.txt)
          fi
          
          if [ -z "$NEWLY_CREATED" ] && [ -z "$PLANNED_CREATES" ]; then
            echo "::notice::No new resources were created in this run. No cleanup needed."
            exit 0
          fi
          
          echo "::notice::Resources created in this failed run:"
          if [ -n "$NEWLY_CREATED" ]; then
            echo "$NEWLY_CREATED" | while read -r resource; do
              [ -n "$resource" ] && echo "::notice::  - $resource"
            done
          fi
          
          # Build destroy command with var files
          DESTROY_ARGS="-auto-approve"
          VAR_FILES="${{ steps.build_var_files.outputs.var_files }}"
          if [ -n "$VAR_FILES" ]; then
            for var_file in $VAR_FILES; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                DESTROY_ARGS="$DESTROY_ARGS -var-file=$var_file"
              fi
            done
          fi
          
          # For targeted cleanup, we'll use terraform destroy but with a filter
          # Since Terraform doesn't support selective destroy easily, we'll:
          # 1. Try to remove newly created resources from state (if they're truly orphaned)
          # 2. Or destroy only if it's safe (non-production, and resources are clearly orphaned)
          
          echo "::notice::Attempting targeted cleanup..."
          
          # Option 1: Try to remove orphaned resources from state (if they don't exist in AWS)
          REMOVED_FROM_STATE=0
          if [ -n "$NEWLY_CREATED" ]; then
            echo "$NEWLY_CREATED" | while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              # Check if resource actually exists in AWS by trying to refresh it
              REFRESH_OUTPUT=$(terraform -chdir="$PLAN_DIR" state show "$resource" 2>&1 || echo "")
              if echo "$REFRESH_OUTPUT" | grep -q "doesn't exist" || echo "$REFRESH_OUTPUT" | grep -q "not found"; then
                echo "::notice::Removing orphaned resource from state: $resource"
                terraform -chdir="$PLAN_DIR" state rm "$resource" 2>&1 || true
                REMOVED_FROM_STATE=$((REMOVED_FROM_STATE + 1))
              fi
            done
          fi
          
          # Option 2: If resources exist in AWS, we need to destroy them properly
          # But only if they're truly orphaned (not part of working infrastructure)
          if [ -n "$NEWLY_CREATED" ] && [ "$REMOVED_FROM_STATE" -eq 0 ]; then
            echo "::warning::Resources exist in AWS. Checking if they're safe to destroy..."
            
            # For ECS Fargate, be very careful - only destroy if they're clearly failed
            # Check if any ECS services are in failed state
            FAILED_SERVICES=""
            while IFS= read -r resource; do
              [ -z "$resource" ] && continue
              
              if echo "$resource" | grep -q "aws_ecs_service"; then
                # Extract service name and check status
                SERVICE_NAME=$(echo "$resource" | sed 's/.*\["\(.*\)"\]/\1/')
                # Service might be in failed state - check AWS
                # For now, we'll be conservative and not auto-destroy ECS services
                echo "::warning::ECS service detected: $SERVICE_NAME - manual cleanup recommended"
              fi
            done <<< "$NEWLY_CREATED"
            
            # For non-critical resources (like Service Discovery), we can be more aggressive
            echo "::notice::For safety, manual cleanup is recommended for resources created in this run"
            echo "::notice::Review the resources listed above and destroy manually if needed"
          fi
          
          # Cleanup temp files
          rm -f /tmp/state_before_apply.txt /tmp/resources_to_create.txt
          
          echo "::notice::Targeted cleanup analysis completed"

      - name: Production apply failure notification
        if: steps.vars.outputs.action == 'apply' && steps.apply.outcome == 'failure' && steps.vars.outputs.environment == 'production'
        run: |
          echo "::error::Terraform apply failed in PRODUCTION environment."
          echo "::error::Automatic cleanup is disabled for production. Manual intervention required."
          echo "::error::Please review the Terraform state and decide on appropriate action:"
          echo "::error::  1. Review what resources were created before the failure"
          echo "::error::  2. Manually destroy partially created resources if needed"
          echo "::error::  3. Fix the issue and retry the deployment"
          exit 1

      - name: No Changes Detected
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'success' && steps.plan_changes.outputs.has_changes == 'false'
        run: |
          echo "::notice::No changes to apply. Infrastructure is already up to date."
          echo "## âœ… No Changes Required" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The Terraform plan showed no changes. Your infrastructure matches the configuration." >> $GITHUB_STEP_SUMMARY

      - name: Plan Failed
        if: steps.vars.outputs.action == 'apply' && steps.plan.outcome == 'failure'
        run: |
          echo "::error::Terraform plan failed. Apply cannot proceed."
          exit 1

      - name: Terraform Destroy (Production Protection)
        if: steps.vars.outputs.action == 'destroy' && steps.vars.outputs.environment == 'production'
        run: |
          echo "::error::Destroy action is blocked for production environment for safety."
          exit 1

      - name: Build var files list for destroy
        id: build_var_files_destroy
        if: steps.vars.outputs.action == 'destroy' && steps.vars.outputs.environment != 'production'
        run: |
          VAR_FILES=""
          if [ "${{ steps.check_tfvars.outputs.tfvars_exists }}" == "true" ]; then
            VAR_FILES="terraform.tfvars"
          fi
          # For destroy, include services.generated.json even if empty (needed for proper cleanup)
          if [ "${{ steps.check_services_json.outputs.services_json_exists }}" == "true" ]; then
            if [ -n "$VAR_FILES" ]; then
              VAR_FILES="$VAR_FILES services.generated.json"
            else
              VAR_FILES="services.generated.json"
            fi
          fi
          echo "var_files=$VAR_FILES" >> $GITHUB_OUTPUT

      - name: Terraform Destroy
        if: steps.vars.outputs.action == 'destroy' && steps.vars.outputs.environment != 'production'
        run: |
          PLAN_DIR="DEVOPS/live/${{ steps.vars.outputs.environment }}/${{ steps.vars.outputs.module_path }}"
          DESTROY_ARGS="-auto-approve"
          if [ -n "${{ steps.build_var_files_destroy.outputs.var_files }}" ]; then
            for var_file in ${{ steps.build_var_files_destroy.outputs.var_files }}; do
              if [ -f "$PLAN_DIR/$var_file" ]; then
                DESTROY_ARGS="$DESTROY_ARGS -var-file=$var_file"
              fi
            done
          fi
          # Ensure Terraform is initialized before destroy (idempotency)
          terraform -chdir="$PLAN_DIR" init -upgrade -no-color || {
            echo "::error::Terraform init failed before destroy"
            exit 1
          }
          
          # Verify state exists before destroying (resilience)
          if ! terraform -chdir="$PLAN_DIR" state list -no-color >/dev/null 2>&1; then
            echo "::notice::No Terraform state found - nothing to destroy"
            exit 0
          fi
          
          # Execute destroy with proper error handling
          # Use -refresh=false to work directly with state (avoids variable requirements)
          # Add -no-color for cleaner logs and -input=false to prevent prompts
          echo "::notice::Executing terraform destroy with -refresh=false (state-based)..."
          
          # Run destroy and capture output to check for lock errors
          # Use a temp file to capture output and get proper exit code
          TEMP_DESTROY_LOG=$(mktemp)
          terraform -chdir="$PLAN_DIR" destroy -refresh=false -input=false -no-color $DESTROY_ARGS > "$TEMP_DESTROY_LOG" 2>&1
          DESTROY_EXIT=$?
          
          # Always read output from temp file (more reliable than variable capture)
          DESTROY_OUTPUT=$(cat "$TEMP_DESTROY_LOG" 2>/dev/null || echo "") || true
          
          # Always show output
          echo "$DESTROY_OUTPUT" || true
          
          # Debug: Verify temp file exists and has content
          echo "::notice::Debug: DESTROY_EXIT=$DESTROY_EXIT" || true
          if [ -f "$TEMP_DESTROY_LOG" ]; then
            TEMP_FILE_SIZE=$(wc -c < "$TEMP_DESTROY_LOG" 2>/dev/null || echo "0") || true
            echo "::notice::Temp file exists, size: $TEMP_FILE_SIZE bytes" || true
          else
            echo "::warning::Temp file does not exist!" || true
          fi
          
          # Use explicit comparison to avoid issues with bash -e
          if [ "$DESTROY_EXIT" = "0" ]; then
            echo "::notice::âœ“ Destroy completed successfully"
            rm -f "$TEMP_DESTROY_LOG"
          else
            # Debug: Show what we're checking
            echo "::notice::=== DESTROY FAILED ===" || true
            echo "::notice::Destroy failed with exit code: $DESTROY_EXIT" || true
            echo "::notice::Checking for state lock error..." || true
            
            # Check temp file directly for lock errors (most reliable method)
            HAS_LOCK_ERROR=false
            if [ -f "$TEMP_DESTROY_LOG" ]; then
              echo "::notice::Checking temp file for lock errors..."
              if grep -qi "Error acquiring the state lock" "$TEMP_DESTROY_LOG" 2>/dev/null; then
                echo "::notice::Found 'Error acquiring the state lock' in temp file"
                HAS_LOCK_ERROR=true
              elif grep -qi "Lock Info:" "$TEMP_DESTROY_LOG" 2>/dev/null; then
                echo "::notice::Found 'Lock Info:' in temp file"
                HAS_LOCK_ERROR=true
              else
                echo "::notice::No lock error patterns found in temp file"
              fi
            else
              echo "::warning::Temp file not found for lock check"
            fi
            
            # Also check output variable as fallback
            if [ "$HAS_LOCK_ERROR" = "false" ] && [ -n "$DESTROY_OUTPUT" ]; then
              if echo "$DESTROY_OUTPUT" | grep -qi "Error acquiring the state lock"; then
                HAS_LOCK_ERROR=true
              elif echo "$DESTROY_OUTPUT" | grep -qi "Lock Info:"; then
                HAS_LOCK_ERROR=true
              fi
            fi
            
            if [ "$HAS_LOCK_ERROR" = "true" ]; then
              echo "::warning::State lock detected. Attempting to force unlock stale lock..."
              
              # Extract lock ID from error message - check temp file first, then output variable
              LOCK_ID=""
              
              # Try extraction from temp file (most reliable)
              if [ -f "$TEMP_DESTROY_LOG" ]; then
                echo "::notice::Extracting lock ID from temp file..."
                # Pattern 1: Look for "ID:" followed by the UUID
                LOCK_ID=$(grep -A 10 "Lock Info:" "$TEMP_DESTROY_LOG" 2>/dev/null | grep -i "ID:" | awk '{print $2}' | head -1 | tr -d '[:space:]' || echo "")
                echo "::notice::Pattern 1 result: ${LOCK_ID:-empty}"
                
                # Pattern 2: Alternative extraction using sed
                if [ -z "$LOCK_ID" ] || [ "$LOCK_ID" = "" ]; then
                  LOCK_ID=$(grep -A 10 "Lock Info:" "$TEMP_DESTROY_LOG" 2>/dev/null | grep -i "ID:" | sed -n 's/.*ID:[[:space:]]*\([a-f0-9-]\{36\}\).*/\1/p' | head -1 || echo "")
                  echo "::notice::Pattern 2 result: ${LOCK_ID:-empty}"
                fi
                
                # Pattern 3: Direct grep for UUID pattern
                if [ -z "$LOCK_ID" ] || [ "$LOCK_ID" = "" ]; then
                  LOCK_ID=$(grep -oE '[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}' "$TEMP_DESTROY_LOG" 2>/dev/null | head -1 || echo "")
                  echo "::notice::Pattern 3 result: ${LOCK_ID:-empty}"
                fi
              else
                echo "::warning::Temp file not found for lock ID extraction"
              fi
              
              # Fallback to output variable if temp file extraction failed
              if [ -z "$LOCK_ID" ] || [ "$LOCK_ID" = "" ]; then
                if [ -n "$DESTROY_OUTPUT" ]; then
                  LOCK_ID=$(echo "$DESTROY_OUTPUT" | grep -A 10 "Lock Info:" | grep -i "ID:" | awk '{print $2}' | head -1 | tr -d '[:space:]' || echo "")
                  if [ -z "$LOCK_ID" ] || [ "$LOCK_ID" = "" ]; then
                    LOCK_ID=$(echo "$DESTROY_OUTPUT" | grep -oE '[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}' | head -1 || echo "")
                  fi
                fi
              fi
              
              # Debug: Show what we found
              if [ -n "$LOCK_ID" ] && [ "$LOCK_ID" != "" ]; then
                echo "::notice::Extracted lock ID: $LOCK_ID"
              else
                echo "::warning::Could not extract lock ID automatically"
                echo "::notice::Lock Info section from temp file:"
                if [ -f "$TEMP_DESTROY_LOG" ]; then
                  grep -A 10 "Lock Info:" "$TEMP_DESTROY_LOG" 2>/dev/null || echo "No Lock Info section found in temp file"
                else
                  echo "Temp file not found"
                fi
              fi
              
              if [ -n "$LOCK_ID" ] && [ "$LOCK_ID" != "" ]; then
                echo "::notice::Force unlocking state lock: $LOCK_ID"
                echo "::warning::This lock may be from a previous failed run"
                
                # Force unlock the stale lock
                terraform -chdir="$PLAN_DIR" force-unlock -force "$LOCK_ID" 2>&1 || {
                  echo "::warning::Failed to force unlock. Lock may be from another active operation."
                  echo "::error::Please manually unlock: terraform -chdir=\"$PLAN_DIR\" force-unlock $LOCK_ID"
                  rm -f "$TEMP_DESTROY_LOG"
                  exit 1
                }
                
                echo "::notice::âœ“ Lock released. Retrying destroy..."
                
                # Retry destroy after unlocking
                terraform -chdir="$PLAN_DIR" destroy -refresh=false -input=false -no-color $DESTROY_ARGS || {
                  echo "::error::Terraform destroy failed after force unlock"
                  echo "::error::Some resources may have been destroyed. Review state and AWS console."
                  rm -f "$TEMP_DESTROY_LOG"
                  exit 1
                }
                echo "::notice::âœ“ Destroy completed successfully after force unlock"
                rm -f "$TEMP_DESTROY_LOG"
              else
                echo "::error::State lock detected but could not extract lock ID"
                echo "::error::You may need to manually unlock the state"
                echo "::error::Check the error output above for the lock ID"
                echo "::error::Lock ID format: b298ae0c-cd1f-af5a-f231-e02d8a246d86"
                rm -f "$TEMP_DESTROY_LOG"
                exit 1
              fi
            else
              echo "::error::Terraform destroy failed"
              echo "::error::Some resources may have been destroyed. Review state and AWS console."
              rm -f "$TEMP_DESTROY_LOG"
              exit 1
            fi
          fi
